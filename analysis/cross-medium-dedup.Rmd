---
title: "Cross-Medium Deduplication Analysis"
author: "NTT Dedup System"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Load required libraries
library(jsonlite)
library(ggplot2)
library(dplyr)
library(tidyr)
library(knitr)
library(scales)

# Try to load UpSetR if available
if (!require("UpSetR", quietly = TRUE)) {
  cat("Note: UpSetR not installed. Install with: install.packages('UpSetR')\n")
}
```

## Executive Summary

This report analyzes cross-medium blob sharing patterns in the NTT deduplication system.

```{r load-data}
# Load JSON/CSV data generated by SQL script
nhardlinks <- fromJSON("/tmp/nhardlinks.json")
sharing_matrix <- read.csv("/tmp/sharing_matrix.csv")
jaccard_data <- fromJSON("/tmp/jaccard.json")

# Basic stats
total_media <- length(jaccard_data)
total_pairs <- nrow(sharing_matrix)
```

**Key Statistics:**

- **Media analyzed:** `r total_media`
- **Cross-medium pairs:** `r total_pairs`
- **Total duplicated blobs:** `r sum(nhardlinks$num_blobs)`
- **Total inode instances:** `r comma(sum(nhardlinks$total_inodes))`

---

## 1. N-Hardlinks Distribution

Understanding how frequently blobs are duplicated across the system.

```{r nhardlinks-table}
# Format the table
nhardlinks_formatted <- nhardlinks %>%
  mutate(
    range = paste0(lower_bound, "-", upper_bound),
    pct_blobs = round(100 * num_blobs / sum(num_blobs), 1),
    pct_inodes = round(100 * total_inodes / sum(total_inodes), 1)
  ) %>%
  select(range, num_blobs, total_inodes, pct_blobs, pct_inodes)

kable(nhardlinks_formatted,
      col.names = c("Copy Range", "# Blobs", "Total Inodes", "% Blobs", "% Inodes"),
      format.args = list(big.mark = ","))
```

```{r nhardlinks-plot, fig.width=10, fig.height=6}
# Visualize distribution
nhardlinks %>%
  mutate(range_label = paste0(lower_bound, "-", upper_bound)) %>%
  ggplot(aes(x = reorder(range_label, lower_bound), y = num_blobs)) +
  geom_col(fill = "steelblue") +
  scale_y_log10(labels = comma) +
  labs(
    title = "N-Hardlinks Distribution (Logarithmic Scale)",
    subtitle = "Frequency of blob duplication across the system",
    x = "Duplication Range (# copies)",
    y = "Number of Blobs (log scale)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Interpretation:**

- **Primary mode:** `r nhardlinks$num_blobs[1]` blobs with 2-3 copies (simple pairs)
- **Secondary mode:** Look for peaks in the 32-63 range (systemic duplication)
- **Long tail:** Few blobs with extreme duplication (10K+ copies)

---

## 2. Cross-Medium Sharing Matrix

Storage impact of deduplication between media pairs.

```{r sharing-matrix-prep}
# Calculate Jaccard similarity
sharing_enriched <- sharing_matrix %>%
  rowwise() %>%
  mutate(
    blobs_a = jaccard_data[[medium_a]]$unique_blobs,
    blobs_b = jaccard_data[[medium_b]]$unique_blobs,
    union_size = blobs_a + blobs_b - shared_blobs,
    jaccard = round(shared_blobs / union_size, 4),
    shared_gb = round(shared_bytes / 1e9, 1),
    medium_a_short = paste0(substr(medium_a, 1, 4), "...", substr(medium_a, nchar(medium_a)-3, nchar(medium_a))),
    medium_b_short = paste0(substr(medium_b, 1, 4), "...", substr(medium_b, nchar(medium_b)-3, nchar(medium_b)))
  ) %>%
  arrange(desc(shared_gb))
```

```{r sharing-table}
# Top pairs by storage impact
sharing_enriched %>%
  select(medium_a_short, medium_b_short, shared_blobs, shared_gb, jaccard) %>%
  head(10) %>%
  kable(
    col.names = c("Medium A", "Medium B", "Shared Blobs", "Storage (GB)", "Jaccard"),
    format.args = list(big.mark = ",")
  )
```

```{r sharing-plot, fig.width=10, fig.height=6}
# Visualize top sharing pairs
sharing_enriched %>%
  head(10) %>%
  mutate(pair_label = paste(medium_a_short, "â†”", medium_b_short)) %>%
  ggplot(aes(x = reorder(pair_label, shared_gb), y = shared_gb)) +
  geom_col(aes(fill = jaccard)) +
  coord_flip() +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Jaccard\nSimilarity") +
  labs(
    title = "Top Cross-Medium Storage Savings",
    subtitle = "Storage impact of deduplication between media pairs",
    x = NULL,
    y = "Shared Storage (GB)"
  ) +
  theme_minimal()
```

**Key Insights:**

- Top pair saves: **`r sharing_enriched$shared_gb[1]` GB** between `r sharing_enriched$medium_a_short[1]` and `r sharing_enriched$medium_b_short[1]`
- Jaccard similarity: **`r sharing_enriched$jaccard[1]`** (0 = disjoint, 1 = identical)
- Total storage saved: **`r comma(sum(sharing_enriched$shared_gb))` GB** across all pairs

---

## 3. Per-Medium Profile

Understanding each medium's role in the dedup ecosystem.

```{r medium-profile}
# Convert jaccard_data list to dataframe
medium_stats <- data.frame(
  medium_hash = names(jaccard_data),
  unique_blobs = sapply(jaccard_data, function(x) x$unique_blobs)
) %>%
  mutate(
    pct_of_total = round(100 * unique_blobs / sum(unique_blobs), 1),
    medium_short = paste0(substr(medium_hash, 1, 4), "...", substr(medium_hash, nchar(medium_hash)-3, nchar(medium_hash)))
  ) %>%
  arrange(desc(unique_blobs))

kable(medium_stats %>% select(medium_short, unique_blobs, pct_of_total),
      col.names = c("Medium", "Unique Blobs", "% of Total"),
      format.args = list(big.mark = ","))
```

```{r medium-plot, fig.width=8, fig.height=5}
# Visualize medium sizes
medium_stats %>%
  ggplot(aes(x = reorder(medium_short, -unique_blobs), y = unique_blobs)) +
  geom_col(fill = "coral") +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Per-Medium Blob Counts",
    subtitle = "Size of each medium's unique content",
    x = "Medium",
    y = "Unique Blobs"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Role Assignment:**

- **Hub:** `r medium_stats$medium_short[1]` (`r comma(medium_stats$unique_blobs[1])` blobs, `r medium_stats$pct_of_total[1]`%)
- **Satellites:** Other media depend on hub for cross-medium dedup

---

## 4. Network Topology

```{r network-analysis}
# Identify hub (medium with most connections or highest total sharing)
hub_candidate <- sharing_enriched %>%
  group_by(medium_a) %>%
  summarise(total_shared = sum(shared_blobs)) %>%
  arrange(desc(total_shared)) %>%
  slice(1) %>%
  pull(medium_a)

hub_short <- paste0(substr(hub_candidate, 1, 4), "...", substr(hub_candidate, nchar(hub_candidate)-3, nchar(hub_candidate)))
```

**Network Structure:** Star topology with **`r hub_short`** as central hub

**Properties:**

- **Hub medium:** `r hub_short` connects to all others
- **Weak clustering:** Jaccard scores <10% indicate independent content collections
- **No tight communities:** Media are largely independent

---

## 5. UpSet Visualization

```{r upset-check}
upset_available <- file.exists("/tmp/upset_data.csv") && requireNamespace("UpSetR", quietly = TRUE)
```

`r if (upset_available) "### Intersection Pattern Visualization" else "UpSet visualization not available (requires UpSetR package and /tmp/upset_data.csv)"`

```{r upset-plot, eval=upset_available, fig.width=12, fig.height=8}
if (upset_available) {
  upset_data <- read.csv("/tmp/upset_data.csv", row.names = 1)

  # Only use first 10 media columns if more exist
  media_cols <- grep("^media_", names(upset_data), value = TRUE)[1:min(10, sum(grepl("^media_", names(upset_data))))]

  UpSetR::upset(
    upset_data[, media_cols],
    sets = media_cols,
    order.by = "freq",
    text.scale = 1.3,
    mainbar.y.label = "Blob Intersection Size",
    sets.x.label = "Blobs per Medium"
  )
}
```

---

## 6. Recommendations

### Storage Optimization

- **Priority 1:** Top pair already achieving maximum dedup (`r sharing_enriched$shared_gb[1]` GB)
- **Priority 2:** Monitor hub medium (`r hub_short`) - critical for cross-medium dedup
- **Priority 3:** Low Jaccard scores suggest distinct content - limited further dedup opportunity

### Data Management

- **If hub is archival:** Ensure redundancy (it's the dedup anchor)
- **If storage reduction needed:** Consider consolidating satellites into hub
- **For new ingestion:** Expect 70-80% new content, 20-30% dedup from hub

---

## Technical Notes

**Analysis Runtime:** `r Sys.time()`

**Data Sources:**

- `/tmp/nhardlinks.json` - n_hardlinks distribution
- `/tmp/sharing_matrix.csv` - cross-medium sharing
- `/tmp/jaccard.json` - per-medium blob counts
- `/tmp/upset_data.csv` - UpSet visualization data (optional)

**To regenerate data:** Run `./cross-medium-dedup.sql` from the analysis directory

**Performance:** Analysis queries use materialized `blob_media_matrix` table and INTERSECT operations for efficiency.
