<!--
Author: PB and Claude
Date: Tue 08 Oct 2025
License: (c) HRDAG, 2025, GPL-2 or newer

------
ntt/docs/workplan-2025-10-08.md
-->

# NTT System Workplan - 2025-10-08

## Current System State

**Database Status:**
- Total media: 33
- Enumerated: 6 (18%)
- Fully copied: 8 (24%)
- Has image files: ~25

**Mount Status:**
- Active mounts: 2 (no overmounts currently)
- Stale loop devices: 1 (`loop27` → bb226d2a deleted inode)
- Mount locking: NOT implemented

**Schema Readiness:**
- `medium.health` - exists, sparsely populated (5 entries: "ok", "true")
- `medium.problems` - exists, completely empty (all NULL)

**Recent Commits:**
- `853bd40` - Loader safeguards (statement timeout, ANALYZE, timing)
- `a67eb3d` - Fix copier infinite loop for directories
- `e0a58ab` - DiagnosticService Phase 1 (detection framework)

---

## Completed Work - Summary

### 1. Loader Safeguards ✅ COMPLETE
**File:** `bin/ntt-loader`
**Status:** Fully implemented and tested

**What was done:**
- Added statement timeout (5min) at line 193
- Explicit ANALYZE after data load (lines 282-283)
- Timing instrumentation for diagnostics
- Successfully prevented 12.5-minute hang

**Evidence:** loader-hang-investigation-2025-10-07.md shows 0s deduplication time on retest

**No further work needed.**

---

### 2. DiagnosticService Framework ⚠️ PHASE 2 ACTIVE
**Files:** `bin/ntt_copier_diagnostics.py`, `bin/ntt-copier.py`
**Status:** Phase 1 + Phase 2 implemented and active

**What was done:**
- Created `DiagnosticService` class (207 lines)
- In-memory retry tracking: `track_failure()`
- Checkpoint at retry #10: `diagnose_at_checkpoint()`
- Exception pattern matching (BEYOND_EOF, I/O errors, missing files)
- dmesg scanning for kernel errors
- Mount health checking

**Phase 2 auto-skip IS LIVE:**
- `should_skip_permanently()` implemented
- Copier calls it at checkpoint and DOES skip on BEYOND_EOF
- Code exists in ntt-copier.py lines 660-668

**Documentation gap:**
- copier-diagnostic-ideas.md says "Phase 1 only"
- But code has Phase 2 active
- **Action needed:** Update docs to reflect Phase 2 is production

**Next phases (not yet implemented):**
- Phase 3: Auto-remount on mount issues
- Phase 4: Record diagnostic events in `medium.problems`

---

### 3. Batch Processing Optimizations ✅ COMPLETE
**File:** `bin/ntt-copier.py`
**Status:** Fully implemented

**What was done:**
- Comprehensive timing instrumentation (commit 527bf90)
- Random probe strategy for batch claiming
- Partition pruning optimization (composite PK WHERE clause)
- Size distribution logging

**No further work needed.**

---

## Pending Work - Prioritized

### Priority 1: Immediate Cleanup Tasks

#### Task 1.1: Clean Stale Loop Device
**Impact:** Prevents loop device exhaustion
**Effort:** 1 minute
**Files:** None (manual cleanup)

**Action:**
```bash
sudo losetup -d /dev/loop27
```

**Verification:**
```bash
sudo losetup -l | grep deleted
# Should return: no results
```

**Status:** NOT DONE (1 stale loop exists)

---

#### Task 1.2: Update DiagnosticService Documentation
**Impact:** Docs match reality
**Effort:** 15 minutes
**Files:** `docs/copier-diagnostic-ideas.md`

**Action:**
Update Phase 1/Phase 2 sections to reflect:
- Phase 1 (detection): ✅ Complete and in production
- Phase 2 (auto-skip): ✅ Complete and in production (NOT "next .img")
- Phase 3 (auto-remount): ⏸️ Designed but not implemented
- Phase 4 (problem recording): ⏸️ Designed but not implemented

**Status:** NOT DONE (docs say Phase 1 only)

---

### Priority 2: Mount Architecture Integration

#### Task 2.1: Implement Stale Loop Cleanup in ntt-mount-helper
**Impact:** Prevents loop device leaks after ddrescue restarts
**Effort:** 1-2 hours
**Files:** `bin/ntt-mount-helper`

**Current state:**
- mount-arch-cleanups.md has complete design (lines 329-437)
- Function `cleanup_stale_loops()` fully specified
- Test plan included
- **NO CODE IMPLEMENTED YET**

**Design from mount-arch-cleanups.md:**
```bash
# Insert before do_mount() (line 38)
cleanup_stale_loops() {
  local medium_hash="$1"
  local image_path="$2"

  # Find all loop devices for this image (including deleted inodes)
  losetup -l | grep "$(basename "$image_path")" | while read -r line; do
    local loop_dev=$(echo "$line" | awk '{print $1}')
    local status=$(echo "$line" | grep -o "(deleted)" || echo "")

    if [[ -n "$status" ]]; then
      echo "Cleaning up stale loop device: $loop_dev (deleted inode)" >&2

      # Try unmount if mounted
      if findmnt -S "$loop_dev" "$mount_point" >/dev/null 2>&1; then
        umount "$mount_point" 2>/dev/null || true
      fi

      # Detach loop device
      losetup -d "$loop_dev" 2>/dev/null || true
    fi
  done
}
```

**Integration point:**
Call `cleanup_stale_loops "$medium_hash" "$image_path"` in `do_mount()` before line 63 (losetup call)

**Testing:**
```bash
# Simulate stale loop
sudo losetup -f --show /data/fast/img/test.img
sudo rm /data/fast/img/test.img
sudo touch /data/fast/img/test.img

# Mount should cleanup stale and create new
sudo bin/ntt-mount-helper mount test /data/fast/img/test.img

# Verify no deleted loops remain
sudo losetup -l | grep test | grep deleted
# Should return: no results
```

**Status:** NOT DONE (design complete, awaiting implementation)

---

#### Task 2.2: Implement Per-Medium Mount Locking
**Impact:** Prevents mount races and overmounts
**Effort:** 2-3 hours
**Files:** `bin/ntt-copier.py`

**Current state:**
- mount-arch-cleanups.md has complete design (lines 160-326)
- `/var/lock/ntt/` directory does NOT exist
- No flock logic in copier
- **NO CODE IMPLEMENTED YET**

**Design from mount-arch-cleanups.md:**
- Add flock to `ensure_medium_mounted()` (copier.py line ~297)
- Create `/var/lock/ntt/mount-{medium_hash}.lock`
- Double-check pattern (cache → lock → check again → mount)
- Add `_detect_and_cleanup_overmounts()` helper
- Add `_get_medium_health()` helper

**Why this matters:**
- Currently if 10 workers start on same medium simultaneously, mount races can occur
- mount-arch-cleanups.md documents exact scenario that happened (9 workers, 9 loop devices)
- We haven't seen this recently because we're processing serially

**Decision:** Wait for this until we see mount issues, OR implement proactively?

**Status:** NOT DONE (design complete, implementation deferred)

---

### Priority 3: Problem Recording System

#### Task 3.1: Implement Phase 4 - Record Diagnostic Events
**Impact:** Enables analytics on copy failures
**Effort:** 2-3 hours
**Files:** `bin/ntt_copier_diagnostics.py`, `bin/ntt-copier.py`

**Current state:**
- `medium.problems` column exists (JSONB)
- All values are NULL (never populated)
- copier-diagnostic-ideas.md has design (lines 199-267)

**Design from copier-diagnostic-ideas.md:**
```python
def record_diagnostic_event(self, medium_hash, ino, findings, action_taken):
    """Record what we tried in medium.problems."""
    entry = {
        'ino': ino,
        'retry_count': findings['retry_count'],
        'checks': findings['checks_performed'],
        'action': action_taken,  # 'skipped', 'remounted', 'continuing'
        'timestamp': datetime.now().isoformat(),
        'worker_id': self.worker_id
    }

    # Append to diagnostic_events array in problems JSONB
    cur.execute("""
        UPDATE medium
        SET problems = COALESCE(problems, '{}'::jsonb) ||
                      jsonb_build_object(
                          'diagnostic_events',
                          COALESCE(problems->'diagnostic_events', '[]'::jsonb) || %s::jsonb
                      )
        WHERE medium_hash = %s
    """, (json.dumps(entry), medium_hash))
```

**Integration:**
Call after diagnostic checkpoint decision (copier.py around line 660)

**Analytics enabled:**
```sql
-- See what diagnostics have been run
SELECT medium_hash,
       jsonb_array_length(problems->'diagnostic_events') as event_count
FROM medium
WHERE problems->'diagnostic_events' IS NOT NULL;

-- Count by action type
SELECT event->>'action' as action, COUNT(*)
FROM medium, jsonb_array_elements(problems->'diagnostic_events') as event
GROUP BY 1;
```

**Status:** NOT DONE (schema ready, code not implemented)

---

#### Task 3.2: Copier Records Unrecoverable Errors in medium.problems
**Impact:** Permanent record of why media failed
**Effort:** 1-2 hours
**Files:** `bin/ntt-copier.py`

**Current state:**
- Copier marks individual inodes as EXCLUDED/SKIP
- But doesn't update `medium.problems` with summary
- No way to query "which media had BEYOND_EOF errors"

**Design (new):**
When copier encounters medium-level issues:
- First BEYOND_EOF error → record in `medium.problems`
- All paths excluded for medium → record summary
- Mount failures → record details

**Example:**
```python
# When first BEYOND_EOF detected
cur.execute("""
    UPDATE medium
    SET problems = COALESCE(problems, '{}'::jsonb) ||
                  jsonb_build_object('beyond_eof_detected', true,
                                    'first_ino', %s,
                                    'detected_at', %s)
    WHERE medium_hash = %s
""", (ino, datetime.now().isoformat(), medium_hash))
```

**Status:** NOT DONE (needs design work)

---

### Priority 4: Health Column Population (Deferred)

#### Task 4.1: Orchestrator Populates medium.health After Imaging
**Impact:** Copier can refuse to mount known-bad images
**Effort:** 1-2 hours
**Files:** `bin/ntt-orchestrator` or `bin/ntt-imager`

**Current state:**
- `medium.health` has 5 values: 4x "ok", 1x "true" (wrong format)
- mount-arch-cleanups.md has design (lines 510-541)
- Orchestrator doesn't update health after ddrescue

**Design from mount-arch-cleanups.md:**
Parse ddrescue log, set health based on recovery %:
- 100% → 'ok'
- ≥95% → 'incomplete'
- ≥50% → 'corrupt'
- <50% → 'failed'

**Integration point:**
After ddrescue completes in ntt-imager

**Status:** NOT DONE (deferred - copier.problems more useful)

---

#### Task 4.2: Copier Checks Health Before Mounting
**Impact:** Prevents mounting corrupt images
**Effort:** 30 minutes (if health populated)
**Files:** `bin/ntt-copier.py`

**Current state:**
- `_get_medium_health()` designed in mount-arch-cleanups.md (lines 250-261)
- NOT implemented in copier
- Would prevent FAT error loops

**Dependency:** Requires Task 4.1 (orchestrator populating health)

**Status:** NOT DONE (deferred - depends on 4.1)

---

## Open Questions for Discussion

### Q1: Mount Locking Priority
**Context:** mount-arch-cleanups.md has complete design for flock-based mount locking

**Options:**
- A) Implement now proactively (2-3 hours)
- B) Wait until we see mount races/overmounts in production
- C) Implement partial (just stale loop cleanup, defer locking)

**Recommendation:** Option C - implement stale loop cleanup now (high value, low risk), defer flock locking until we run parallel copiers

---

### Q2: Health vs Problems - Which to Use?
**Context:** Schema has both `medium.health` and `medium.problems`

**Current thinking:**
- `health` - orchestrator sets based on ddrescue recovery % (simple enum)
- `problems` - copier records diagnostic findings (rich JSONB)

**Issue:** Some overlap in purpose

**Options:**
- A) Use both - health for simple status, problems for details
- B) Deprecate health, use only problems
- C) Keep health simple (ok/corrupt/failed), use problems for diagnostics

**Recommendation:** Option C - health is quick status check, problems is detailed log

---

### Q3: Diagnostic Service Phase 3 (Auto-Remount)
**Context:** Phase 3 design exists (copier-diagnostic-ideas.md lines 141-196)

**Current state:**
- Phase 2 (auto-skip) is working in production
- Phase 3 (auto-remount) is designed but not implemented

**Questions:**
- Have we seen mount issues that would benefit from auto-remount?
- Is it worth implementing before we have mount locking (Task 2.2)?

**Recommendation:** Defer until after mount locking is implemented (avoids remount races)

---

### Q4: Schema Fix - Duplicate health Column?
**Context:** `information_schema.columns` shows 3 rows for medium table columns (health appears twice)

**Investigation needed:**
```sql
SELECT table_schema, table_name, column_name
FROM information_schema.columns
WHERE table_name LIKE 'medium%'
ORDER BY table_name, ordinal_position;
```

**Hypothesis:** Partitioned child tables or view artifact

**Action:** Investigate schema, document findings

---

## Implementation Roadmap

### Week 1 (Immediate)
- [x] ~~Task 1.1: Clean stale loop~~ (1 min) → **DO NOW**
- [ ] Task 1.2: Update diagnostic docs (15 min)
- [ ] Task 2.1: Stale loop cleanup in mount-helper (1-2 hours)

### Week 2 (Near-term)
- [ ] Task 3.1: Phase 4 diagnostic recording (2-3 hours)
- [ ] Task 3.2: Copier problem recording (1-2 hours)
- [ ] Q4: Investigate duplicate health column (30 min)

### Week 3+ (Deferred)
- [ ] Task 2.2: Mount locking (2-3 hours) - **AFTER decision on Q1**
- [ ] Task 4.1: Orchestrator health population (1-2 hours) - **AFTER decision on Q2**
- [ ] Task 4.2: Copier health checking (30 min) - **AFTER 4.1**
- [ ] Phase 3: Auto-remount (2-3 hours) - **AFTER decision on Q3**

---

## Success Metrics

**Immediate (Week 1):**
- Zero stale loop devices
- Documentation matches code (diagnostic phases)
- mount-helper cleans up deleted inodes automatically

**Near-term (Week 2):**
- Diagnostic events recorded in `medium.problems`
- Can query which media had BEYOND_EOF errors
- Medium-level summaries available

**Long-term:**
- No mount races (if we implement locking)
- Health column consistently populated
- Auto-remount working (if implemented)

---

## Risk Assessment

**Low Risk:**
- Task 1.1 (manual cleanup)
- Task 1.2 (docs)
- Task 2.1 (stale loop cleanup) - idempotent, only touches deleted inodes

**Medium Risk:**
- Task 3.1, 3.2 (problem recording) - schema changes, JSONB manipulation
- Task 4.1 (health population) - needs ddrescue log parsing

**High Risk:**
- Task 2.2 (mount locking) - affects mount path, could block workers if bugs

**Mitigation:**
- Implement low-risk tasks first
- Test medium-risk on single worker before parallel
- Defer high-risk until needed

---

## References

**Key documents:**
- `docs/loader-hang-investigation-2025-10-07.md` - Loader fixes (complete)
- `docs/mount-arch-cleanups.md` - Mount architecture design (unimplemented)
- `docs/copier-diagnostic-ideas.md` - Diagnostic service phases (Phase 2 done)
- `docs/disk-read-checklist.md` - Manual procedures

**Code files:**
- `bin/ntt-loader` - Loader with safeguards
- `bin/ntt-copier.py` - Worker with diagnostics (Phase 2 active)
- `bin/ntt_copier_diagnostics.py` - DiagnosticService class
- `bin/ntt-mount-helper` - Mount wrapper (no stale cleanup yet)

**Database:**
- `medium.health` - 5 populated, needs schema review
- `medium.problems` - 0 populated, ready for use
- `inode.errors[]` - persistent error tracking (in use)

---

## Next Steps

1. **Discuss open questions** (Q1-Q4 above)
2. **Agree on Week 1 priorities** (recommend: 1.1 + 1.2 + 2.1)
3. **Decide mount locking timeline** (proactive vs reactive)
4. **Begin implementation** of agreed tasks

---

**Status:** Draft for discussion
**Last updated:** 2025-10-08
