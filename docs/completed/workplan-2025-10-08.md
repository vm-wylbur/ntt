<!-- completed: Status snapshot as of 2025-10-09; documents completed work (Phase 1-4 diagnostics, loader safeguards, stale loop cleanup) -->

<!--
Author: PB and Claude
Date: Tue 08 Oct 2025
License: (c) HRDAG, 2025, GPL-2 or newer

------
ntt/docs/workplan-2025-10-08.md
-->

# NTT System Workplan - 2025-10-08

## Current System State

**Database Status:**
- Total media: 33
- Enumerated: 6 (18%)
- Fully copied: 8 (24%)
- Has image files: ~25

**Mount Status:**
- Active mounts: 2 (no overmounts currently)
- Stale loop devices: 2 (loop0, loop1 - kernel won't release, awaiting reboot)
- Mount locking: NOT implemented

**Schema Readiness:**
- `medium.health` - exists, sparsely populated (5 entries: "ok", "true")
- `medium.problems` - exists, completely empty (all NULL)
- Test schema `copyjob_test` - removed 2025-10-09 (cleanup complete)

**Recent Commits:**
- `853bd40` - Loader safeguards (statement timeout, ANALYZE, timing)
- `a67eb3d` - Fix copier infinite loop for directories
- `e0a58ab` - DiagnosticService Phase 1 (detection framework)

---

## Completed Work - Summary

### 1. Loader Safeguards ✅ COMPLETE
**File:** `bin/ntt-loader`
**Status:** Fully implemented and tested

**What was done:**
- Added statement timeout (5min) at line 193
- Explicit ANALYZE after data load (lines 282-283)
- Timing instrumentation for diagnostics
- Successfully prevented 12.5-minute hang

**Evidence:** loader-hang-investigation-2025-10-07.md shows 0s deduplication time on retest

**No further work needed.**

---

### 2. DiagnosticService Framework ✅ PHASES 1, 2, 4 COMPLETE
**Files:** `bin/ntt_copier_diagnostics.py`, `bin/ntt-copier.py`
**Status:** Phase 1 + Phase 2 + Phase 4 implemented and active (Phase 3 deferred)

**What was done:**
- Created `DiagnosticService` class (207 lines)
- In-memory retry tracking: `track_failure()`
- Checkpoint at retry #10: `diagnose_at_checkpoint()`
- Exception pattern matching (BEYOND_EOF, I/O errors, missing files)
- dmesg scanning for kernel errors
- Mount health checking

**Phase 2 auto-skip IS LIVE:**
- `should_skip_permanently()` implemented
- Copier calls it at checkpoint and DOES skip on BEYOND_EOF
- Code exists in ntt-copier.py lines 660-668

**Phase 4 diagnostic recording COMPLETE (2025-10-09):**
- Deferred write pattern implemented to preserve FOR UPDATE SKIP LOCKED
- Three-phase transaction architecture: batch → diagnostics → summaries
- `record_diagnostic_event_no_commit()` method (ntt_copier_diagnostics.py:208-253)
- Medium-level summaries: `beyond_eof_detected`, `high_error_rate`
- Diagnostic queries documented (docs/diagnostic-queries.md - 20 SQL queries)
- Tested on e5727c34fb46e18c87153d576388ea32 (150GB HFS+ Time Machine)

**Remaining phase (deferred):**
- Phase 3: Auto-remount on mount issues (defer until after mount locking)

---

### 3. Batch Processing Optimizations ✅ COMPLETE
**File:** `bin/ntt-copier.py`
**Status:** Fully implemented

**What was done:**
- Comprehensive timing instrumentation (commit 527bf90)
- Random probe strategy for batch claiming
- Partition pruning optimization (composite PK WHERE clause)
- Size distribution logging

**No further work needed.**

---

## Completed Since Last Update (2025-10-09)

### Task 1.1: Stale Loop Cleanup in ntt-mount-helper ✅
- **Status:** DONE (implemented in commit 93cf94a)
- **Implementation:** `cleanup_stale_loops()` function added to ntt-mount-helper
- **Location:** bin/ntt-mount-helper lines 38-69
- **Functionality:** Detects and cleans up loop devices pointing to deleted inodes
- **Integration:** Called automatically before mount operations

### Task 1.2: Diagnostic Documentation ✅
- **Status:** DONE
- **Files updated:** docs/copier-diagnostic-ideas.md, docs/diagnostic-queries.md (NEW)
- **What:** Documented Phase 4 completion, deferred write pattern, 20 SQL queries

### Task 3.1: Phase 4 Diagnostic Recording ✅
- **Status:** DONE
- **Implementation:** Deferred write pattern with three-phase transactions
- **Files:** bin/ntt_copier_diagnostics.py, bin/ntt-copier.py
- **Testing:** Validated on e5727c34 (150GB HFS+ Time Machine, 1M files)

### Ignore Patterns System ✅
- **Status:** DONE
- **Added patterns:** 3 new patterns for LaTeX/TextMate problematic filenames
- **Trigger:** e5727c34 had 6 files causing 120K retry attempts
- **Documentation:** docs/ignore-patterns-guide.md (NEW - comprehensive reference)

### Production Testing ✅
- **Medium:** e5727c34fb46e18c87153d576388ea32 (150GB HFS+ Time Machine)
- **Result:** 1,005,315 files copied (99.9994% success)
- **Pipeline:** Full enum → load → copy → archive validated
- **Duration:** ~9 hours overnight processing

### Infrastructure Cleanup (2025-10-09) ✅
- **Task C1:** Cleaned stale loop devices
  - loop27: Successfully removed (was held by md5 RAID array, stopped md5)
  - loop0, loop1: Kernel won't release (deleted backing files, AUTOCLEAR=1, will clean at reboot)
- **Task C2:** Investigated "duplicate" health column
  - Found intentional test schema `copyjob_test` (created 2025-10-01)
  - Not a schema bug - proper test isolation infrastructure
- **Task C3:** Removed test infrastructure
  - Dropped `copyjob_test` schema (11 objects: 10 tables + 1 view)
  - Dropped `copyjob_test_user` role
  - No more duplicate health columns in information_schema

---

## Pending Work - Prioritized

### Priority 1: Immediate Cleanup Tasks

#### Task 1.1: Clean Stale Loop Device ✅ DONE (partial)
**Impact:** Prevents loop device exhaustion
**Effort:** 1 minute (+ investigation time for stubborn loops)
**Files:** None (manual cleanup)

**Completed:** 2025-10-09

**What was done:**
- Successfully removed loop27 (was held by md5 RAID array, stopped md5 first)
- loop0, loop1: Kernel won't release despite:
  - No holders in /sys/block/loopX/holders/
  - AUTOCLEAR=1 flag set
  - Deleted backing files
  - Multiple losetup -d attempts
- Investigation: Kernel reference counting issue, will auto-cleanup at reboot

**Status:** ✅ DONE (loop27 removed, loop0/loop1 awaiting reboot cleanup)

---

#### Task 1.2: Update DiagnosticService Documentation ✅ DONE
**Impact:** Docs match reality
**Effort:** 15 minutes
**Files:** `docs/copier-diagnostic-ideas.md`

**Completed:** 2025-10-09

**What was done:**
- Updated `copier-diagnostic-ideas.md` to reflect Phase 4 complete
- Documented deferred write pattern and three-phase transactions
- Added test results from e5727c34 (150GB HFS+ Time Machine)
- Created `diagnostic-queries.md` with 20 SQL query examples

**Status:** ✅ DONE

---

### Priority 2: Mount Architecture Integration

#### Task 2.1: Implement Stale Loop Cleanup in ntt-mount-helper ✅ DONE
**Impact:** Prevents loop device leaks after ddrescue restarts
**Effort:** 1-2 hours
**Files:** `bin/ntt-mount-helper`

**Completed:** 2025-10-09 (commit 93cf94a)

**Implementation:**
- Added `cleanup_stale_loops()` function (lines 38-69)
- Automatically detects loop devices pointing to deleted inodes
- Unmounts if necessary, then detaches stale loop devices
- Called before every mount operation

**Code location:**
```bash
# bin/ntt-mount-helper lines 38-69
cleanup_stale_loops() {
  local medium_hash="$1"
  local image_path="$2"
  local mount_point="/mnt/ntt/$medium_hash"

  # Find all loop devices for this image (including deleted inodes)
  losetup -l | grep "$(basename "$image_path")" | while read -r line; do
    local loop_dev=$(echo "$line" | awk '{print $1}')
    local status=$(echo "$line" | grep -o "(deleted)" || echo "")

    # Only cleanup deleted inodes (active image is fine)
    if [[ -n "$status" ]]; then
      echo "Cleaning up stale loop device: $loop_dev (deleted inode)" >&2
      # ... unmount and detach logic ...
    fi
  done
}
```

**Integration:**
- Called in `do_mount()` before creating new loop device (line 90)
- Idempotent - safe to call multiple times
- Only affects deleted inodes, not active images

**Status:** ✅ DONE (in production)

---

#### Task 2.2: Implement Per-Medium Mount Locking
**Impact:** Prevents mount races and overmounts
**Effort:** 2-3 hours
**Files:** `bin/ntt-copier.py`

**Current state:**
- mount-arch-cleanups.md has complete design (lines 160-326)
- `/var/lock/ntt/` directory does NOT exist
- No flock logic in copier
- **NO CODE IMPLEMENTED YET**

**Design from mount-arch-cleanups.md:**
- Add flock to `ensure_medium_mounted()` (copier.py line ~297)
- Create `/var/lock/ntt/mount-{medium_hash}.lock`
- Double-check pattern (cache → lock → check again → mount)
- Add `_detect_and_cleanup_overmounts()` helper
- Add `_get_medium_health()` helper

**Why this matters:**
- Currently if 10 workers start on same medium simultaneously, mount races can occur
- mount-arch-cleanups.md documents exact scenario that happened (9 workers, 9 loop devices)
- We haven't seen this recently because we're processing serially

**Decision:** Wait for this until we see mount issues, OR implement proactively?

**Status:** NOT DONE (design complete, implementation deferred)

---

### Priority 3: Problem Recording System

#### Task 3.1: Implement Phase 4 - Record Diagnostic Events ✅ DONE
**Impact:** Enables analytics on copy failures
**Effort:** 2-3 hours
**Files:** `bin/ntt_copier_diagnostics.py`, `bin/ntt-copier.py`

**Completed:** 2025-10-09

**Implementation:**
- Added `record_diagnostic_event_no_commit()` method (ntt_copier_diagnostics.py:208-253)
- Implemented deferred write pattern: queue events during batch, write after commit
- Three-phase transaction architecture:
  1. **Batch processing** - holds FOR UPDATE SKIP LOCKED
  2. **Diagnostic events** - writes queued events (separate transaction)
  3. **Medium summaries** - records beyond_eof_detected, high_error_rate
- Added medium-level helper methods in ntt-copier.py:
  - `check_and_record_beyond_eof()` (lines 887-943)
  - `check_and_record_high_error_rate()` (lines 945-990)

**Critical design decision:**
- Cannot commit during batch processing (breaks FOR UPDATE SKIP LOCKED locks)
- Must queue events and write after batch commit releases locks
- Prevents race conditions where other workers steal rows

**Testing:**
- Validated on e5727c34 (150GB HFS+ Time Machine, 1M files)
- No lock conflicts observed
- Normal operation: 0 diagnostic events (expected)
- Error testing: Still needs medium with actual I/O errors

**Analytics enabled:**
See `docs/diagnostic-queries.md` for 20 SQL query examples including:
- Query diagnostic events by medium
- Count events by action type
- Analyze error patterns
- Medium-level problem summaries

**Status:** ✅ DONE (production-ready, validated for normal operation)

---

#### Task 3.2: Copier Records Unrecoverable Errors in medium.problems
**Impact:** Permanent record of why media failed
**Effort:** 1-2 hours
**Files:** `bin/ntt-copier.py`

**Current state:**
- Copier marks individual inodes as EXCLUDED/SKIP
- But doesn't update `medium.problems` with summary
- No way to query "which media had BEYOND_EOF errors"

**Design (new):**
When copier encounters medium-level issues:
- First BEYOND_EOF error → record in `medium.problems`
- All paths excluded for medium → record summary
- Mount failures → record details

**Example:**
```python
# When first BEYOND_EOF detected
cur.execute("""
    UPDATE medium
    SET problems = COALESCE(problems, '{}'::jsonb) ||
                  jsonb_build_object('beyond_eof_detected', true,
                                    'first_ino', %s,
                                    'detected_at', %s)
    WHERE medium_hash = %s
""", (ino, datetime.now().isoformat(), medium_hash))
```

**Status:** NOT DONE (needs design work)

---

### Priority 4: Health Column Population (Deferred)

#### Task 4.1: Orchestrator Populates medium.health After Imaging
**Impact:** Copier can refuse to mount known-bad images
**Effort:** 1-2 hours
**Files:** `bin/ntt-orchestrator` or `bin/ntt-imager`

**Current state:**
- `medium.health` has 5 values: 4x "ok", 1x "true" (wrong format)
- mount-arch-cleanups.md has design (lines 510-541)
- Orchestrator doesn't update health after ddrescue

**Design from mount-arch-cleanups.md:**
Parse ddrescue log, set health based on recovery %:
- 100% → 'ok'
- ≥95% → 'incomplete'
- ≥50% → 'corrupt'
- <50% → 'failed'

**Integration point:**
After ddrescue completes in ntt-imager

**Status:** NOT DONE (deferred - copier.problems more useful)

---

#### Task 4.2: Copier Checks Health Before Mounting
**Impact:** Prevents mounting corrupt images
**Effort:** 30 minutes (if health populated)
**Files:** `bin/ntt-copier.py`

**Current state:**
- `_get_medium_health()` designed in mount-arch-cleanups.md (lines 250-261)
- NOT implemented in copier
- Would prevent FAT error loops

**Dependency:** Requires Task 4.1 (orchestrator populating health)

**Status:** NOT DONE (deferred - depends on 4.1)

---

## Open Questions for Discussion

### Q1: Mount Locking Priority
**Context:** mount-arch-cleanups.md has complete design for flock-based mount locking

**Options:**
- A) Implement now proactively (2-3 hours)
- B) Wait until we see mount races/overmounts in production
- C) Implement partial (just stale loop cleanup, defer locking)

**Recommendation:** Option C - implement stale loop cleanup now (high value, low risk), defer flock locking until we run parallel copiers

---

### Q2: Health vs Problems - Which to Use?
**Context:** Schema has both `medium.health` and `medium.problems`

**Current thinking:**
- `health` - orchestrator sets based on ddrescue recovery % (simple enum)
- `problems` - copier records diagnostic findings (rich JSONB)

**Issue:** Some overlap in purpose

**Options:**
- A) Use both - health for simple status, problems for details
- B) Deprecate health, use only problems
- C) Keep health simple (ok/corrupt/failed), use problems for diagnostics

**Recommendation:** Option C - health is quick status check, problems is detailed log

---

### Q3: Diagnostic Service Phase 3 (Auto-Remount)
**Context:** Phase 3 design exists (copier-diagnostic-ideas.md lines 141-196)

**Current state:**
- Phase 2 (auto-skip) is working in production
- Phase 3 (auto-remount) is designed but not implemented

**Questions:**
- Have we seen mount issues that would benefit from auto-remount?
- Is it worth implementing before we have mount locking (Task 2.2)?

**Recommendation:** Defer until after mount locking is implemented (avoids remount races)

---

### Q4: Schema Fix - Duplicate health Column? ✅ RESOLVED
**Context:** `information_schema.columns` showed 3 rows for medium table columns (health appeared twice)

**Completed:** 2025-10-09

**Finding:** Not a bug - two intentional schemas
- `public.medium` (production, 46 media records)
- `copyjob_test.medium` (testing, created 2025-10-01 by test/setup_test_env.sh)

**Resolution:** Removed test schema
- Test infrastructure was no longer needed
- Dropped `copyjob_test` schema (11 objects)
- Dropped `copyjob_test_user` role
- Now only one `health` column exists (in `public.medium`)

**Status:** ✅ RESOLVED (test schema removed)

---

## Implementation Roadmap

### Week 1 (Immediate)
- [x] ~~Task 1.1: Clean stale loop~~ ✅ DONE (2025-10-09, commit 93cf94a)
- [x] ~~Task 1.2: Update diagnostic docs~~ ✅ DONE (2025-10-09)
- [x] ~~Task 2.1: Stale loop cleanup in mount-helper~~ ✅ DONE (2025-10-09, commit 93cf94a)

### Week 2 (Near-term)
- [x] ~~Task 3.1: Phase 4 diagnostic recording~~ ✅ DONE (2025-10-09, commit 6c963c7)
- [ ] Task 3.2: Copier problem recording (partial - medium summaries done, needs refinement)
- [x] ~~Q4: Investigate duplicate health column~~ ✅ RESOLVED (2025-10-09, test schema removed)

### Week 3+ (Deferred)
- [ ] Task 2.2: Mount locking (2-3 hours) - **AFTER decision on Q1**
- [ ] Task 4.1: Orchestrator health population (1-2 hours) - **AFTER decision on Q2**
- [ ] Task 4.2: Copier health checking (30 min) - **AFTER 4.1**
- [ ] Phase 3: Auto-remount (2-3 hours) - **AFTER decision on Q3**

---

## Success Metrics

**Immediate (Week 1):**
- Zero stale loop devices
- Documentation matches code (diagnostic phases)
- mount-helper cleans up deleted inodes automatically

**Near-term (Week 2):**
- Diagnostic events recorded in `medium.problems`
- Can query which media had BEYOND_EOF errors
- Medium-level summaries available

**Long-term:**
- No mount races (if we implement locking)
- Health column consistently populated
- Auto-remount working (if implemented)

---

## Risk Assessment

**Low Risk:**
- Task 1.1 (manual cleanup)
- Task 1.2 (docs)
- Task 2.1 (stale loop cleanup) - idempotent, only touches deleted inodes

**Medium Risk:**
- Task 3.1, 3.2 (problem recording) - schema changes, JSONB manipulation
- Task 4.1 (health population) - needs ddrescue log parsing

**High Risk:**
- Task 2.2 (mount locking) - affects mount path, could block workers if bugs

**Mitigation:**
- Implement low-risk tasks first
- Test medium-risk on single worker before parallel
- Defer high-risk until needed

---

## References

**Key documents:**
- `docs/loader-hang-investigation-2025-10-07.md` - Loader fixes (complete)
- `docs/mount-arch-cleanups.md` - Mount architecture design (unimplemented)
- `docs/copier-diagnostic-ideas.md` - Diagnostic service phases (Phase 2 done)
- `docs/disk-read-checklist.md` - Manual procedures

**Code files:**
- `bin/ntt-loader` - Loader with safeguards
- `bin/ntt-copier.py` - Worker with diagnostics (Phase 2 active)
- `bin/ntt_copier_diagnostics.py` - DiagnosticService class
- `bin/ntt-mount-helper` - Mount wrapper (no stale cleanup yet)

**Database:**
- `medium.health` - 5 populated, needs schema review
- `medium.problems` - 0 populated, ready for use
- `inode.errors[]` - persistent error tracking (in use)

---

## Next Steps

1. **Discuss open questions** (Q1-Q4 above)
2. **Agree on Week 1 priorities** (recommend: 1.1 + 1.2 + 2.1)
3. **Decide mount locking timeline** (proactive vs reactive)
4. **Begin implementation** of agreed tasks

---

**Status:** Updated with completions
**Last updated:** 2025-10-09
