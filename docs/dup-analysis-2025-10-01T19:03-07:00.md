# Cross-Medium Duplication Analysis
**Date:** 2025-10-01T19:03-07:00  
**Database:** copyjob (PostgreSQL 17)  
**Scope:** 3.4M duplicated blobs, 21M inodes, 4 media with cross-medium sharing

---

## Executive Summary

Analyzed cross-medium blob sharing patterns using three complementary approaches:
1. **n_hardlinks distribution** - Understanding blob duplication frequency
2. **Shared storage matrix** - Quantifying storage impact between media pairs
3. **Jaccard similarity** - Measuring content similarity for clustering

**Key Finding:** Only **4 media** contain duplicated files, arranged in a **star topology** with medium `236d` (1.2M blobs) as the hub. Two medium pairs account for **188 GB** of deduplication savings.

---

## 1. Methodology & Code

### 1.1 Data Preparation

Created materialized view of blob-medium relationships:

```sql
-- Materialize blob_media relationships for analysis
CREATE TABLE blob_media_matrix AS
SELECT DISTINCT i.hash, i.medium_hash, i.size
FROM inode i
JOIN blobs b ON i.hash = b.blobid
WHERE i.copied = true
  AND i.fs_type = 'f'
  AND b.n_hardlinks > 1;

CREATE INDEX ON blob_media_matrix(hash);
CREATE INDEX ON blob_media_matrix(medium_hash);

-- CRITICAL: Analyze to collect statistics (queries timeout without this!)
ANALYZE blob_media_matrix;
```

**Result:** 1,603,496 rows (1.5M unique hashes, 4 distinct media)

**Performance Note:** Initial queries timed out due to missing statistics. After `ANALYZE`, queries completed in seconds. This demonstrates PostgreSQL's dependency on current statistics for query planning.

### 1.2 N-Hardlinks Distribution Analysis

**Goal:** Understand blob duplication frequency patterns

```sql
-- Logarithmic histogram of blob duplication
SELECT
  pow(2, floor(log(2, n_hardlinks)))::int AS lower_bound,
  (pow(2, floor(log(2, n_hardlinks))+1)-1)::int AS upper_bound,
  COUNT(*) as num_blobs,
  SUM(n_hardlinks) as total_inodes,
  pg_size_pretty(SUM(size)) as total_size
FROM blobs
WHERE n_hardlinks > 1
GROUP BY 1, 2
ORDER BY 1;
```

**Why logarithmic bins?** Duplication follows a power-law distribution (many low-duplication blobs, few high-duplication). Linear bins would obscure patterns in the long tail.

### 1.3 Cross-Medium Sharing Matrix (Methods 1 & 2)

**Goal:** Count shared blobs and quantify storage impact between each medium pair

**Initial Approach (FAILED):**
```sql
-- Self-join approach - TIMED OUT even after ANALYZE
SELECT bm1.medium_hash AS medium_a,
       bm2.medium_hash AS medium_b,
       COUNT(*) AS shared_blob_count
FROM blob_media_matrix bm1
JOIN blob_media_matrix bm2 ON bm1.hash = bm2.hash
  AND bm1.medium_hash < bm2.medium_hash
GROUP BY 1, 2;
```

**Why it failed:** 1.6M × 1.6M self-join creates massive intermediate result, even with indexes.

**Successful Approach: INTERSECT (Gemini's recommendation):**
```bash
# For each of 6 medium pairs (4 choose 2), compute intersection
for pair in \
  "1d7c:236d" "1d7c:2ae4" "1d7c:c267" \
  "236d:2ae4" "236d:c267" "2ae4:c267"; do
  m1=$(echo $pair | cut -d: -f1)
  m2=$(echo $pair | cut -d: -f2)
  
  psql copyjob -c "
    SELECT '$m1' AS medium_a, '$m2' AS medium_b,
           COUNT(*) AS shared_blobs,
           pg_size_pretty(SUM(size)) AS shared_storage
    FROM (
      SELECT hash, size FROM blob_media_matrix WHERE medium_hash = '$m1'
      INTERSECT
      SELECT hash, size FROM blob_media_matrix WHERE medium_hash = '$m2'
    ) x;"
done
```

**Why INTERSECT works:** 
- Planner optimizes each subquery independently (index scan on medium_hash)
- INTERSECT operator is highly optimized for set operations
- No self-join cartesian product explosion

### 1.4 Jaccard Similarity Matrix (Method 3)

**Goal:** Normalize similarity scores for clustering analysis

**Formula:** `J(A,B) = |A∩B| / |A∪B|` where `|A∪B| = |A| + |B| - |A∩B|`

```bash
# Step 1: Count unique blobs per medium
for m in 1d7c 236d 2ae4 c267; do
  psql copyjob -c "
    SELECT '$m' AS medium, 
           COUNT(DISTINCT hash) AS unique_blobs 
    FROM blob_media_matrix 
    WHERE medium_hash = '$m';"
done

# Step 2: Compute Jaccard from intersection (already obtained) and union
python3 -c "
data = [
    ('236d ↔ 2ae4', 68808, 1226706 + 92099 - 68808),
    ('236d ↔ 1d7c', 29161, 1226706 + 42358 - 29161),
    # ... etc
]
for pair, inter, union in data:
    print(f'{pair}: {inter / union:.4f}')
"
```

**Interpretation of Jaccard scores:**
- **0.0-0.1:** Low similarity (different content)
- **0.1-0.3:** Moderate similarity (some shared patterns)
- **0.3-0.7:** High similarity (related content)
- **0.7-1.0:** Very high similarity (near-duplicates)

---

## 2. Detailed Results

### 2.1 N-Hardlinks Distribution

| Range | # Blobs | Total Inodes | % of Blobs | Interpretation |
|-------|---------|--------------|------------|----------------|
| **2-3** | **1,746,952** | 3,716,791 | **51.9%** | **Primary mode: simple pairs/triples** |
| 4-7 | 680,031 | 3,121,798 | 20.2% | Small sharing groups |
| 8-15 | 213,061 | 2,330,984 | 6.3% | Medium sharing |
| 16-31 | 129,718 | 2,947,286 | 3.9% | Growing distribution |
| **32-63** | **392,158** | **19,805,156** | **11.7%** | **Secondary mode: systemic duplication** |
| 64-127 | 146,082 | 14,097,275 | 4.3% | Wide sharing (system files?) |
| 128-255 | 41,214 | 7,456,682 | 1.2% | Very wide sharing |
| 256-511 | 13,378 | 4,586,462 | 0.4% | Extreme duplication |
| 512-1023 | 2,738 | 1,794,107 | 0.1% | Ultra-wide sharing |
| 1024-2047 | 310 | 412,521 | 0.009% | Massive duplication |
| **32768-65535** | **1** | **40,040** | **0.00003%** | **Single most-duplicated blob** |

**Key Insights:**
1. **Bimodal distribution** confirms two distinct duplication patterns:
   - **Incidental sharing** (2-7 copies): 72% of blobs - likely user files that happen to appear multiple times
   - **Systemic sharing** (32+ copies): 18% of blobs - likely OS files, dependencies, build artifacts

2. **Long tail is significant:** Only 0.5% of blobs have 256+ copies, but they account for 14.2M inodes (24% of all duplicated instances)

3. **The 40K blob:** One blob appears 40,040 times. Investigation needed - likely:
   - Empty file (0 bytes)
   - Common icon/favicon (tiny file)
   - Build system artifact (Makefile snippet, SVN metadata)

### 2.2 Cross-Medium Sharing Matrix

#### Per-Medium Profile

| Medium Hash (abbrev) | Unique Blobs | % of Total | Role |
|---------------------|--------------|------------|------|
| **236d5e0d** | **1,226,706** | **83.2%** | **Hub** |
| c2676ab2 | 242,333 | 16.4% | Secondary |
| 2ae4eb92 | 92,099 | 6.2% | Tertiary |
| 1d7c9dc8 | 42,358 | 2.9% | Satellite |

**Note:** Percentages don't sum to 100% due to blob overlap between media.

#### Pairwise Sharing (Sorted by Storage Impact)

| Medium A | Medium B | Shared Blobs | Shared Storage | % of A | % of B | Priority |
|----------|----------|--------------|----------------|--------|--------|----------|
| **236d** | **2ae4** | **68,808** | **100 GB** | 5.6% | **74.7%** | **🔥 Critical** |
| **236d** | **1d7c** | **29,161** | **88 GB** | 2.4% | **68.8%** | **🔥 Critical** |
| 236d | c267 | 31,444 | 8.7 GB | 2.6% | 13.0% | Medium |
| 1d7c | 2ae4 | 834 | 4.2 GB | 2.0% | 0.9% | Low |
| 2ae4 | c267 | 8 | 4.3 MB | 0.009% | 0.003% | Minimal |
| 1d7c | c267 | 2 | 0 bytes | 0.005% | 0.001% | Negligible |

**Critical Observations:**

1. **236d is the deduplication anchor:**
   - Contains 83% of all unique blobs
   - All significant sharing involves 236d
   - Removing 236d would eliminate most cross-medium dedup

2. **Asymmetric dependencies:**
   - **2ae4** is 75% dependent on 236d (68K/92K blobs shared)
   - **1d7c** is 69% dependent on 236d (29K/42K blobs shared)
   - But 236d is only 5.6% dependent on 2ae4, 2.4% on 1d7c
   
3. **Storage concentration:**
   - Just **2 pairs** (236d↔2ae4, 236d↔1d7c) account for **188 GB** of deduplication savings
   - Remaining 4 pairs contribute <13 GB combined

4. **c267 and 1d7c are nearly disjoint** (only 2 shared blobs) - suggests different content domains or time periods

### 2.3 Jaccard Similarity Matrix

| Pair | Intersection | Union | Jaccard | Similarity Level | Interpretation |
|------|--------------|-------|---------|------------------|----------------|
| **236d ↔ 2ae4** | 68,808 | 1,249,997 | **0.0550** | **Moderate** | Some shared content patterns |
| 236d ↔ 1d7c | 29,161 | 1,239,903 | 0.0235 | Low | Different content domains |
| 236d ↔ c267 | 31,444 | 1,437,595 | 0.0219 | Low | Different content domains |
| 1d7c ↔ 2ae4 | 834 | 133,623 | 0.0062 | Very Low | Minimal overlap |
| 236d ↔ 2ae4 | 8 | 334,424 | 0.000024 | Negligible | Essentially disjoint |
| 1d7c ↔ c267 | 2 | 284,689 | 0.000007 | Negligible | Essentially disjoint |

**Clustering Analysis:**

1. **No tight clusters detected:** 
   - Highest Jaccard score is only 5.5% (236d↔2ae4)
   - Typical "similar" media would show 20-40% Jaccard
   - These media represent largely independent content collections

2. **Weak hub-spoke pattern:**
   - 236d shows 2-5% similarity with all others
   - Non-hub pairs show <1% similarity
   - Suggests 236d is a "superset" or "master collection"

3. **Content independence:**
   - Low Jaccard scores indicate these media came from:
     - Different systems/users
     - Different time periods
     - Different purposes (backups vs live data vs archives)

### 2.4 Network Topology

**Graph representation** (nodes = media, edges = shared blobs):

```
        1d7c (42K)
         /|\
    29K / | \ 834
       /  |  \
   236d --+-- 2ae4 (92K)
  (1.2M)  |   /
     31K  |  / 69K
         \| /
        c267 (242K)
```

**Network metrics:**
- **Density:** 4/6 = 0.67 (4 significant edges out of 6 possible)
- **Diameter:** 2 (max distance between any two nodes via 236d)
- **Centrality:** 236d has degree 3, others have degree 1-2
- **Modularity:** Low (no distinct communities)

**Star topology characteristics:**
- **Hub (236d):** Controls deduplication flow
- **Spokes (others):** Depend on hub for cross-medium sharing
- **Failure impact:** Removing 236d fragments the network into 3 isolated nodes

---

## 3. Actionable Insights

### 3.1 Storage Optimization Priorities

**High Priority (>50 GB savings):**
1. ✅ **236d ↔ 2ae4:** Already achieving 100 GB deduplication
2. ✅ **236d ↔ 1d7c:** Already achieving 88 GB deduplication

**Medium Priority (5-50 GB):**
3. **236d ↔ c267:** 8.7 GB deduplication (consider if space-constrained)

**Low Priority (<5 GB):**
4. All other pairs: <5 GB combined savings

**Recommendation:** Current deduplication strategy is optimal. Focus on maintaining 236d integrity.

### 3.2 Data Management Strategy

**If 236d is a backup/archive:**
- ✅ Keep it - it's the deduplication anchor
- ⚠️ Ensure redundancy - losing it costs 188 GB+ in dedup savings
- 📊 Monitor it - 83% of unique content lives here

**If storage needs reduction:**
- ❌ Don't remove 236d (huge dedup loss)
- ✅ Consider consolidating 1d7c into 236d (69% redundant)
- ✅ Consider consolidating 2ae4 into 236d (75% redundant)

**If archiving old data:**
- c267 and 1d7c are independent - can archive separately
- 2ae4 depends heavily on 236d - archive together

### 3.3 Further Investigation

**High-value questions:**

1. **What is the 40K-copy blob?**
   ```sql
   SELECT blobid, size, pg_size_pretty(size) as size_pretty
   FROM blobs 
   WHERE n_hardlinks = 40040;
   
   -- Then find paths:
   SELECT p.path 
   FROM inode i 
   JOIN path p USING (medium_hash, ino)
   WHERE i.hash = '<that_blobid>' 
   LIMIT 10;
   ```

2. **What's in the 32-63 copy range?** (392K blobs, 19.8M inodes)
   ```sql
   -- Sample 100 blobs from this range
   SELECT b.blobid, b.size, b.n_hardlinks,
          array_agg(DISTINCT p.path) as sample_paths
   FROM blobs b
   JOIN inode i ON i.hash = b.blobid
   JOIN path p USING (medium_hash, ino)
   WHERE b.n_hardlinks BETWEEN 32 AND 63
   GROUP BY b.blobid, b.size, b.n_hardlinks
   ORDER BY RANDOM()
   LIMIT 100;
   ```

3. **What are the media identities?**
   ```sql
   -- Map medium_hash to human-readable names
   SELECT medium_hash, source_path 
   FROM medium 
   WHERE medium_hash IN (
     '236d5e0d89eb0e5e78edadf040a7a934',
     'c2676ab2865c5392b7d4745681ebe5b7',
     '2ae4eb92379b9892ba93693e49f42e08',
     '1d7c9dc81a26c871ccafc71ab284b4aa'
   );
   ```

4. **Temporal analysis:** Are these media from different time periods?
   ```sql
   SELECT i.medium_hash,
          date_trunc('year', to_timestamp(i.mtime)) as year,
          COUNT(*) as files
   FROM inode i
   WHERE i.medium_hash IN (...)
     AND i.copied = true
     AND i.fs_type = 'f'
   GROUP BY 1, 2
   ORDER BY 1, 2;
   ```

---

## 4. Visualization Plan: UpSetR

**Why UpSetR?** Traditional Venn diagrams fail with >3 sets. UpSetR excels at showing set intersections for multiple groups.

**Data export for UpSetR:**

```sql
-- Export blob-medium memberships
COPY (
  SELECT hash,
         MAX(CASE WHEN medium_hash = '1d7c9dc81a26c871ccafc71ab284b4aa' THEN 1 ELSE 0 END) as media_1d7c,
         MAX(CASE WHEN medium_hash = '236d5e0d89eb0e5e78edadf040a7a934' THEN 1 ELSE 0 END) as media_236d,
         MAX(CASE WHEN medium_hash = '2ae4eb92379b9892ba93693e49f42e08' THEN 1 ELSE 0 END) as media_2ae4,
         MAX(CASE WHEN medium_hash = 'c2676ab2865c5392b7d4745681ebe5b7' THEN 1 ELSE 0 END) as media_c267
  FROM blob_media_matrix
  GROUP BY hash
) TO '/tmp/upset_data.csv' WITH CSV HEADER;
```

**R script for UpSetR:**

```r
library(UpSetR)

# Load data
data <- read.csv('/tmp/upset_data.csv')

# Create UpSet plot
upset(data, 
      sets = c('media_1d7c', 'media_236d', 'media_2ae4', 'media_c267'),
      order.by = 'freq',
      text.scale = 1.5,
      mainbar.y.label = 'Blob Intersection Size',
      sets.x.label = 'Blobs per Medium')
```

**Expected visualization:** 
- Largest bar: blobs unique to 236d (~1.1M)
- Second bar: blobs shared by 236d+2ae4 (~69K)
- Third bar: blobs shared by 236d+1d7c (~29K)
- Shows combination patterns at-a-glance

---

## 5. Lessons Learned

### Performance
1. **ANALYZE is non-negotiable** for new tables - queries timeout without statistics
2. **INTERSECT > self-join** for set operations on large tables
3. **Logarithmic binning** reveals distribution modes in power-law data

### Analysis
1. **Jaccard similarity** normalizes for set size - essential for clustering
2. **Blob count ≠ storage impact** - must weight by size for business value
3. **Star topology** indicates centralized archive/backup pattern

### PostgreSQL
1. Partial indexes excel for filtered queries (`WHERE copied = true AND fs_type = 'f'`)
2. Index on both join columns (`hash`, `medium_hash`) critical for INTERSECT performance
3. Temporary tables + manual ANALYZE faster than complex CTEs for multi-step analysis

---

## 6. UpSet Analysis Results

**UpSet visualization** (text-based, as R/UpSetR not available):

### Intersection Pattern Distribution

Total unique blobs analyzed: **1,473,864**

| Pattern | Count | % | Visualization |
|---------|-------|---|---------------|
| **236d only** | **1,097,916** | **74.5%** | ████████████████████████████████████████████████████████████████████████ |
| **c267 only** | **210,889** | **14.3%** | ██████████████ |
| **236d + 2ae4** | **68,187** | **4.6%** | ████ |
| **236d + c267** | **31,436** | **2.1%** | ██ |
| **1d7c + 236d** | **28,546** | **1.9%** | █ |
| **2ae4 only** | **23,072** | **1.6%** | █ |
| **1d7c only** | **12,978** | **0.9%** | |
| 1d7c + 236d + 2ae4 | 613 | 0.04% | |
| 1d7c + 2ae4 | 219 | 0.01% | |
| 236d + 2ae4 + c267 | 6 | 0.0004% | |
| **All 4 media** | **2** | **0.0001%** | |

### Key Findings

1. **Massive uniqueness to 236d:**
   - 74.5% of blobs (1.1M) exist ONLY on 236d
   - 236d is not just a hub - it's a **master archive**

2. **c267 is largely independent:**
   - 14.3% of blobs (211K) exist ONLY on c267
   - Minimal overlap with other media (except 236d)

3. **Cross-medium sharing is rare:**
   - Only 4.6% shared between 236d+2ae4
   - Only 1.9% shared between 1d7c+236d
   - 3-way and 4-way intersections are negligible (<0.05%)

4. **The 2 universal blobs:**
   - Only 2 blobs appear on ALL 4 media
   - Likely system files (e.g., empty file, common config)

### UpSet Interpretation

The pattern distribution confirms the **star topology** findings:
- **236d** is the dominant collection (83% of all blobs touch it)
- **c267** is a secondary independent collection (16% total)
- **2ae4** and **1d7c** are small satellites (6% and 3%)
- **Cross-medium sharing is minimal** - these are distinct content collections with limited overlap

### What This Means for Deduplication

**Current state:**
- ✅ System is already achieving maximum dedup potential
- ✅ 236d+2ae4 sharing: 68K blobs (100 GB saved)
- ✅ 236d+1d7c sharing: 29K blobs (88 GB saved)

**Why more dedup isn't happening:**
- 74.5% of content is unique to one medium (no dedup opportunity)
- Only 6.6% of content appears on 2+ media (dedup is working on this)
- No significant 3+ way sharing exists (by design or by data characteristics)

**Recommendation:** 
Focus on **data management** not dedup optimization. The system is already deduping everything it can. The question is whether 236d's 1.1M unique blobs need to be retained or can be archived/deleted.

---

## 7. Final Recommendations

### Immediate Actions

1. **Identify the 2 universal blobs:**
   ```sql
   SELECT i.hash, i.size, pg_size_pretty(i.size),
          array_agg(DISTINCT p.path) as sample_paths
   FROM inode i
   JOIN path p USING (medium_hash, ino)
   WHERE i.hash IN (
     SELECT hash FROM (
       SELECT hash, 
              SUM(CASE WHEN medium_hash = '1d7c9dc81a26c871ccafc71ab284b4aa' THEN 1 ELSE 0 END) +
              SUM(CASE WHEN medium_hash = '236d5e0d89eb0e5e78edadf040a7a934' THEN 1 ELSE 0 END) +
              SUM(CASE WHEN medium_hash = '2ae4eb92379b9892ba93693e49f42e08' THEN 1 ELSE 0 END) +
              SUM(CASE WHEN medium_hash = 'c2676ab2865c5392b7d4745681ebe5b7' THEN 1 ELSE 0 END) as media_count
       FROM blob_media_matrix
       GROUP BY hash
       HAVING media_count = 4
     ) x
   )
   GROUP BY i.hash, i.size;
   ```

2. **Map medium hashes to human names:**
   ```sql
   SELECT medium_hash, source_path, 
          enumeration_date, 
          file_count, 
          total_size
   FROM medium 
   WHERE medium_hash IN (
     '236d5e0d89eb0e5e78edadf040a7a934',
     'c2676ab2865c5392b7d4745681ebe5b7',
     '2ae4eb92379b9892ba93693e49f42e08',
     '1d7c9dc81a26c871ccafc71ab284b4aa'
   );
   ```

3. **Investigate 236d's 1.1M unique blobs:**
   - Are they archival (old backups)?
   - Are they operational (live data)?
   - Can they be tiered/compressed?

### Long-term Strategy

**If 236d is archival data:**
- ✅ Keep it - it's the foundation of dedup
- ⚠️ Ensure redundancy (RAID, backups)
- 📊 Monitor growth - 74% uniqueness is high

**If storage reduction needed:**
- ❌ Don't delete 236d (loses 188 GB dedup savings)
- ✅ Consider archiving c267 (14% unique, minimal sharing)
- ✅ Consider consolidating 1d7c into 236d (69% overlap)

**For future ingestion:**
- 📌 New media will likely follow same pattern (low Jaccard, star topology)
- 📌 Expect 70-80% new content, 20-30% dedup from 236d
- 📌 Cross-medium sharing will remain low unless data sources change

---

## Appendix A: Performance Notes

### Critical Lessons

1. **ANALYZE is mandatory** for newly created tables
   - All queries timed out until we ran `ANALYZE blob_media_matrix`
   - PostgreSQL planner is helpless without statistics
   - This cost us 30+ minutes of debugging

2. **INTERSECT >> Self-join** for set operations
   - Self-join on 1.6M rows: timeout
   - INTERSECT on same data: <1 second
   - Planner optimizes INTERSECT better than JOIN

3. **GROUP BY struggles** without proper indexing
   - Even with indexes, 1.6M row GROUP BY timed out
   - Python post-processing was faster than SQL aggregation
   - Sometimes ETL (extract to CSV, process externally) wins

### Index Strategy

**What worked:**
- `CREATE INDEX ON blob_media_matrix(hash)`
- `CREATE INDEX ON blob_media_matrix(medium_hash)`
- Individual queries using these indexes

**What didn't work:**
- Self-joins even with both indexes
- GROUP BY even with index on grouping column
- DISTINCT ON with ORDER BY

**Lesson:** Indexes help individual operations but don't overcome algorithmic complexity (O(n²) joins, expensive aggregations).

---

## Appendix B: Code Repository

All analysis code is available in this document. Key scripts:

1. **Data preparation:** Section 1.1
2. **n_hardlinks analysis:** Section 1.2
3. **INTERSECT-based matrix:** Section 1.3
4. **Jaccard calculation:** Section 1.4
5. **UpSet data export:** Section 6

To reproduce analysis:
```bash
# 1. Create blob_media_matrix (see Section 1.1)
# 2. Run n_hardlinks query (see Section 1.2)
# 3. Run INTERSECT loop (see Section 1.3)
# 4. Calculate Jaccard (see Section 1.4)
# 5. Export UpSet data (see Section 6)
```

---

**Analysis completed:** 2025-10-01T19:03-07:00  
**Total analysis time:** ~2 hours  
**Key blocker:** Missing ANALYZE (30 min lost)  
**Key insight:** Star topology with 236d as master archive
