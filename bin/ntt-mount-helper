#!/usr/bin/env bash
# Author: PB and Claude
# Date: 2025-10-05
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# ntt/bin/ntt-mount-helper
#
# Sudo wrapper for mounting/unmounting NTT disk images
# Validates inputs and uses standard mount paths

set -euo pipefail

usage() {
  cat <<EOF
Usage:
  $0 mount <medium_hash> <image_path>    Mount image to /mnt/ntt/<medium_hash>
  $0 unmount <medium_hash>               Unmount and detach loop device
  $0 status <medium_hash>                Check if mounted (exit 0=yes, 1=no)

Security:
  - Only mounts to /mnt/ntt/<hash> (no arbitrary paths)
  - Validates medium_hash format (hex only)
  - Read-only mounts with nosuid,nodev,noatime
EOF
  exit 1
}

# Validate medium hash (32-64 hex chars)
validate_hash() {
  local hash="$1"
  if [[ ! "$hash" =~ ^[a-f0-9]{16,64}$ ]]; then
    echo "Error: Invalid medium_hash format (must be 16-64 hex chars)" >&2
    exit 1
  fi
}

# Extract LVM metadata from partition device
# Returns JSON string with LVM metadata or empty if not LVM
extract_lvm_metadata() {
  local part_dev="$1"

  # Check if this is an LVM member
  local lvm_type
  lvm_type=$(blkid -o value -s TYPE "$part_dev" 2>/dev/null || echo "")

  if [[ "$lvm_type" != "LVM1_member" ]] && [[ "$lvm_type" != "LVM2_member" ]]; then
    return 1
  fi

  # Extract PV UUID
  local pv_uuid
  pv_uuid=$(blkid -o value -s UUID "$part_dev" 2>/dev/null || echo "")

  # Extract VG name from raw LVM metadata
  # LVM1 and LVM2 store VG name in metadata at start of PV
  # Look for repeated alphanumeric strings that look like VG names
  local vg_name=""
  local vg_candidates
  vg_candidates=$(dd if="$part_dev" bs=4096 count=16 2>/dev/null | strings | grep -E "^[a-zA-Z0-9_-]{2,32}$" | sort | uniq -c | sort -rn | head -5)

  # VG name appears many times in metadata (highest frequency wins)
  vg_name=$(echo "$vg_candidates" | head -1 | awk '{print $2}')

  # Extract LV names (look for /dev/VG/LV patterns)
  local lv_names=()
  if [[ -n "$vg_name" ]]; then
    while IFS= read -r lv_path; do
      # Extract just the LV name from /dev/VG/LV
      local lv_name="${lv_path##*/}"
      [[ -n "$lv_name" ]] && lv_names+=("$lv_name")
    done < <(dd if="$part_dev" bs=4096 count=64 2>/dev/null | strings | grep -E "^/dev/$vg_name/[a-zA-Z0-9_-]+" | sort -u)
  fi

  # Build JSON output
  local lv_list=""
  if [[ ${#lv_names[@]} -gt 0 ]]; then
    # Properly format JSON array with quoted elements
    local lv_json=""
    for lv in "${lv_names[@]}"; do
      [[ -n "$lv_json" ]] && lv_json+=","
      lv_json+="\"$lv\""
    done
    lv_list="[$lv_json]"
  else
    lv_list="[]"
  fi

  echo "{\"lvm_type\":\"$lvm_type\",\"pv_uuid\":\"$pv_uuid\",\"vg_name\":\"$vg_name\",\"lv_names\":$lv_list}"
  return 0
}

# Assemble RAID array from partition device
# Returns md device path on success, empty string on failure
assemble_raid_array() {
  local part_dev="$1"
  local part_num="$2"

  # Check if mdadm is available
  if ! command -v mdadm >/dev/null 2>&1; then
    echo "  Warning: mdadm not found, cannot assemble RAID arrays" >&2
    return 1
  fi

  # Examine RAID metadata
  local raid_info
  raid_info=$(mdadm --examine "$part_dev" 2>/dev/null)
  if [[ -z "$raid_info" ]]; then
    echo "  Warning: No valid RAID metadata on $part_dev" >&2
    return 1
  fi

  # Extract UUID for array identification
  local raid_uuid
  raid_uuid=$(echo "$raid_info" | grep "Array UUID" | awk '{print $4}')

  # Check if array is already assembled
  local existing_md
  existing_md=$(mdadm --detail --scan 2>/dev/null | grep "$raid_uuid" | awk '{print $2}' | head -1)
  if [[ -n "$existing_md" ]]; then
    echo "  RAID array already assembled: $existing_md" >&2
    echo "$existing_md"
    return 0
  fi

  # Assemble array (read-only, run mode)
  # --run forces assembly even if degraded (single-device RAID1)
  local md_device
  md_device=$(mdadm --assemble --readonly --run --uuid="$raid_uuid" "$part_dev" 2>&1 | grep -o '/dev/md[0-9]*' | head -1)

  if [[ -n "$md_device" ]]; then
    echo "  Assembled RAID array: $md_device from $part_dev" >&2
    # Wait for device to be ready
    sleep 0.2
    echo "$md_device"
    return 0
  else
    echo "  Failed to assemble RAID from $part_dev" >&2
    return 1
  fi
}

# Stop all RAID arrays associated with a mount point
stop_raid_arrays() {
  local mount_point="$1"

  # Find all md devices mounted under this mount point
  local md_devices=()
  while IFS= read -r md_dev; do
    md_devices+=("$md_dev")
  done < <(findmnt -rn -o SOURCE | grep "^/dev/md" | sort -u)

  # Check each md device to see if it's mounted under our mount point
  for md_dev in "${md_devices[@]}"; do
    local md_mount
    md_mount=$(findmnt -n -o TARGET -S "$md_dev" 2>/dev/null || echo "")

    if [[ "$md_mount" =~ ^$mount_point ]]; then
      echo "  Stopping RAID array: $md_dev" >&2
      # mdadm --stop will fail if array is still mounted, which is fine
      # (we unmount first, then stop the array)
      if mdadm --stop "$md_dev" 2>/dev/null; then
        echo "  Stopped $md_dev" >&2
      else
        echo "  Warning: Could not stop $md_dev (may still be in use)" >&2
      fi
    fi
  done
}

# Get mount options for filesystem type
# ext4 needs 'noload' to skip journal recovery on readonly mounts
# ext3 needs 'norecovery' instead of 'noload' when journal is dirty
get_mount_options() {
  local fs_type="$1"
  local base_opts="ro,noatime,nodev,nosuid"

  if [[ "$fs_type" == "ext4" ]]; then
    # ext4: noload works reliably for read-only mounts
    echo "${base_opts},noload"
  elif [[ "$fs_type" == "ext3" ]]; then
    # ext3: use norecovery to handle dirty journals
    # norecovery allows read-only mount even with unclean shutdown
    echo "${base_opts},norecovery"
  elif [[ "$fs_type" == "ext2" ]]; then
    # ext2: no journal, no special options needed
    echo "$base_opts"
  else
    echo "$base_opts"
  fi
}

# Cleanup stale loop devices pointing to deleted inodes
cleanup_stale_loops() {
  local medium_hash="$1"
  local image_path="$2"
  local mount_point="/mnt/ntt/$medium_hash"

  # Find all loop devices for this image (including deleted inodes)
  # Format: /dev/loop29  0  0  0  1  /data/fast/img/HASH.img (deleted)  0  512
  local matching_loops
  matching_loops=$(losetup -l | grep "$(basename "$image_path")" || true)

  if [[ -n "$matching_loops" ]]; then
    echo "$matching_loops" | while read -r line; do
      local loop_dev=$(echo "$line" | awk '{print $1}')
      local status=$(echo "$line" | grep -o "(deleted)" || echo "")

      # Only cleanup deleted inodes (active image is fine)
      if [[ -n "$status" ]]; then
        echo "Cleaning up stale loop device: $loop_dev (deleted inode)" >&2

        # Try to unmount if it's mounted at our mount point
        if findmnt -S "$loop_dev" "$mount_point" >/dev/null 2>&1; then
          echo "  Unmounting $loop_dev from $mount_point" >&2
          umount "$mount_point" 2>/dev/null || echo "  Warning: umount failed" >&2
        fi

        # Detach loop device
        if losetup -d "$loop_dev" 2>/dev/null; then
          echo "  Detached $loop_dev" >&2
        else
          echo "  Warning: Could not detach $loop_dev (may be in use)" >&2
        fi
      fi
    done
  fi
}

# Mount command
do_mount() {
  local medium_hash="$1"
  local image_path="$2"

  validate_hash "$medium_hash"

  if [[ ! -f "$image_path" ]]; then
    echo "Error: Image file not found: $image_path" >&2
    exit 1
  fi

  local mount_point="/mnt/ntt/$medium_hash"

  # Check if already mounted
  if findmnt "$mount_point" >/dev/null 2>&1; then
    echo "Already mounted at $mount_point"
    exit 0
  fi

  # Cleanup stale loop devices before mounting
  cleanup_stale_loops "$medium_hash" "$image_path"

  # Create mount point
  mkdir -p "$mount_point"

  # Create loop device (read-only, with partition scanning)
  local loop_device
  loop_device=$(losetup -f --show -r -P "$image_path")
  if [[ -z "$loop_device" ]]; then
    rmdir "$mount_point" 2>/dev/null || true
    echo "Error: Failed to create loop device for $image_path" >&2
    exit 1
  fi

  # Wait briefly for kernel to create partition devices
  sleep 0.2

  # Check for partition devices (multi-partition disk)
  local partition_devices=()
  local has_partitions=false

  # Explicitly find partition devices
  while IFS= read -r part_dev; do
    partition_devices+=("$part_dev")
    has_partitions=true
  done < <(ls "${loop_device}"p* 2>/dev/null || true)

  if [[ "$has_partitions" == "true" ]]; then
    # Multi-partition disk detected - but may only have one mountable partition
    echo "Multi-partition disk detected" >&2

    # Check for APM hybrid ISO/HFS+ discs (Mac optical media)
    # These have Apple Partition Map with unmountable APM metadata in p1,
    # but whole-disk has ISO9660 filesystem overlay
    local pttype
    pttype=$(blkid -o value -s PTTYPE "$loop_device" 2>/dev/null || echo "")
    local whole_disk_fs
    whole_disk_fs=$(blkid -o value -s TYPE "$loop_device" 2>/dev/null || echo "")

    if [[ "$pttype" == "mac" ]] && [[ "$whole_disk_fs" =~ ^(iso9660|udf)$ ]]; then
      echo "  APM hybrid $whole_disk_fs disc detected - mounting whole disk" >&2
      local mount_opts
      mount_opts=$(get_mount_options "$whole_disk_fs")

      if mount -t "$whole_disk_fs" -o "$mount_opts" "$loop_device" "$mount_point" 2>/dev/null; then
        echo "Mounted $loop_device at $mount_point (APM hybrid, fs_type: $whole_disk_fs)" >&2
        echo '{"layout":"single","device":"'"$loop_device"'","mount":"'"$mount_point"'","fstype":"'"$whole_disk_fs"'","hybrid":"apm"}'
        exit 0
      else
        echo "  Warning: Whole-disk $whole_disk_fs mount failed, trying partitions..." >&2
      fi
    fi

    # Check if any partitions are RAID members before running mdadm
    local has_raid=false
    for part_dev in "${partition_devices[@]}"; do
      local part_type
      part_type=$(blkid -o value -s TYPE "$part_dev" 2>/dev/null || echo "")
      if [[ "$part_type" == "linux_raid_member" ]]; then
        has_raid=true
        break
      fi
    done

    # Only run mdadm if we found RAID members on this specific disk
    if [[ "$has_raid" == "true" ]] && command -v mdadm >/dev/null 2>&1; then
      # Load RAID1 kernel module if not already loaded
      modprobe raid1 2>/dev/null || true

      echo "RAID members detected, assembling arrays..." >&2

      # Assemble arrays from specific partition devices only (not system-wide scan)
      for part_dev in "${partition_devices[@]}"; do
        local part_type
        part_type=$(blkid -o value -s TYPE "$part_dev" 2>/dev/null || echo "")

        # Only try to assemble RAID members
        if [[ "$part_type" == "linux_raid_member" ]]; then
          echo "  Checking $part_dev for RAID metadata..." >&2
          # Try to assemble this specific device (--run allows degraded/single-device RAID1)
          mdadm --assemble --run "$part_dev" 2>/dev/null || true
        fi
      done

      # Activate any inactive arrays that were assembled
      for md_dev in /dev/md[0-9]* /dev/md/*; do
        [[ -b "$md_dev" ]] || [[ -L "$md_dev" && -b "$(readlink -f "$md_dev")" ]] || continue
        # Check if array is inactive in /proc/mdstat
        if grep -q "$(basename "$md_dev").*inactive" /proc/mdstat 2>/dev/null; then
          echo "  Activating inactive array: $md_dev" >&2
          mdadm --run "$md_dev" 2>/dev/null || echo "  Warning: Could not activate $md_dev" >&2
        fi
      done
    fi

    # Count mountable partitions to decide mount structure
    # Single mountable partition → mount at base path (no /p1 subdirectory)
    # Multiple mountable partitions → mount at /p1, /p2 subdirectories
    local mountable_count=0

    # Count regular partitions (skip extended containers, RAID members, LVM members)
    for part_dev in "${partition_devices[@]}"; do
      local part_type
      part_type=$(blkid -o value -s TYPE "$part_dev" 2>/dev/null || echo "")

      # Skip extended partition containers (no TYPE)
      [[ -z "$part_type" ]] && continue

      # Skip RAID members (will be counted via md devices)
      [[ "$part_type" == "linux_raid_member" ]] && continue

      # Skip LVM members (will be handled by LVM activation)
      [[ "$part_type" == "LVM1_member" ]] && continue
      [[ "$part_type" == "LVM2_member" ]] && continue

      ((mountable_count++)) || true
    done

    # Count assembled RAID arrays
    if command -v mdadm >/dev/null 2>&1; then
      for md_dev in /dev/md[0-9]* /dev/md/*; do
        # Check if device exists (block device or symlink to block device)
        [[ -b "$md_dev" ]] || [[ -L "$md_dev" && -b "$(readlink -f "$md_dev")" ]] || continue

        # Check if it has a filesystem
        local md_fs_type
        md_fs_type=$(blkid -o value -s TYPE "$md_dev" 2>/dev/null || echo "")
        [[ -n "$md_fs_type" ]] && { ((mountable_count++)) || true; }
      done
    fi

    echo "  Found $mountable_count mountable partition(s)" >&2

    # Decide mount structure based on mountable partition count
    local use_base_path=false
    if [[ $mountable_count -eq 1 ]]; then
      echo "  Single mountable partition - will mount at base path" >&2
      use_base_path=true
    else
      echo "  Multiple mountable partitions - will use /p{N} subdirectories" >&2
      use_base_path=false
    fi

    local partition_count=0
    local mounted_partitions=()

    for part_dev in "${partition_devices[@]}"; do
      # Extract partition number (e.g., /dev/loop0p1 → 1)
      local part_num="${part_dev##*p}"

      # Skip extended partition containers (blkid returns PTTYPE instead of TYPE)
      local part_type
      part_type=$(blkid -o value -s TYPE "$part_dev" 2>/dev/null || echo "")
      if [[ -z "$part_type" ]]; then
        echo "  Skipping $part_dev (extended partition container)" >&2
        continue
      fi

      # Skip RAID members - they're handled by mdadm --scan above
      # The resulting md devices will be mounted later
      if [[ "$part_type" == "linux_raid_member" ]]; then
        echo "  Skipping $part_dev (RAID member, will mount assembled array)" >&2
        continue
      fi

      # Handle LVM members - extract metadata and skip mounting
      if [[ "$part_type" == "LVM1_member" ]] || [[ "$part_type" == "LVM2_member" ]]; then
        echo "  Detecting LVM member: $part_dev" >&2
        local lvm_metadata
        lvm_metadata=$(extract_lvm_metadata "$part_dev" 2>/dev/null || echo "")

        if [[ -n "$lvm_metadata" ]]; then
          echo "  LVM metadata extracted: $lvm_metadata" >&2
          mounted_partitions+=("$part_num:$part_dev::$part_type:lvm_detected:$lvm_metadata")
        else
          echo "  Warning: Failed to extract LVM metadata from $part_dev" >&2
          mounted_partitions+=("$part_num:$part_dev::$part_type:lvm_no_metadata")
        fi
        continue
      fi

      # Skip swap partitions (can't be mounted as filesystems)
      if [[ "$part_type" == "swap" ]]; then
        echo "  Skipping $part_dev (swap partition)" >&2
        continue
      fi

      # Validate partition geometry before attempting mount
      # Check dmesg for kernel truncation warnings (partition extends beyond EOF)
      # This prevents mount from hanging in D-state on corrupted/truncated partitions
      local loop_name
      loop_name=$(basename "$loop_device")

      # dmesg shows: "loop23: p2 size 19535040 extends beyond EOD, truncated"
      # Search ALL dmesg (not just tail -100) to catch warnings even if they've scrolled
      # This is critical for workers that mount the disk long after orchestrator created it
      if sudo dmesg 2>/dev/null | grep "$loop_name:" | grep -q "p$part_num.*beyond EOD.*truncated"; then
        echo "  Skipping $part_dev (partition extends beyond device end - kernel truncated)" >&2
        mounted_partitions+=("$part_num:$part_dev::$part_type:truncated")
        continue
      fi

      # Create partition mount point
      local part_mount
      if [[ "$use_base_path" == "true" ]]; then
        # Single mountable partition - use base path
        part_mount="$mount_point"
      else
        # Multiple partitions - use /p{N} subdirectory
        part_mount="$mount_point/p$part_num"
      fi
      mkdir -p "$part_mount"

      # Get appropriate mount options for this filesystem type
      local mount_opts
      mount_opts=$(get_mount_options "$part_type")

      # Try to mount this partition - capture errors for diagnostics
      # Use timeout with KILL signal to prevent hangs on corrupted/truncated partitions
      # Note: Mount can enter uninterruptible sleep (D state) which requires SIGKILL
      local mount_error
      mount_error=$(timeout -s KILL 10 mount -t "$part_type" -o "$mount_opts" "$part_dev" "$part_mount" 2>&1)
      local mount_status=$?

      # timeout returns 124 on timeout, 137 when SIGKILL used
      if [[ $mount_status -eq 124 ]] || [[ $mount_status -eq 137 ]]; then
        mount_error="Mount command timed out after 10 seconds (likely corrupted/truncated partition)"
      fi

      if [[ $mount_status -eq 0 ]]; then
        echo "  Mounted $part_dev at $part_mount (fs_type: $part_type)" >&2
        mounted_partitions+=("$part_num:$part_dev:$part_mount:$part_type:ok")
        partition_count=$((partition_count + 1))
      else
        echo "  Failed to mount $part_dev (type: $part_type): $mount_error" >&2
        rmdir "$part_mount" 2>/dev/null || true
        mounted_partitions+=("$part_num:$part_dev::$part_type:failed")
      fi
    done

    # After trying partitions, also check for assembled RAID devices
    # mdadm --scan may have assembled arrays that weren't detected per-partition
    if command -v mdadm >/dev/null 2>&1; then
      local seen_md_devices=()

      for md_dev in /dev/md[0-9]* /dev/md/*; do
        [[ -b "$md_dev" ]] || [[ -L "$md_dev" && -b "$(readlink -f "$md_dev")" ]] || continue
        # Resolve symlinks
        md_dev=$(readlink -f "$md_dev")

        # Skip if already processed (dedup /dev/md5 and /dev/md/5)
        local already_seen=false
        for seen in "${seen_md_devices[@]}"; do
          if [[ "$seen" == "$md_dev" ]]; then
            already_seen=true
            break
          fi
        done
        [[ "$already_seen" == "true" ]] && continue
        seen_md_devices+=("$md_dev")

        # Skip if already mounted (might have been mounted in partition loop)
        local already_mounted=false
        for part_info in "${mounted_partitions[@]}"; do
          IFS=':' read -r num dev mnt fstype status <<< "$part_info"
          if [[ "$dev" == "$md_dev" ]]; then
            already_mounted=true
            break
          fi
        done
        [[ "$already_mounted" == "true" ]] && continue

        # Get filesystem type
        local md_fs_type
        md_fs_type=$(blkid -o value -s TYPE "$md_dev" 2>/dev/null || echo "")
        [[ -n "$md_fs_type" ]] || continue

        # Find a partition number to use (use next available)
        local md_part_num=$((partition_count + 1))

        # Create mount point
        local md_mount
        if [[ "$use_base_path" == "true" ]]; then
          # Single mountable partition - use base path
          md_mount="$mount_point"
        else
          # Multiple partitions - use /p{N} subdirectory
          md_mount="$mount_point/p$md_part_num"
        fi
        mkdir -p "$md_mount"

        # Get appropriate mount options for this filesystem type
        local md_mount_opts
        md_mount_opts=$(get_mount_options "$md_fs_type")

        # Try to mount - capture errors for diagnostics
        # Use timeout with KILL signal to prevent hangs on corrupted partitions
        local md_mount_error
        md_mount_error=$(timeout -s KILL 10 mount -t "$md_fs_type" -o "$md_mount_opts" "$md_dev" "$md_mount" 2>&1)
        local md_mount_status=$?

        # timeout returns 124 on timeout, 137 when SIGKILL used
        if [[ $md_mount_status -eq 124 ]] || [[ $md_mount_status -eq 137 ]]; then
          md_mount_error="Mount command timed out after 10 seconds (likely corrupted partition)"
        fi

        if [[ $md_mount_status -eq 0 ]]; then
          echo "  Mounted $md_dev at $md_mount (RAID, fs_type: $md_fs_type)" >&2
          mounted_partitions+=("$md_part_num:$md_dev:$md_mount:$md_fs_type:ok")
          partition_count=$((partition_count + 1))
        else
          echo "  Failed to mount $md_dev (type: $md_fs_type): $md_mount_error" >&2
          rmdir "$md_mount" 2>/dev/null || true
        fi
      done
    fi

    if [[ $partition_count -eq 0 ]] && [[ ${#mounted_partitions[@]} -eq 0 ]]; then
      # No partitions at all (not even LVM/truncated) - this is a real failure
      stop_raid_arrays "$mount_point"
      losetup -d "$loop_device" 2>/dev/null || true
      rmdir "$mount_point" 2>/dev/null || true
      echo "Error: No partitions could be mounted" >&2
      exit 1
    fi

    # If we have partition info (even if unmounted), output JSON
    # This captures LVM metadata, truncated partitions, etc.
    if [[ $partition_count -eq 0 ]]; then
      echo "Warning: No partitions mounted, but partition metadata captured" >&2
    fi

    # Output JSON for orchestrator
    local layout_type
    if [[ "$use_base_path" == "true" ]]; then
      layout_type="single"
    else
      layout_type="multi"
    fi

    echo "{\"layout\":\"$layout_type\",\"partitions\":["
    local first=true
    for part_info in "${mounted_partitions[@]}"; do
      # Parse all fields (lvm_metadata is optional 6th field)
      IFS=':' read -r num dev mnt fstype status lvm_metadata <<< "$part_info"
      [[ "$first" == "true" ]] || echo ','
      first=false

      # Build JSON with LVM metadata if present
      if [[ -n "$lvm_metadata" ]] && [[ "$lvm_metadata" != "" ]]; then
        echo -n "{\"num\":$num,\"device\":\"$dev\",\"mount\":\"$mnt\",\"fstype\":\"$fstype\",\"status\":\"$status\",\"lvm\":$lvm_metadata}"
      else
        echo -n "{\"num\":$num,\"device\":\"$dev\",\"mount\":\"$mnt\",\"fstype\":\"$fstype\",\"status\":\"$status\"}"
      fi
    done
    echo ']}'

    exit 0
  else
    # Single-partition or whole-disk filesystem
    echo "Single-partition disk detected" >&2
    local fs_type
    fs_type=$(blkid -o value -s TYPE "$loop_device" 2>/dev/null || echo "")

    # Mount with detected filesystem type (or auto-detect)
    if [[ -n "$fs_type" ]]; then
      local mount_opts
      mount_opts=$(get_mount_options "$fs_type")
      if mount -t "$fs_type" -o "$mount_opts" "$loop_device" "$mount_point" 2>/dev/null; then
        echo "Mounted $loop_device at $mount_point (fs_type: $fs_type)" >&2
        echo '{"layout":"single","device":"'"$loop_device"'","mount":"'"$mount_point"'","fstype":"'"$fs_type"'"}'
        exit 0
      fi
    fi

    # Fallback: try auto-detect
    if mount -o ro,noatime,nodev,nosuid "$loop_device" "$mount_point" 2>/dev/null; then
      echo "Mounted $loop_device at $mount_point (auto-detected fs_type)" >&2
      echo '{"layout":"single","device":"'"$loop_device"'","mount":"'"$mount_point"'","fstype":"auto"}'
      exit 0
    fi

    # Optical media fallback: ISO9660/UDF bridge format
    # Many optical discs report "udf" via blkid but mount as "iso9660" (or vice versa)
    if [[ "$fs_type" == "udf" ]]; then
      echo "UDF mount failed, trying ISO9660 (common for bridge-format optical discs)..." >&2
      local iso_opts
      iso_opts=$(get_mount_options "iso9660")
      if mount -t iso9660 -o "$iso_opts" "$loop_device" "$mount_point" 2>/dev/null; then
        echo "Mounted $loop_device at $mount_point (fs_type: iso9660)" >&2
        echo '{"layout":"single","device":"'"$loop_device"'","mount":"'"$mount_point"'","fstype":"iso9660"}'
        exit 0
      fi
    elif [[ "$fs_type" == "iso9660" ]]; then
      echo "ISO9660 mount failed, trying UDF..." >&2
      local udf_opts
      udf_opts=$(get_mount_options "udf")
      if mount -t udf -o "$udf_opts" "$loop_device" "$mount_point" 2>/dev/null; then
        echo "Mounted $loop_device at $mount_point (fs_type: udf)" >&2
        echo '{"layout":"single","device":"'"$loop_device"'","mount":"'"$mount_point"'","fstype":"udf"}'
        exit 0
      fi
    fi

    # Zip disk special case: filesystem may start at offset 16384 bytes (sector 32)
    # Zip disks (100MB/250MB/750MB Iomega removable media) often have FAT16/FAT32
    # filesystems starting at 0x4000 instead of sector 0
    echo "Standard mount failed, trying Zip disk offset (16384 bytes)..." >&2

    # Create offset loop device (mount doesn't support offset= for loop devices)
    local zip_loop_device
    zip_loop_device=$(losetup -f --show -r -o 16384 "$image_path" 2>/dev/null)

    if [[ -n "$zip_loop_device" ]]; then
      # Wait for device to be ready
      sleep 0.1

      # Detect filesystem type at offset
      local zip_fs_type
      zip_fs_type=$(blkid -o value -s TYPE "$zip_loop_device" 2>/dev/null || echo "")

      # Try mounting with detected type or auto-detect
      local mount_success=false
      if [[ -n "$zip_fs_type" ]]; then
        local zip_mount_opts
        zip_mount_opts=$(get_mount_options "$zip_fs_type")
        if mount -t "$zip_fs_type" -o "$zip_mount_opts" "$zip_loop_device" "$mount_point" 2>/dev/null; then
          echo "Mounted $zip_loop_device at $mount_point (Zip disk offset=16384, fs_type=$zip_fs_type)" >&2
          echo '{"layout":"single","device":"'"$zip_loop_device"'","mount":"'"$mount_point"'","fstype":"'"$zip_fs_type"'","offset":16384}'
          mount_success=true
        fi
      fi

      # Fallback: try auto-detect
      if [[ "$mount_success" == "false" ]]; then
        if mount -o ro,noatime,nodev,nosuid "$zip_loop_device" "$mount_point" 2>/dev/null; then
          echo "Mounted $zip_loop_device at $mount_point (Zip disk offset=16384, auto-detected)" >&2
          echo '{"layout":"single","device":"'"$zip_loop_device"'","mount":"'"$mount_point"'","fstype":"auto","offset":16384}'
          mount_success=true
        fi
      fi

      if [[ "$mount_success" == "true" ]]; then
        # Successfully mounted - detach original loop device (offset device is now in use)
        losetup -d "$loop_device" 2>/dev/null || true
        exit 0
      else
        # Mount failed - cleanup offset device
        losetup -d "$zip_loop_device" 2>/dev/null || true
        echo "Failed to mount Zip disk at offset 16384 (fs_type: ${zip_fs_type:-unknown})" >&2
      fi
    fi

    # Mount failed - cleanup
    losetup -d "$loop_device" 2>/dev/null || true
    rmdir "$mount_point" 2>/dev/null || true
    echo "Error: Failed to mount $loop_device" >&2
    exit 1
  fi
}

# Unmount command
do_unmount() {
  local medium_hash="$1"

  validate_hash "$medium_hash"

  local mount_point="/mnt/ntt/$medium_hash"

  # Check if base mount point exists
  if [[ ! -d "$mount_point" ]]; then
    echo "Not mounted: $mount_point (directory does not exist)"
    exit 0
  fi

  # Detect if this is a multi-partition mount (has p{N} subdirectories)
  local partition_mounts=("$mount_point"/p*)
  local has_partition_mounts=false
  local loop_device=""

  if [[ -d "${partition_mounts[0]}" ]]; then
    has_partition_mounts=true
  fi

  if [[ "$has_partition_mounts" == "true" ]]; then
    # Multi-partition disk - unmount each partition
    echo "Multi-partition disk detected, unmounting all partitions" >&2

    for part_mount in "${partition_mounts[@]}"; do
      if findmnt "$part_mount" >/dev/null 2>&1; then
        # Get loop device from first partition (all share same loop device)
        if [[ -z "$loop_device" ]]; then
          loop_device=$(findmnt -n -o SOURCE "$part_mount" || echo "")
          # Extract base loop device (e.g., /dev/loop0p1 → /dev/loop0)
          loop_device="${loop_device%p*}"
        fi

        if umount "$part_mount" 2>/dev/null; then
          echo "  Unmounted $part_mount" >&2
        else
          echo "  Warning: Failed to unmount $part_mount" >&2
        fi
      fi

      # Remove partition mount point
      rmdir "$part_mount" 2>/dev/null || true
    done
  else
    # Single-partition mount
    if findmnt "$mount_point" >/dev/null 2>&1; then
      # Get loop device before unmounting
      loop_device=$(findmnt -n -o SOURCE "$mount_point" || echo "")

      # Unmount
      if umount "$mount_point" 2>/dev/null; then
        echo "Unmounted $mount_point"
      else
        echo "Warning: Failed to unmount $mount_point" >&2
      fi
    else
      echo "Not mounted: $mount_point"
    fi
  fi

  # Stop any RAID arrays that were mounted under this mount point
  stop_raid_arrays "$mount_point"

  # Detach ALL loop devices for this medium (not just the mounted one)
  # This handles orphaned devices from failed mount attempts
  if [[ -n "$loop_device" ]] && [[ "$loop_device" =~ ^/dev/loop ]]; then
    # Get the base loop device name
    local base_loop_device="${loop_device%p*}"

    # Find the image file path from the loop device
    # Extract only field 6 (BACK-FILE column) from losetup -l output
    local image_path
    image_path=$(losetup -l | grep "^$base_loop_device" | awk '{print $6}' | sed 's/(deleted)$//')

    if [[ -n "$image_path" ]]; then
      # Find ALL loop devices pointing to this image file
      local all_loop_devices
      all_loop_devices=$(losetup -l | grep -F "$image_path" | awk '{print $1}')

      if [[ -n "$all_loop_devices" ]]; then
        local device_count=$(echo "$all_loop_devices" | wc -l)
        echo "Found $device_count loop device(s) for this image, detaching all..." >&2

        while IFS= read -r loop_dev; do
          if losetup -d "$loop_dev" 2>/dev/null; then
            echo "  Detached $loop_dev" >&2
          else
            echo "  Warning: Could not detach $loop_dev" >&2
          fi
        done <<< "$all_loop_devices"
      fi
    else
      # Fallback: just detach the one we found
      if losetup -d "$base_loop_device" 2>/dev/null; then
        echo "Detached loop device $base_loop_device"
      else
        echo "Warning: Failed to detach $base_loop_device" >&2
      fi
    fi
  fi

  # Remove base mount point
  rmdir "$mount_point" 2>/dev/null || true

  exit 0
}

# Status command
do_status() {
  local medium_hash="$1"

  validate_hash "$medium_hash"

  local mount_point="/mnt/ntt/$medium_hash"

  if findmnt "$mount_point" >/dev/null 2>&1; then
    echo "Mounted at $mount_point"
    exit 0
  else
    echo "Not mounted: $mount_point"
    exit 1
  fi
}

# Main dispatch
[[ $# -ge 1 ]] || usage

case "$1" in
  mount)
    [[ $# -eq 3 ]] || usage
    do_mount "$2" "$3"
    ;;
  unmount)
    [[ $# -eq 2 ]] || usage
    do_unmount "$2"
    ;;
  status)
    [[ $# -eq 2 ]] || usage
    do_status "$2"
    ;;
  *)
    usage
    ;;
esac
