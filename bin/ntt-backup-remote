#!/usr/bin/env -S /home/pball/.local/bin/uv run --script
# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "psycopg>=3.2.3",
#     "loguru",
#     "humanize",
# ]
# ///
# Author: PB and Claude
# Date: 2025-10-13
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# ntt/bin/ntt-backup-remote
#
# Remote backup tool: pg_dump + deduplicated blob backup to remote server via SSH

import argparse
import psycopg
import subprocess
import sys
import os
import time
import signal
import humanize
from pathlib import Path
from datetime import datetime
from loguru import logger

# Use /tmp for lockfile (user-writable, survives reboots)
LOCKFILE = Path("/tmp/ntt-backup-remote.lock")

# Remote backup configuration
REMOTE_HOST = "pball@chll"
REMOTE_PATH = "/storage/pball"
SSH_KEY = "/home/pball/.ssh/id_ed25519"

class BackupLockError(Exception):
    """Raised when backup is already running"""
    pass

def acquire_lock():
    """Acquire exclusive lock, exit if already running"""
    if LOCKFILE.exists():
        # Check if process is actually running
        try:
            with LOCKFILE.open('r') as f:
                pid = int(f.read().strip())
            # Check if PID exists
            os.kill(pid, 0)  # Doesn't kill, just checks existence
            raise BackupLockError(f"ntt-backup-remote already running (PID {pid})")
        except (ProcessLookupError, ValueError):
            # Stale lockfile, remove it
            logger.warning("Removing stale lockfile", old_pid=LOCKFILE.read_text().strip())
            LOCKFILE.unlink()

    # Write our PID
    with LOCKFILE.open('w') as f:
        f.write(str(os.getpid()))
    logger.info("Lock acquired", pid=os.getpid())

def release_lock():
    """Remove lockfile"""
    LOCKFILE.unlink(missing_ok=True)
    logger.info("Lock released")

def validate_remote_access():
    """Ensure remote server is accessible and directories exist"""
    logger.info("Testing remote connectivity", host=REMOTE_HOST, path=REMOTE_PATH)

    # Test SSH connectivity
    result = subprocess.run([
        'ssh', '-i', SSH_KEY, '-o', 'ConnectTimeout=10',
        REMOTE_HOST, 'echo', 'connected'
    ], capture_output=True, text=True, timeout=15)

    if result.returncode != 0:
        raise RuntimeError(f"Cannot connect to remote host: {REMOTE_HOST}\n{result.stderr}")

    # Check remote directories exist and are writable
    check_cmd = f"test -d {REMOTE_PATH}/by-hash && test -d {REMOTE_PATH}/pgdump && test -w {REMOTE_PATH}"
    result = subprocess.run([
        'ssh', '-i', SSH_KEY, REMOTE_HOST, check_cmd
    ], capture_output=True, text=True)

    if result.returncode != 0:
        raise RuntimeError(f"Remote directories missing or not writable: {REMOTE_HOST}:{REMOTE_PATH}")

    logger.info("Remote access validated", host=REMOTE_HOST, path=REMOTE_PATH)

class BackupWorker:
    def __init__(self, batch_size=1000, skip_pgdump=False, db_url=None):
        self.batch_size = batch_size
        self.skip_pgdump = skip_pgdump
        self.db_url = db_url or os.environ.get('NTT_DB_URL', 'postgres:///copyjob')

        # Fixed paths for remote
        self.source_root = Path('/data/cold/by-hash')
        self.remote_host = REMOTE_HOST
        self.remote_path = REMOTE_PATH
        self.ssh_key = SSH_KEY
        self.dest_root = f"{self.remote_host}:{self.remote_path}/by-hash/"

        self.conn = None
        self.stats = {
            'files_copied': 0,
            'bytes_copied': 0,
            'start_time': time.time(),
            'last_progress_time': time.time()
        }

    def connect_db(self):
        """Connect to database"""
        self.conn = psycopg.connect(self.db_url)
        logger.info("Database connected", db_url=self.db_url)

    def run_pgdump(self):
        """Run pg_dump and copy to remote with rotation"""
        local_dump = Path('/tmp/copyjob.pgdump')
        remote_dump = f"{self.remote_host}:{self.remote_path}/pgdump/copyjob.pgdump"
        remote_prior = f"{self.remote_host}:{self.remote_path}/pgdump/copyjob.pgdump-prior"

        logger.info("Starting database dump", dest=str(local_dump))

        # Dump to /tmp locally
        result = subprocess.run([
            'pg_dump',
            '-d', self.db_url,
            '-F', 'c',  # Custom format
            '-f', str(local_dump)
        ], capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"pg_dump failed: {result.stderr}")

        size_mb = local_dump.stat().st_size / 1024 / 1024
        logger.info("Database dump complete", size_mb=round(size_mb, 1))

        # Rotate remote backup (prior will be overwritten on next backup)
        logger.info("Copying dump to remote", dest=remote_dump)
        subprocess.run([
            'ssh', '-i', self.ssh_key, self.remote_host,
            f'mv {self.remote_path}/pgdump/copyjob.pgdump {self.remote_path}/pgdump/copyjob.pgdump-prior 2>/dev/null || true'
        ], capture_output=True)

        # Copy new dump to remote
        result = subprocess.run([
            'rsync', '-a', '-e', f'ssh -i {self.ssh_key}',
            str(local_dump), remote_dump
        ], capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"Failed to copy dump to remote: {result.stderr}")

        # Clean up local temp file
        local_dump.unlink()

        logger.bind(event="pgdump_complete").info(
            "Database dump copied to remote",
            dest=remote_dump,
            size_mb=round(size_mb, 1)
        )

        return remote_dump

    def get_pending_batch(self):
        """Query DB for next batch of pending blobids for remote backup"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT blobid FROM blobs
                WHERE (remote_copied IS FALSE OR remote_copied IS NULL)
                  AND (remote_copy_failed IS FALSE OR remote_copy_failed IS NULL)
                LIMIT %s
            """, (self.batch_size,))
            return [row[0] for row in cur.fetchall()]

    def blobid_to_relpath(self, blobid):
        """Convert blobid to relative path: 00/00/00002152fd...b327"""
        prefix1 = blobid[0:2]
        prefix2 = blobid[2:4]
        return f"{prefix1}/{prefix2}/{blobid}"

    def relpath_to_blobid(self, relpath):
        """Convert relative path back to blobid"""
        return Path(relpath).name

    def run_rsync_batch(self, blobids):
        """Run rsync for batch to remote via SSH, parse output, return (successful_ids, failed_ids, bytes)"""
        # Create temp file with relative paths
        temp_file = Path(f"/tmp/backup-batch-{int(time.time())}.txt")
        with temp_file.open('w') as f:
            for blobid in blobids:
                f.write(self.blobid_to_relpath(blobid) + '\n')

        try:
            # Run rsync with itemized output to remote via SSH
            result = subprocess.run([
                'rsync', '-a',
                '--no-perms', '--no-owner', '--no-group',
                '--ignore-times',
                '--itemize-changes',
                '--out-format=%i %n %l',
                '-e', f'ssh -i {self.ssh_key}',
                f'--files-from={temp_file}',
                f'{self.source_root}/',
                f'{self.dest_root}'
            ], capture_output=True, text=True, check=False)

            # Parse output
            successful = []
            total_bytes = 0

            for line in result.stdout.splitlines():
                if line.startswith('<f') or line.startswith('>f'):
                    parts = line.split()
                    if len(parts) >= 3:
                        relpath = parts[1]
                        size = int(parts[2])
                        blobid = self.relpath_to_blobid(relpath)
                        successful.append(blobid)
                        total_bytes += size

            # Failed = requested but not in successful
            successful_set = set(successful)
            failed = [bid for bid in blobids if bid not in successful_set]

            return successful, failed, total_bytes

        finally:
            temp_file.unlink(missing_ok=True)

    def mark_batch_copied(self, blobids):
        """Mark batch as successfully copied to remote in single transaction"""
        with self.conn.cursor() as cur:
            cur.execute("""
                UPDATE blobs
                SET remote_copied = TRUE,
                    remote_last_checked = NOW()
                WHERE blobid = ANY(%s)
            """, (blobids,))
            self.conn.commit()

    def mark_batch_failed(self, blobids):
        """Mark batch as failed for remote backup"""
        with self.conn.cursor() as cur:
            cur.execute("""
                UPDATE blobs
                SET remote_copy_failed = TRUE
                WHERE blobid = ANY(%s)
            """, (blobids,))
            self.conn.commit()

        # Log failures
        for blobid in blobids:
            logger.bind(event="error").warning("Remote blob copy failed", blobid=blobid)

    def update_progress(self):
        """Print console progress and log to JSON"""
        elapsed = time.time() - self.stats['start_time']
        files_per_min = (self.stats['files_copied'] / elapsed) * 60 if elapsed > 0 else 0
        mb_per_min = (self.stats['bytes_copied'] / elapsed / 1024 / 1024) * 60 if elapsed > 0 else 0

        # Get remaining count for remote backup
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT COUNT(*) FROM blobs
                WHERE (remote_copied IS FALSE OR remote_copied IS NULL)
                  AND (remote_copy_failed IS FALSE OR remote_copy_failed IS NULL)
            """)
            remaining = cur.fetchone()[0]

        # Calculate ETA
        eta_hours = (remaining / files_per_min / 60) if files_per_min > 0 else 0

        # Format human-readable progress for console
        total_files = self.stats['files_copied'] + remaining
        files_copied_fmt = humanize.intcomma(self.stats['files_copied'])
        total_files_fmt = humanize.intcomma(total_files)
        bytes_fmt = humanize.naturalsize(self.stats['bytes_copied'], binary=True)
        files_rate = int(files_per_min)
        eta_h = int(eta_hours)

        progress_msg = f"{files_copied_fmt}/{total_files_fmt} files | {bytes_fmt} | {files_rate}/min | {eta_h}h left"

        # Log to console with formatted message
        logger.info(progress_msg)

        # Log structured progress to JSON file
        logger.bind(event="progress").info(
            "Backup progress",
            files_copied=self.stats['files_copied'],
            bytes_copied=self.stats['bytes_copied'],
            gb_copied=round(self.stats['bytes_copied'] / 1024 / 1024 / 1024, 2),
            files_per_min=round(files_per_min, 0),
            mb_per_min=round(mb_per_min, 1),
            remaining=remaining,
            eta_hours=round(eta_hours, 1)
        )

        self.stats['last_progress_time'] = time.time()

    def run(self):
        """Main remote backup workflow"""
        logger.bind(event="start").info(
            "Starting remote backup",
            batch_size=self.batch_size,
            source=str(self.source_root),
            dest=self.dest_root,
            remote_host=self.remote_host,
            skip_pgdump=self.skip_pgdump
        )

        try:
            # Connect to database
            self.connect_db()

            # Run pg_dump unless skipped
            if not self.skip_pgdump:
                self.run_pgdump()
            else:
                logger.warning("Skipping database dump (--skip-pgdump)")

            # Blob backup phase
            logger.info("Starting blob backup")

            while True:
                batch = self.get_pending_batch()
                if not batch:
                    logger.info("No more pending blobs. Backup complete!")
                    break

                batch_start = time.time()
                successful, failed, bytes_copied = self.run_rsync_batch(batch)
                batch_duration = time.time() - batch_start

                # Update database
                if successful:
                    self.mark_batch_copied(successful)
                if failed:
                    self.mark_batch_failed(failed)

                # Update stats
                self.stats['files_copied'] += len(successful)
                self.stats['bytes_copied'] += bytes_copied

                # Log batch completion
                logger.bind(event="batch_complete").debug(
                    "Batch processed",
                    batch_size=len(batch),
                    successful=len(successful),
                    failed=len(failed),
                    bytes=bytes_copied,
                    duration_sec=round(batch_duration, 2)
                )

                # Progress update every 60 seconds
                if time.time() - self.stats['last_progress_time'] > 60:
                    self.update_progress()

        except KeyboardInterrupt:
            logger.warning("Interrupted by user")
            self.update_progress()
        except Exception as e:
            logger.exception("Backup failed", error=str(e))
            raise
        finally:
            if self.conn:
                self.conn.close()
            logger.bind(event="stop").info(
                "Backup stopped",
                total_files=self.stats['files_copied'],
                total_bytes=self.stats['bytes_copied']
            )

def setup_logging():
    """Configure loguru for console + JSON file logging"""
    logger.remove()  # Remove default handler

    # Console: human-readable (exclude structured events that don't have meaningful messages)
    def console_filter(record):
        # Skip the "Backup progress" structured log line (event="progress")
        # We already print the human-readable progress line before it
        return record["extra"].get("event") != "progress"

    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level="INFO",
        filter=console_filter
    )

    # File: structured JSON
    log_file = Path("/var/log/ntt/backup-remote.jsonl")
    log_file.parent.mkdir(parents=True, exist_ok=True)

    logger.add(
        str(log_file),
        format="{message}",
        level="DEBUG",
        serialize=True  # Output as JSON
    )

def main():
    parser = argparse.ArgumentParser(
        description='NTT Remote Backup: Database + deduplicated blob backup to remote server via SSH'
    )
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='Number of blobs per batch (default: 1000)')
    parser.add_argument('--skip-pgdump', action='store_true',
                       help='Skip database backup (for testing)')
    parser.add_argument('--db-url', default=None,
                       help='Database URL (default: from NTT_DB_URL env or postgres:///copyjob)')

    args = parser.parse_args()

    # Setup logging
    setup_logging()

    try:
        # Acquire lock
        acquire_lock()

        # Validate remote access
        validate_remote_access()

        # Run backup
        worker = BackupWorker(
            batch_size=args.batch_size,
            skip_pgdump=args.skip_pgdump,
            db_url=args.db_url
        )

        worker.run()

    except BackupLockError as e:
        logger.error(str(e))
        sys.exit(1)
    except Exception as e:
        logger.exception("Fatal error")
        sys.exit(1)
    finally:
        release_lock()

if __name__ == '__main__':
    main()
