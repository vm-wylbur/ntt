#!/usr/bin/env -S /home/pball/.local/bin/uv run --script
# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "psycopg>=3.2.3",
#     "loguru",
#     "humanize",
# ]
# ///
# Author: PB and Claude
# Date: 2025-10-13
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# ntt/bin/ntt-backup-remote-scp-nocontrol
#
# TEST VERSION: Remote backup without ControlMaster (testing performance)
# Remote backup tool: pg_dump + deduplicated blob backup to remote server via SSH

import argparse
import psycopg
import subprocess
import sys
import os
import time
import signal
import humanize
import multiprocessing
from pathlib import Path
from datetime import datetime
from loguru import logger

# Use /tmp for lockfile (user-writable, survives reboots)
LOCKFILE = Path("/tmp/ntt-backup-remote.lock")

# Remote backup configuration
REMOTE_HOST = "pball@chll-script"
REMOTE_PATH = "/storage/pball"
SSH_KEY = "/home/pball/.ssh/id_ed25519"

class BackupLockError(Exception):
    """Raised when backup is already running"""
    pass

def acquire_lock():
    """Acquire exclusive lock, exit if already running"""
    if LOCKFILE.exists():
        # Check if process is actually running
        try:
            with LOCKFILE.open('r') as f:
                pid = int(f.read().strip())
            # Check if PID exists
            os.kill(pid, 0)  # Doesn't kill, just checks existence
            raise BackupLockError(f"ntt-backup-remote already running (PID {pid})")
        except (ProcessLookupError, ValueError):
            # Stale lockfile, remove it
            logger.warning("Removing stale lockfile", old_pid=LOCKFILE.read_text().strip())
            LOCKFILE.unlink()

    # Write our PID
    with LOCKFILE.open('w') as f:
        f.write(str(os.getpid()))
    logger.info("Lock acquired", pid=os.getpid())

def release_lock():
    """Remove lockfile"""
    LOCKFILE.unlink(missing_ok=True)
    logger.info("Lock released")

def validate_remote_access():
    """Ensure remote server is accessible and directories exist"""
    logger.info("Testing remote connectivity", host=REMOTE_HOST, path=REMOTE_PATH)

    # Test SSH connectivity (override config RemoteCommand, RequestTTY, and X11)
    result = subprocess.run([
        'ssh', '-i', SSH_KEY, '-o', 'ConnectTimeout=10',
        '-o', 'RemoteCommand=none', '-o', 'RequestTTY=no',
        '-o', 'ForwardX11=no',
        REMOTE_HOST, 'echo connected'
    ], capture_output=True, text=True, timeout=15)

    if result.returncode != 0:
        raise RuntimeError(f"Cannot connect to remote host: {REMOTE_HOST}\n{result.stderr}")

    # Check remote directories exist and are writable
    check_cmd = f"test -d {REMOTE_PATH}/by-hash && test -d {REMOTE_PATH}/pgdump && test -w {REMOTE_PATH}"
    result = subprocess.run([
        'ssh', '-i', SSH_KEY,
        '-o', 'RemoteCommand=none', '-o', 'RequestTTY=no', '-o', 'ForwardX11=no',
        REMOTE_HOST, check_cmd
    ], capture_output=True, text=True)

    if result.returncode != 0:
        raise RuntimeError(f"Remote directories missing or not writable: {REMOTE_HOST}:{REMOTE_PATH}")

    logger.info("Remote access validated", host=REMOTE_HOST, path=REMOTE_PATH)

class BackupWorker:
    def __init__(self, batch_size=1000, skip_pgdump=False, db_url=None,
                 worker_id=0, num_workers=1, progress_queue=None, shutdown_event=None):
        self.batch_size = batch_size
        self.skip_pgdump = skip_pgdump
        self.db_url = db_url or os.environ.get('NTT_DB_URL', 'postgres:///copyjob')
        self.worker_id = worker_id
        self.num_workers = num_workers
        self.progress_queue = progress_queue
        self.shutdown_event = shutdown_event

        # Calculate hex range for this worker (partition by first 2 chars of blobid)
        self.hex_start, self.hex_end = self._calculate_hex_range()

        # Fixed paths for remote
        self.source_root = Path('/data/fast/ntt/by-hash')
        self.remote_host = REMOTE_HOST
        self.remote_path = REMOTE_PATH
        self.ssh_key = SSH_KEY
        self.dest_root = f"{self.remote_host}:{self.remote_path}/by-hash/"

        self.conn = None
        # Removed: self.control_path (testing without ControlMaster)
        self.stats = {
            'files_copied': 0,
            'bytes_copied': 0,
            'start_time': time.time(),
            'last_progress_time': time.time()
        }

    def _calculate_hex_range(self):
        """Calculate hex range boundaries for this worker based on worker_id"""
        # Divide 0x00-0xff into num_workers equal ranges
        range_size = 256 // self.num_workers
        start = self.worker_id * range_size

        # Convert to hex strings (2 chars, lowercase)
        hex_start = f"{start:02x}"

        # Last worker gets everything to 'gg' (beyond 'ff')
        if self.worker_id == self.num_workers - 1:
            hex_end = "gg"  # Beyond all valid hex values
        else:
            end = (self.worker_id + 1) * range_size
            hex_end = f"{end:02x}"

        return hex_start, hex_end

    def connect_db(self):
        """Connect to database"""
        self.conn = psycopg.connect(self.db_url)
        logger.info("Database connected", db_url=self.db_url)

    def run_pgdump(self):
        """Run pg_dump and copy to remote with rotation"""
        local_dump = Path('/tmp/copyjob.pgdump')
        remote_dump = f"{self.remote_host}:{self.remote_path}/pgdump/copyjob.pgdump"
        remote_prior = f"{self.remote_host}:{self.remote_path}/pgdump/copyjob.pgdump-prior"

        logger.info("Starting database dump", dest=str(local_dump))

        # Dump to /tmp locally
        result = subprocess.run([
            'pg_dump',
            '-d', self.db_url,
            '-F', 'c',  # Custom format
            '-f', str(local_dump)
        ], capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"pg_dump failed: {result.stderr}")

        size_mb = local_dump.stat().st_size / 1024 / 1024
        logger.info("Database dump complete", size_mb=round(size_mb, 1))

        # Rotate remote backup (prior will be overwritten on next backup)
        logger.info("Copying dump to remote", dest=remote_dump)
        subprocess.run([
            'ssh', '-i', self.ssh_key, self.remote_host,
            f'mv {self.remote_path}/pgdump/copyjob.pgdump {self.remote_path}/pgdump/copyjob.pgdump-prior 2>/dev/null || true'
        ], capture_output=True)

        # Copy new dump to remote
        result = subprocess.run([
            'rsync', '-a', '-e', f'ssh -i {self.ssh_key}',
            str(local_dump), remote_dump
        ], capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"Failed to copy dump to remote: {result.stderr}")

        # Clean up local temp file
        local_dump.unlink()

        logger.bind(event="pgdump_complete").info(
            "Database dump copied to remote",
            dest=remote_dump,
            size_mb=round(size_mb, 1)
        )

        return remote_dump

    def get_pending_batch(self):
        """Query DB for next batch of pending blobids for remote backup (filtered by worker's hex range)"""
        with self.conn.cursor() as cur:
            # Filter by hex range: blobid >= hex_start AND blobid < hex_end
            cur.execute("""
                SELECT blobid FROM blobs
                WHERE (remote_copied IS FALSE OR remote_copied IS NULL)
                  AND (remote_copy_failed IS FALSE OR remote_copy_failed IS NULL)
                  AND blobid >= %s
                  AND blobid < %s
                LIMIT %s
            """, (self.hex_start, self.hex_end, self.batch_size))
            return [row[0] for row in cur.fetchall()]

    def blobid_to_relpath(self, blobid):
        """Convert blobid to relative path: 00/00/00002152fd...b327"""
        prefix1 = blobid[0:2]
        prefix2 = blobid[2:4]
        return f"{prefix1}/{prefix2}/{blobid}"

    def relpath_to_blobid(self, relpath):
        """Convert relative path back to blobid"""
        return Path(relpath).name

    def run_scp_batch(self, blobids):
        """Run scp for batch to remote via SSH, return (successful_ids, failed_ids, bytes)"""
        # Build list of source files and calculate total bytes
        blobid_to_size = {}
        missing_blobids = []
        rel_files = []

        for blobid in blobids:
            relpath = self.blobid_to_relpath(blobid)
            source_path = Path(self.source_root) / relpath
            if source_path.exists():
                blobid_to_size[blobid] = source_path.stat().st_size
                rel_files.append(relpath)
            else:
                logger.warning("Source file missing", worker_id=self.worker_id, blobid=blobid, path=str(source_path))
                missing_blobids.append(blobid)

        # Debug log for first few batches
        if self.stats['files_copied'] < 200:
            logger.debug("Batch file list created",
                       worker_id=self.worker_id,
                       requested=len(blobids),
                       included=len(rel_files),
                       missing=len(missing_blobids))

        if not rel_files:
            # All files missing
            return [], blobids, 0

        # Create temp file with relative paths for tar -T
        temp_file = Path(f"/tmp/backup-batch-{int(time.time())}-{self.worker_id}.txt")
        with temp_file.open('w') as f:
            for relpath in rel_files:
                f.write(relpath + '\n')

        # Use tar over ssh to transfer with directory structure (no ControlMaster)
        tar_cmd = f"cd {self.source_root} && tar cf - -T {temp_file} | ssh -C -o BatchMode=yes -o RemoteCommand=none -o RequestTTY=no -o ForwardX11=no -o StrictHostKeyChecking=accept-new -i {self.ssh_key} {self.remote_host} 'cd {self.remote_path}/by-hash && tar xf -'"

        result = subprocess.run(tar_cmd, shell=True, capture_output=True, text=True, check=False)

        # Log actual errors (not SSH warnings)
        if result.returncode != 0:
            logger.warning("SCP/tar batch failed",
                          worker_id=self.worker_id,
                          returncode=result.returncode,
                          stderr=result.stderr,
                          batch_size=len(blobids))

        # Debug: log output for first few batches
        if self.stats['files_copied'] < 200:
            logger.debug("SCP/tar output",
                       worker_id=self.worker_id,
                       returncode=result.returncode,
                       stderr_sample=result.stderr[:500] if result.stderr else "")

        # If returncode is 0, all files succeeded
        if result.returncode == 0:
            successful = list(blobid_to_size.keys())
            failed = missing_blobids
            total_bytes = sum(blobid_to_size.values())
        else:
            # All files failed
            successful = []
            failed = blobids
            total_bytes = 0

        # Clean up temp file
        temp_file.unlink(missing_ok=True)

        return successful, failed, total_bytes

    def mark_batch_copied(self, blobids):
        """Mark batch as successfully copied to remote in single transaction"""
        with self.conn.cursor() as cur:
            cur.execute("""
                UPDATE blobs
                SET remote_copied = TRUE,
                    remote_last_checked = NOW()
                WHERE blobid = ANY(%s)
            """, (blobids,))
            self.conn.commit()

    def mark_batch_failed(self, blobids):
        """Mark batch as failed for remote backup"""
        with self.conn.cursor() as cur:
            cur.execute("""
                UPDATE blobs
                SET remote_copy_failed = TRUE
                WHERE blobid = ANY(%s)
            """, (blobids,))
            self.conn.commit()

        # Log failures
        for blobid in blobids:
            logger.bind(event="error").warning("Remote blob copy failed", blobid=blobid)

    def update_progress(self):
        """Print console progress and log to JSON"""
        elapsed = time.time() - self.stats['start_time']

        # Calculate byte-based metrics
        bytes_per_sec = (self.stats['bytes_copied'] / elapsed) if elapsed > 0 else 0
        mb_per_sec = bytes_per_sec / 1024 / 1024

        # Calculate remaining
        bytes_remaining = self.stats['total_bytes'] - self.stats['bytes_copied']
        files_remaining = self.stats['total_files'] - self.stats['files_copied']

        # Calculate ETA based on bytes
        eta_hours = (bytes_remaining / bytes_per_sec / 3600) if bytes_per_sec > 0 else 0

        # Format compact byte sizes (use gnu=True for short format like 3.8G)
        bytes_copied_fmt = humanize.naturalsize(self.stats['bytes_copied'], binary=True, gnu=True)
        total_bytes_fmt = humanize.naturalsize(self.stats['total_bytes'], binary=True, gnu=True)

        # Format file counts
        files_copied_fmt = humanize.intcomma(self.stats['files_copied'])
        # Format total files as compact (6M instead of 6,000,000)
        if self.stats['total_files'] >= 1_000_000:
            total_files_fmt = f"{self.stats['total_files'] / 1_000_000:.1f}M"
        elif self.stats['total_files'] >= 1_000:
            total_files_fmt = f"{self.stats['total_files'] / 1_000:.1f}K"
        else:
            total_files_fmt = humanize.intcomma(self.stats['total_files'])

        progress_msg = f"{bytes_copied_fmt}/{total_bytes_fmt} | {mb_per_sec:.0f} MB/s | {files_copied_fmt}/{total_files_fmt} files | ETA {eta_hours:.0f}h"

        # Log to console with formatted message
        logger.info(progress_msg)

        # Log structured progress to JSON file
        logger.bind(event="progress").info(
            "Backup progress",
            files_copied=self.stats['files_copied'],
            files_remaining=files_remaining,
            bytes_copied=self.stats['bytes_copied'],
            bytes_remaining=bytes_remaining,
            gb_copied=round(self.stats['bytes_copied'] / 1024 / 1024 / 1024, 2),
            mb_per_sec=round(mb_per_sec, 1),
            eta_hours=round(eta_hours, 1)
        )

        self.stats['last_progress_time'] = time.time()

    def run(self):
        """Main remote backup workflow"""
        logger.bind(event="start").info(
            "Starting remote backup worker",
            worker_id=self.worker_id,
            num_workers=self.num_workers,
            hex_range=f"{self.hex_start}-{self.hex_end}",
            batch_size=self.batch_size,
            source=str(self.source_root),
            dest=self.dest_root,
            remote_host=self.remote_host,
            skip_pgdump=self.skip_pgdump
        )

        try:
            # Connect to database
            self.connect_db()

            # Run pg_dump unless skipped (only in single-worker mode)
            if not self.skip_pgdump:
                self.run_pgdump()
            elif self.num_workers == 1:
                # Only log skip message in single-worker mode
                logger.warning("Skipping database dump (--skip-pgdump)")

            # Blob backup phase
            logger.info("Starting blob backup")

            # REMOVED: ControlMaster connection setup (testing without it)

            # Query totals once at startup (filtered by this worker's hex range)
            with self.conn.cursor() as cur:
                # Total bytes (join to inode for size, use DISTINCT ON to avoid counting hardlinks multiple times)
                cur.execute("""
                    SELECT COALESCE(SUM(size), 0)
                    FROM (
                        SELECT DISTINCT ON (b.blobid) i.size
                        FROM blobs b
                        JOIN inode i ON b.blobid = i.blobid
                        WHERE (b.remote_copied IS FALSE OR b.remote_copied IS NULL)
                          AND (b.remote_copy_failed IS FALSE OR b.remote_copy_failed IS NULL)
                          AND b.blobid >= %s
                          AND b.blobid < %s
                    ) AS unique_blobs
                """, (self.hex_start, self.hex_end))
                self.stats['total_bytes'] = float(cur.fetchone()[0])

                # Total files
                cur.execute("""
                    SELECT COUNT(*) FROM blobs
                    WHERE (remote_copied IS FALSE OR remote_copied IS NULL)
                      AND (remote_copy_failed IS FALSE OR remote_copy_failed IS NULL)
                      AND blobid >= %s
                      AND blobid < %s
                """, (self.hex_start, self.hex_end))
                self.stats['total_files'] = cur.fetchone()[0]

            logger.info("Backup scope calculated",
                       worker_id=self.worker_id,
                       hex_range=f"{self.hex_start}-{self.hex_end}",
                       total_files=self.stats['total_files'],
                       total_bytes=self.stats['total_bytes'],
                       total_gb=round(self.stats['total_bytes'] / 1024 / 1024 / 1024, 2))

            while True:
                # Check for shutdown signal
                if self.shutdown_event and self.shutdown_event.is_set():
                    logger.info("Shutdown signal received, stopping gracefully", worker_id=self.worker_id)
                    break

                batch = self.get_pending_batch()
                if not batch:
                    logger.info("No more pending blobs. Backup complete!", worker_id=self.worker_id)
                    break

                batch_start = time.time()
                successful, failed, bytes_copied = self.run_scp_batch(batch)
                batch_duration = time.time() - batch_start

                # Update database
                if successful:
                    self.mark_batch_copied(successful)
                if failed:
                    self.mark_batch_failed(failed)

                # Update stats
                self.stats['files_copied'] += len(successful)
                self.stats['bytes_copied'] += bytes_copied

                # Send progress update to coordinator if in multi-worker mode
                if self.progress_queue:
                    self.progress_queue.put({
                        'worker_id': self.worker_id,
                        'files_copied': self.stats['files_copied'],
                        'bytes_copied': self.stats['bytes_copied'],
                        'total_files': self.stats['total_files'],
                        'total_bytes': self.stats['total_bytes']
                    })

                # Log batch completion
                logger.bind(event="batch_complete").debug(
                    "Batch processed",
                    worker_id=self.worker_id,
                    batch_size=len(batch),
                    successful=len(successful),
                    failed=len(failed),
                    bytes=bytes_copied,
                    duration_sec=round(batch_duration, 2)
                )

                # Progress update every 60 seconds (only if not in multi-worker mode)
                if not self.progress_queue and time.time() - self.stats['last_progress_time'] > 60:
                    self.update_progress()

        except KeyboardInterrupt:
            logger.warning("Interrupted by user")
            self.update_progress()
        except Exception as e:
            logger.exception("Backup failed", error=str(e))
            raise
        finally:
            # REMOVED: ControlMaster cleanup (testing without it)

            if self.conn:
                self.conn.close()
            logger.bind(event="stop").info(
                "Backup stopped",
                total_files=self.stats['files_copied'],
                total_bytes=self.stats['bytes_copied']
            )

def worker_process(worker_id, num_workers, batch_size, skip_pgdump, db_url, progress_queue, shutdown_event):
    """Entry point for worker process"""
    try:
        worker = BackupWorker(
            batch_size=batch_size,
            skip_pgdump=True,  # Only main process does pgdump
            db_url=db_url,
            worker_id=worker_id,
            num_workers=num_workers,
            progress_queue=progress_queue,
            shutdown_event=shutdown_event
        )
        worker.run()
    except Exception as e:
        logger.exception(f"Worker {worker_id} failed", error=str(e))
        raise

class WorkerCoordinator:
    """Coordinates multiple backup workers for parallel execution"""

    def __init__(self, num_workers, batch_size, skip_pgdump, db_url):
        self.num_workers = num_workers
        self.batch_size = batch_size
        self.skip_pgdump = skip_pgdump
        self.db_url = db_url

        # Multiprocessing primitives
        self.shutdown_event = multiprocessing.Event()
        self.progress_queue = multiprocessing.Queue()
        self.workers = []

        # Aggregated stats
        self.worker_stats = {}
        self.start_time = time.time()
        self.last_progress_time = time.time()

        # Signal handling
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)

    def signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully"""
        logger.warning("Received shutdown signal, stopping workers...")
        self.shutdown_event.set()

    def spawn_workers(self):
        """Spawn N worker processes"""
        logger.info(f"Spawning {self.num_workers} worker processes")

        for worker_id in range(self.num_workers):
            p = multiprocessing.Process(
                target=worker_process,
                args=(worker_id, self.num_workers, self.batch_size, self.skip_pgdump,
                      self.db_url, self.progress_queue, self.shutdown_event),
                name=f"BackupWorker-{worker_id}"
            )
            p.start()
            self.workers.append(p)
            logger.info(f"Started worker {worker_id} (PID {p.pid})")

    def collect_progress(self):
        """Collect progress updates from all workers (non-blocking)"""
        try:
            while not self.progress_queue.empty():
                update = self.progress_queue.get_nowait()
                self.worker_stats[update['worker_id']] = update
        except:
            pass

    def display_progress(self):
        """Display aggregated progress from all workers"""
        # Aggregate stats (convert to float to avoid Decimal issues from PostgreSQL)
        total_files_copied = sum(s['files_copied'] for s in self.worker_stats.values())
        total_bytes_copied = sum(s['bytes_copied'] for s in self.worker_stats.values())
        total_files = sum(s['total_files'] for s in self.worker_stats.values())
        total_bytes = float(sum(s['total_bytes'] for s in self.worker_stats.values()))

        elapsed = time.time() - self.start_time

        # Calculate metrics
        bytes_per_sec = (total_bytes_copied / elapsed) if elapsed > 0 else 0
        mb_per_sec = bytes_per_sec / 1024 / 1024

        bytes_remaining = total_bytes - total_bytes_copied
        files_remaining = total_files - total_files_copied

        eta_hours = (bytes_remaining / bytes_per_sec / 3600) if bytes_per_sec > 0 else 0

        # Format compact byte sizes
        bytes_copied_fmt = humanize.naturalsize(total_bytes_copied, binary=True, gnu=True)
        total_bytes_fmt = humanize.naturalsize(total_bytes, binary=True, gnu=True)

        # Format file counts
        files_copied_fmt = humanize.intcomma(total_files_copied)
        if total_files >= 1_000_000:
            total_files_fmt = f"{total_files / 1_000_000:.1f}M"
        elif total_files >= 1_000:
            total_files_fmt = f"{total_files / 1_000:.1f}K"
        else:
            total_files_fmt = humanize.intcomma(total_files)

        progress_msg = f"{bytes_copied_fmt}/{total_bytes_fmt} | {mb_per_sec:.0f} MB/s | {files_copied_fmt}/{total_files_fmt} files | ETA {eta_hours:.0f}h | {len(self.worker_stats)}/{self.num_workers} workers"

        logger.info(progress_msg)

        # Log structured progress
        logger.bind(event="progress").info(
            "Backup progress",
            files_copied=total_files_copied,
            files_remaining=files_remaining,
            bytes_copied=total_bytes_copied,
            bytes_remaining=bytes_remaining,
            gb_copied=round(total_bytes_copied / 1024 / 1024 / 1024, 2),
            mb_per_sec=round(mb_per_sec, 1),
            eta_hours=round(eta_hours, 1),
            active_workers=len(self.worker_stats)
        )

    def monitor_workers(self):
        """Monitor worker progress and display updates"""
        logger.info("Monitoring worker progress...")

        while True:
            # Check if all workers finished
            alive_workers = [w for w in self.workers if w.is_alive()]
            if not alive_workers:
                logger.info("All workers finished")
                break

            # Collect and display progress
            self.collect_progress()

            if time.time() - self.last_progress_time > 60:
                self.display_progress()
                self.last_progress_time = time.time()

            # Sleep briefly before next check
            time.sleep(5)

        # Final progress update
        self.collect_progress()
        self.display_progress()

    def stop_workers(self, timeout=30):
        """Stop all workers gracefully"""
        logger.info("Stopping workers...")

        # Wait for workers to finish current batch
        for worker in self.workers:
            worker.join(timeout=timeout)
            if worker.is_alive():
                logger.warning(f"Worker {worker.name} (PID {worker.pid}) didn't exit, terminating")
                worker.terminate()

        # Final cleanup - kill any stragglers
        time.sleep(1)
        for worker in self.workers:
            if worker.is_alive():
                logger.error(f"Worker {worker.name} (PID {worker.pid}) still alive, killing")
                worker.kill()

    def run(self):
        """Main coordinator workflow"""
        try:
            # Run pg_dump on main process
            if not self.skip_pgdump:
                logger.info("Running database dump on main process")
                worker = BackupWorker(batch_size=self.batch_size, db_url=self.db_url)
                worker.run_pgdump()
            else:
                logger.warning("Skipping database dump (--skip-pgdump)")

            # Spawn workers
            self.spawn_workers()

            # Monitor progress
            self.monitor_workers()

        except KeyboardInterrupt:
            logger.warning("Interrupted by user")
            self.shutdown_event.set()
        except Exception as e:
            logger.exception("Coordinator failed", error=str(e))
            self.shutdown_event.set()
            raise
        finally:
            self.stop_workers()

def setup_logging():
    """Configure loguru for console + JSON file logging"""
    logger.remove()  # Remove default handler

    # Console: human-readable (exclude structured events that don't have meaningful messages)
    def console_filter(record):
        # Skip the "Backup progress" structured log line (event="progress")
        # We already print the human-readable progress line before it
        return record["extra"].get("event") != "progress"

    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level="INFO",
        filter=console_filter
    )

    # File: structured JSON
    log_file = Path("/var/log/ntt/backup-remote.jsonl")
    log_file.parent.mkdir(parents=True, exist_ok=True)

    logger.add(
        str(log_file),
        format="{message}",
        level="DEBUG",
        serialize=True  # Output as JSON
    )

def main():
    parser = argparse.ArgumentParser(
        description='NTT Remote Backup: Database + deduplicated blob backup to remote server via SSH'
    )
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='Number of blobs per batch (default: 1000)')
    parser.add_argument('--skip-pgdump', action='store_true',
                       help='Skip database backup (for testing)')
    parser.add_argument('--db-url', default=None,
                       help='Database URL (default: from NTT_DB_URL env or postgres:///copyjob)')
    parser.add_argument('--workers', type=int, default=1,
                       help='Number of parallel workers (default: 1)')

    args = parser.parse_args()

    # Setup logging
    setup_logging()

    try:
        # Acquire lock
        acquire_lock()

        # Validate remote access
        validate_remote_access()

        # Choose execution mode based on worker count
        if args.workers > 1:
            # Multi-worker mode: use coordinator
            logger.info(f"Starting multi-worker backup with {args.workers} workers")
            coordinator = WorkerCoordinator(
                num_workers=args.workers,
                batch_size=args.batch_size,
                skip_pgdump=args.skip_pgdump,
                db_url=args.db_url
            )
            coordinator.run()
        else:
            # Single-worker mode: direct execution (backward compatible)
            logger.info("Starting single-worker backup")
            worker = BackupWorker(
                batch_size=args.batch_size,
                skip_pgdump=args.skip_pgdump,
                db_url=args.db_url
            )
            worker.run()

    except BackupLockError as e:
        logger.error(str(e))
        sys.exit(1)
    except Exception as e:
        logger.exception("Fatal error")
        sys.exit(1)
    finally:
        release_lock()

if __name__ == '__main__':
    main()
