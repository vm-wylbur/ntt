#!/usr/bin/env bash
# Author: PB, Claude, and Kimi
# Date: 2025-10-05
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# ntt/bin/ntt-loader-partitioned
#
# NTT loader with partition support – streaming .raw (034/NUL) → Postgres partitioned tables
#
# Key differences from ntt-loader:
#   - Auto-creates partition for new medium_hash before INSERT
#   - Dramatically faster dedupe phase (empty partition = zero ON CONFLICT overhead)
#
# Expected performance: bb22 (11.2M paths) ~5min COPY + ~30sec dedupe = ~6min total
set -euo pipefail

FILE=${1:?file.raw required}
MEDIUM_HASH=${2:?medium_hash required}
DB_URL=${NTT_DB_URL:-postgres:///copyjob}
LOG_JSON=${NTT_LOADER_LOG:-/var/log/ntt/loader.jsonl}
ENUM_LOG=${NTT_ENUM_LOG:-/var/log/ntt/enum.jsonl}

mkdir -p "$(dirname "$LOG_JSON")"
chmod 755 "$(dirname "$LOG_JSON")" 2>/dev/null || true

# ---------- logging functions ----------
log() {
  jq -cn --arg ts "$(date -Iseconds)" \
        --arg stage "$1" \
        --argjson extra "$2" \
        '$extra + {ts: $ts, stage: $stage}' \
  >> "$LOG_JSON"
  chmod 644 "$LOG_JSON" 2>/dev/null || true
}

fail() {
  echo "Error: $1" >&2
  log error "{\"msg\": \"$1\"}"
  exit 1
}

# ---------- 0. auto-create partition for this medium ----------
echo "[$(date -Iseconds)] Creating partition for medium $MEDIUM_HASH..." >&2
log partition_check "{\"medium_hash\": \"$MEDIUM_HASH\"}"

# Use first 8 chars of medium_hash for partition name
PARTITION_SUFFIX="${MEDIUM_HASH:0:8}"

# Auto-create partitions (IF NOT EXISTS makes this idempotent)
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
CREATE TABLE IF NOT EXISTS inode_p_${PARTITION_SUFFIX}
    PARTITION OF inode FOR VALUES IN ('$MEDIUM_HASH');

CREATE TABLE IF NOT EXISTS path_p_${PARTITION_SUFFIX}
    PARTITION OF path FOR VALUES IN ('$MEDIUM_HASH');
" || fail "Failed to create partitions for medium $MEDIUM_HASH"

echo "[$(date -Iseconds)] Partition ready (inode_p_${PARTITION_SUFFIX}, path_p_${PARTITION_SUFFIX})" >&2

# ---------- 1. create working table ----------
TABLE_NAME="tmp_path_$$"
echo "[$(date -Iseconds)] Creating working table..." >&2
log start "{\"file\": \"$FILE\", \"medium_hash\": \"$MEDIUM_HASH\"}"

# Create temp table that matches raw file format
TEMP_TABLE="raw_$$"
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
CREATE TABLE $TEMP_TABLE (
  mode        text,
  dev         bigint,
  ino         bigint,
  nlink       int,
  size        bigint,
  mtime       bigint,
  path        text
);

CREATE TABLE $TABLE_NAME (
  medium_hash text,
  dev         bigint,
  ino         bigint,
  nlink       int,
  size        bigint,
  mtime       numeric,
  path        text
);
" || fail "Failed to create working tables"

# ---------- 2. raw → working table (034 delimiter) ----------
echo "[$(date -Iseconds)] Loading raw data into working table..." >&2
log load_start "{\"file\": \"$FILE\"}"

# Get expected record count from enum log instead of re-counting
EXPECTED_RECORDS=$(jq -rs --arg file "$FILE" '
  map(select(.stage == "enum_complete" and .out == $file)) |
  sort_by(.ts) | last | .rows // "unknown"
' "$ENUM_LOG")
echo "[$(date -Iseconds)] Expecting $EXPECTED_RECORDS records (from enum log)..." >&2

# Escape literal CR/LF in data, then convert null to LF for PostgreSQL
echo "[$(date -Iseconds)] Starting data conversion (escape CR/LF, null → LF)..." >&2
echo "[$(date -Iseconds)] Running PostgreSQL COPY command..." >&2

# Use temporary file to capture both output and exit code
TEMP_OUTPUT="/tmp/ntt-copy-output-$$"
# Pipeline: escape special chars (backslash, CR, LF) → PostgreSQL COPY
# NOTE: Paths are stored as bytea to preserve invalid UTF-8 sequences
# Use Latin1 encoding to accept all byte values without UTF-8 validation
if perl -pe 'BEGIN{$/=\1} s/\\/\\\\/g; s/\r/\\r/g; s/\n/\\n/g; s/\0/\n/g' < "$FILE" | \
    sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
SET client_encoding = 'LATIN1';

COPY $TEMP_TABLE(mode,dev,ino,nlink,size,mtime,path)
FROM STDIN
WITH (FORMAT text, DELIMITER E'\\034', NULL '');
" > "$TEMP_OUTPUT" 2>&1; then
  echo "[$(date -Iseconds)] PostgreSQL COPY command completed" >&2
  COPY_RESULT=$(cat "$TEMP_OUTPUT")
else
  echo "[$(date -Iseconds)] PostgreSQL COPY command failed" >&2
  echo "PostgreSQL COPY error:" >&2
  cat "$TEMP_OUTPUT" >&2
  rm -f "$TEMP_OUTPUT"
  fail "Failed to load raw data into temp table"
fi
rm -f "$TEMP_OUTPUT"

# Extract actual imported count from PostgreSQL output
ACTUAL_RECORDS=$(echo "$COPY_RESULT" | grep "COPY" | grep -o '[0-9]*' | tail -1)
echo "[$(date -Iseconds)] PostgreSQL imported $ACTUAL_RECORDS records" >&2

# Sanity check: compare expected vs actual
if [[ "$EXPECTED_RECORDS" != "unknown" ]] && [[ "$EXPECTED_RECORDS" != "$ACTUAL_RECORDS" ]]; then
  echo "WARNING: Record count mismatch! Expected: $EXPECTED_RECORDS, Actual: $ACTUAL_RECORDS" >&2
  log warning "{\"expected\": $EXPECTED_RECORDS, \"actual\": $ACTUAL_RECORDS, \"msg\": \"record_count_mismatch\"}"
fi

# Transfer data to working table with medium_hash
# Note: path is kept as text in temp tables, will be converted to bytea when inserting to final path table
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
INSERT INTO $TABLE_NAME (medium_hash,dev,ino,nlink,size,mtime,path)
SELECT '$MEDIUM_HASH', dev, ino, nlink, size, mtime, path
FROM $TEMP_TABLE;
" || fail "Failed to transfer data to working table"

# Create indexes on working table for fast deduplication operations
echo "[$(date -Iseconds)] Creating indexes on working table..." >&2
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
CREATE INDEX idx_${TABLE_NAME}_ino ON $TABLE_NAME(ino);
CREATE INDEX idx_${TABLE_NAME}_ino_path ON $TABLE_NAME(ino, path);
" || fail "Failed to create indexes on working table"

# ---------- 3. dedupe into real tables ----------
echo "[$(date -Iseconds)] Deduplicating into final tables..." >&2
log dedupe_start "{\"medium_hash\": \"$MEDIUM_HASH\"}"

sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
-- Performance tuning for bulk load
SET work_mem = '256MB';
SET maintenance_work_mem = '1GB';
SET synchronous_commit = OFF;

-- medium row (if new) - use known medium_hash instead of scanning table
INSERT INTO medium (medium_hash, added_at)
VALUES ('$MEDIUM_HASH', now())
ON CONFLICT DO NOTHING;

-- SAFETY CHECK: Verify partitions are attached before proceeding
-- This catches spontaneous detachment issues early
DO \$\$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_inherits
    WHERE inhrelid = 'path_p_${PARTITION_SUFFIX}'::regclass
      AND inhparent = 'path'::regclass
  ) THEN
    RAISE EXCEPTION 'SAFETY CHECK FAILED: partition path_p_${PARTITION_SUFFIX} is not attached to parent table';
  END IF;
  IF NOT EXISTS (
    SELECT 1 FROM pg_inherits
    WHERE inhrelid = 'inode_p_${PARTITION_SUFFIX}'::regclass
      AND inhparent = 'inode'::regclass
  ) THEN
    RAISE EXCEPTION 'SAFETY CHECK FAILED: partition inode_p_${PARTITION_SUFFIX} is not attached to parent table';
  END IF;
END\$\$;

-- Clear any existing data for this medium (loads are always fresh/complete)
-- Use TRUNCATE instead of DELETE (faster, simpler, no FK cascade complexity)
-- Safe with P2P FK architecture - only affects this partition pair
TRUNCATE path_p_${PARTITION_SUFFIX}, inode_p_${PARTITION_SUFFIX};

-- inode rows: Use DISTINCT ON instead of GROUP BY with MAX
-- DISTINCT ON is faster and correct since inode metadata is identical for hardlinks
-- No ON CONFLICT needed - partition is now empty (TRUNCATE above)
INSERT INTO inode (medium_hash,dev,ino,nlink,size,mtime)
SELECT DISTINCT ON (medium_hash, ino)
       '$MEDIUM_HASH' as medium_hash, dev, ino, nlink, size, mtime
FROM   $TABLE_NAME
ORDER BY medium_hash, ino;

-- path rows: Direct SELECT (no GROUP BY needed - verified zero duplicate ino,path pairs)
-- Convert Latin1 text to bytea to preserve exact bytes from filesystem
-- No ON CONFLICT needed - partition is now empty (TRUNCATE above)
INSERT INTO path (medium_hash,dev,ino,path)
SELECT '$MEDIUM_HASH' as medium_hash, dev, ino, convert_to(path, 'LATIN1')
FROM   $TABLE_NAME;
" || fail "Failed to deduplicate into final tables"

# ---------- 4. cleanup and summary ----------
ROWS=$(sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -t -A -c "SELECT count(*) FROM path WHERE medium_hash='$MEDIUM_HASH'" 2>/dev/null | grep -o '[0-9]*' | head -1 || echo "unknown")

# Drop the working tables
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "DROP TABLE $TABLE_NAME, $TEMP_TABLE;" 2>/dev/null || true
if [[ "$ACTUAL_RECORDS" == "0" ]]; then
  echo "[$(date -Iseconds)] ✓ Loading complete: $ROWS paths total for medium $MEDIUM_HASH (no new records - already loaded)" >&2
else
  echo "[$(date -Iseconds)] ✓ Loading complete: $ROWS paths loaded for medium $MEDIUM_HASH" >&2
fi
log done "{\"rows\": \"$ROWS\", \"exit\": 0}"

exit 0
