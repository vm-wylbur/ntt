#!/usr/bin/env bash
# Author: PB and Claude
# Date: 2025-10-06
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# ntt/bin/ntt-loader-detach
#
# NTT loader with DETACH/ATTACH pattern for fast partition reload
#
# Phase 2 optimization: Replace DELETE with DETACH/TRUNCATE/ATTACH pattern
#   - Avoids cross-partition FK validation overhead (4min 41s DELETE → 3s DETACH/ATTACH)
#   - Expected performance: bb22 (11.2M paths) ~5.5min total (vs 10min with DELETE)
#
# Key differences from ntt-loader:
#   - DETACH partitions before load (removes FK temporarily)
#   - TRUNCATE instead of DELETE (instant, no FK checks)
#   - Add CHECK constraints before ATTACH (enables fast constraint inference)
#   - ATTACH partitions after load (1-2s with CHECK vs 3-5min without)
set -euo pipefail

FILE=${1:?file.raw required}
MEDIUM_HASH=${2:?medium_hash required}
DB_URL=${NTT_DB_URL:-postgres:///copyjob}
LOG_JSON=${NTT_LOADER_LOG:-/var/log/ntt/loader.jsonl}
ENUM_LOG=${NTT_ENUM_LOG:-/var/log/ntt/enum.jsonl}

mkdir -p "$(dirname "$LOG_JSON")"
chmod 755 "$(dirname "$LOG_JSON")" 2>/dev/null || true

# ---------- logging functions ----------
log() {
  jq -cn --arg ts "$(date -Iseconds)" \
        --arg stage "$1" \
        --argjson extra "$2" \
        '$extra + {ts: $ts, stage: $stage}' \
  >> "$LOG_JSON"
  chmod 644 "$LOG_JSON" 2>/dev/null || true
}

fail() {
  echo "Error: $1" >&2
  log error "{\"msg\": \"$1\"}"
  exit 1
}

# ---------- timing helper ----------
time_start=$(date +%s)
log_timing() {
  local phase=$1
  local now=$(date +%s)
  local elapsed=$((now - time_start))
  echo "[$(date -Iseconds)] Phase '$phase' completed in ${elapsed}s" >&2
  log timing "{\"phase\": \"$phase\", \"elapsed_seconds\": $elapsed}"
  time_start=$now
}

# ---------- 0. check if partitions exist and DETACH if needed ----------
echo "[$(date -Iseconds)] Checking partition for medium $MEDIUM_HASH..." >&2
log partition_check "{\"medium_hash\": \"$MEDIUM_HASH\"}"

# Use first 8 chars of medium_hash for partition name
PARTITION_SUFFIX="${MEDIUM_HASH:0:8}"

# Check if partitions exist (grep to filter out pager messages)
INODE_EXISTS=$(sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -tA -c "
SELECT COUNT(*) FROM pg_class c
JOIN pg_namespace n ON n.oid = c.relnamespace
WHERE c.relname = 'inode_p_${PARTITION_SUFFIX}'
AND n.nspname = 'public';
" | grep -o '^[0-9]*$' | head -1)

PATH_EXISTS=$(sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -tA -c "
SELECT COUNT(*) FROM pg_class c
JOIN pg_namespace n ON n.oid = c.relnamespace
WHERE c.relname = 'path_p_${PARTITION_SUFFIX}'
AND n.nspname = 'public';
" | grep -o '^[0-9]*$' | head -1)

if [[ "$INODE_EXISTS" == "1" ]] && [[ "$PATH_EXISTS" == "1" ]]; then
  echo "[$(date -Iseconds)] Partitions exist - DETACHING for fast reload..." >&2
  log detach_start "{\"medium_hash\": \"$MEDIUM_HASH\", \"partition_suffix\": \"$PARTITION_SUFFIX\"}"

  # DETACH partitions to avoid FK validation overhead during TRUNCATE
  # CONCURRENTLY mode allows other operations to continue
  # CRITICAL: Must detach inode FIRST, then path (FK dependency order)
  # Use heredoc to run in single psql session (avoids transaction block issues)
  sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" << EOF
ALTER TABLE inode DETACH PARTITION inode_p_${PARTITION_SUFFIX} CONCURRENTLY;
ALTER TABLE path DETACH PARTITION path_p_${PARTITION_SUFFIX} CONCURRENTLY;
EOF
  if [[ $? -ne 0 ]]; then
    fail "Failed to DETACH partitions for medium $MEDIUM_HASH"
  fi

  log_timing "detach"

  echo "[$(date -Iseconds)] TRUNCATING detached partitions..." >&2
  log truncate_start "{\"partition_suffix\": \"$PARTITION_SUFFIX\"}"

  # TRUNCATE is instant on detached partitions (no FK checks needed)
  sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
TRUNCATE path_p_${PARTITION_SUFFIX}, inode_p_${PARTITION_SUFFIX} CASCADE;
" || fail "Failed to TRUNCATE partitions"

  log_timing "truncate"

  NEED_ATTACH=true
else
  echo "[$(date -Iseconds)] Partitions do not exist - creating new..." >&2
  log partition_create "{\"medium_hash\": \"$MEDIUM_HASH\", \"partition_suffix\": \"$PARTITION_SUFFIX\"}"

  # Create new partitions (they will be empty, no DETACH needed)
  sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
CREATE TABLE IF NOT EXISTS inode_p_${PARTITION_SUFFIX}
    PARTITION OF inode FOR VALUES IN ('$MEDIUM_HASH');

CREATE TABLE IF NOT EXISTS path_p_${PARTITION_SUFFIX}
    PARTITION OF path FOR VALUES IN ('$MEDIUM_HASH');
" || fail "Failed to create partitions for medium $MEDIUM_HASH"

  NEED_ATTACH=false
fi

echo "[$(date -Iseconds)] Partition ready (inode_p_${PARTITION_SUFFIX}, path_p_${PARTITION_SUFFIX})" >&2

# ---------- 1. create working table ----------
TABLE_NAME="tmp_path_$$"
echo "[$(date -Iseconds)] Creating working table..." >&2
log start "{\"file\": \"$FILE\", \"medium_hash\": \"$MEDIUM_HASH\"}"

# Create temp table that matches raw file format
TEMP_TABLE="raw_$$"
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
CREATE TABLE $TEMP_TABLE (
  mode        text,
  dev         bigint,
  ino         bigint,
  nlink       int,
  size        bigint,
  mtime       bigint,
  path        text
);

CREATE TABLE $TABLE_NAME (
  medium_hash text,
  dev         bigint,
  ino         bigint,
  nlink       int,
  size        bigint,
  mtime       numeric,
  path        text
);
" || fail "Failed to create working tables"

# ---------- 2. raw → working table (034 delimiter) ----------
echo "[$(date -Iseconds)] Loading raw data into working table..." >&2
log load_start "{\"file\": \"$FILE\"}"

# Get expected record count from enum log instead of re-counting
EXPECTED_RECORDS=$(jq -rs --arg file "$FILE" '
  map(select(.stage == "enum_complete" and .out == $file)) |
  sort_by(.ts) | last | .rows // "unknown"
' "$ENUM_LOG")
echo "[$(date -Iseconds)] Expecting $EXPECTED_RECORDS records (from enum log)..." >&2

# Escape literal CR/LF in data, then convert null to LF for PostgreSQL
echo "[$(date -Iseconds)] Starting data conversion (escape CR/LF, null → LF)..." >&2
echo "[$(date -Iseconds)] Running PostgreSQL COPY command..." >&2

# Use temporary file to capture both output and exit code
TEMP_OUTPUT="/tmp/ntt-copy-output-$$"
# Pipeline: escape special chars (backslash, CR, LF) → PostgreSQL COPY
# NOTE: Paths are stored as bytea to preserve invalid UTF-8 sequences
# Use Latin1 encoding to accept all byte values without UTF-8 validation
if perl -pe 'BEGIN{$/=\1} s/\\/\\\\/g; s/\r/\\r/g; s/\n/\\n/g; s/\0/\n/g' < "$FILE" | \
    sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
SET client_encoding = 'LATIN1';

COPY $TEMP_TABLE(mode,dev,ino,nlink,size,mtime,path)
FROM STDIN
WITH (FORMAT text, DELIMITER E'\\034', NULL '');
" > "$TEMP_OUTPUT" 2>&1; then
  echo "[$(date -Iseconds)] PostgreSQL COPY command completed" >&2
  COPY_RESULT=$(cat "$TEMP_OUTPUT")
else
  echo "[$(date -Iseconds)] PostgreSQL COPY command failed" >&2
  echo "PostgreSQL COPY error:" >&2
  cat "$TEMP_OUTPUT" >&2
  rm -f "$TEMP_OUTPUT"
  fail "Failed to load raw data into temp table"
fi
rm -f "$TEMP_OUTPUT"

# Extract actual imported count from PostgreSQL output
ACTUAL_RECORDS=$(echo "$COPY_RESULT" | grep "COPY" | grep -o '[0-9]*' | tail -1)
echo "[$(date -Iseconds)] PostgreSQL imported $ACTUAL_RECORDS records" >&2

log_timing "copy"

# Sanity check: compare expected vs actual
if [[ "$EXPECTED_RECORDS" != "unknown" ]] && [[ "$EXPECTED_RECORDS" != "$ACTUAL_RECORDS" ]]; then
  echo "WARNING: Record count mismatch! Expected: $EXPECTED_RECORDS, Actual: $ACTUAL_RECORDS" >&2
  log warning "{\"expected\": $EXPECTED_RECORDS, \"actual\": $ACTUAL_RECORDS, \"msg\": \"record_count_mismatch\"}"
fi

# Transfer data to working table with medium_hash
# Note: path is kept as text in temp tables, will be converted to bytea when inserting to final path table
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
INSERT INTO $TABLE_NAME (medium_hash,dev,ino,nlink,size,mtime,path)
SELECT '$MEDIUM_HASH', dev, ino, nlink, size, mtime, path
FROM $TEMP_TABLE;
" || fail "Failed to transfer data to working table"

# Create indexes on working table for fast deduplication operations
echo "[$(date -Iseconds)] Creating indexes on working table..." >&2
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
CREATE INDEX idx_${TABLE_NAME}_ino ON $TABLE_NAME(ino);
CREATE INDEX idx_${TABLE_NAME}_ino_path ON $TABLE_NAME(ino, path);
" || fail "Failed to create indexes on working table"

log_timing "index_working_table"

# ---------- 3. dedupe into real tables ----------
echo "[$(date -Iseconds)] Deduplicating into final tables..." >&2
log dedupe_start "{\"medium_hash\": \"$MEDIUM_HASH\"}"

sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
-- Performance tuning for bulk load
SET work_mem = '256MB';
SET maintenance_work_mem = '1GB';
SET synchronous_commit = OFF;

-- medium row (if new) - use known medium_hash instead of scanning table
INSERT INTO medium (medium_hash, added_at)
VALUES ('$MEDIUM_HASH', now())
ON CONFLICT DO NOTHING;

-- inode rows: Use DISTINCT ON instead of GROUP BY with MAX
-- DISTINCT ON is faster and correct since inode metadata is identical for hardlinks
-- Partition is empty (TRUNCATE or new), so no ON CONFLICT needed
INSERT INTO inode_p_${PARTITION_SUFFIX} (medium_hash,dev,ino,nlink,size,mtime)
SELECT DISTINCT ON (medium_hash, ino)
       '$MEDIUM_HASH' as medium_hash, dev, ino, nlink, size, mtime
FROM   $TABLE_NAME
ORDER BY medium_hash, ino;

-- path rows: Direct SELECT (no GROUP BY needed - verified zero duplicate ino,path pairs)
-- Convert Latin1 text to bytea to preserve exact bytes from filesystem
-- Partition is empty (TRUNCATE or new), so no ON CONFLICT needed
INSERT INTO path_p_${PARTITION_SUFFIX} (medium_hash,dev,ino,path)
SELECT '$MEDIUM_HASH' as medium_hash, dev, ino, convert_to(path, 'LATIN1')
FROM   $TABLE_NAME;
" || fail "Failed to deduplicate into final tables"

log_timing "insert"

# ---------- 4. ATTACH partitions if needed ----------
if [[ "$NEED_ATTACH" == "true" ]]; then
  echo "[$(date -Iseconds)] Adding CHECK constraints before ATTACH..." >&2
  log attach_prep "{\"partition_suffix\": \"$PARTITION_SUFFIX\"}"

  # Add CHECK constraints that match partition bounds
  # This enables PostgreSQL to infer partition constraints without table scan
  # Critical: ATTACH with CHECK = 1-2s, without CHECK = 3-5min for 11M rows
  sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
ALTER TABLE inode_p_${PARTITION_SUFFIX}
  ADD CONSTRAINT check_inode_p_${PARTITION_SUFFIX}
  CHECK (medium_hash = '$MEDIUM_HASH');

ALTER TABLE path_p_${PARTITION_SUFFIX}
  ADD CONSTRAINT check_path_p_${PARTITION_SUFFIX}
  CHECK (medium_hash = '$MEDIUM_HASH');
" || fail "Failed to add CHECK constraints"

  echo "[$(date -Iseconds)] ATTACHING partitions (with CHECK constraint optimization)..." >&2
  log attach_start "{\"medium_hash\": \"$MEDIUM_HASH\", \"partition_suffix\": \"$PARTITION_SUFFIX\"}"

  # Enable debug logging to verify CHECK constraint is being used
  sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
SET client_min_messages = 'debug4';

-- ATTACH inode partition first (must exist before path FK references it)
ALTER TABLE inode ATTACH PARTITION inode_p_${PARTITION_SUFFIX}
  FOR VALUES IN ('$MEDIUM_HASH');

-- ATTACH path partition
ALTER TABLE path ATTACH PARTITION path_p_${PARTITION_SUFFIX}
  FOR VALUES IN ('$MEDIUM_HASH');
" 2>&1 | grep -i "partition constraint" || echo "[$(date -Iseconds)] WARNING: No 'partition constraint implied' message - CHECK may not be optimizing ATTACH!" >&2

  log_timing "attach"

  echo "[$(date -Iseconds)] Dropping CHECK constraints (no longer needed)..." >&2

  # Drop CHECK constraints - no longer needed after ATTACH
  sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
ALTER TABLE inode_p_${PARTITION_SUFFIX} DROP CONSTRAINT check_inode_p_${PARTITION_SUFFIX};
ALTER TABLE path_p_${PARTITION_SUFFIX} DROP CONSTRAINT check_path_p_${PARTITION_SUFFIX};
" || echo "Warning: Failed to drop CHECK constraints (non-critical)" >&2
fi

# ---------- 5. ANALYZE and cleanup ----------
echo "[$(date -Iseconds)] Running ANALYZE..." >&2
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "
ANALYZE inode_p_${PARTITION_SUFFIX};
ANALYZE path_p_${PARTITION_SUFFIX};
" || echo "Warning: ANALYZE failed (non-critical)" >&2

log_timing "analyze"

# Get final row count
ROWS=$(sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -t -A -c "SELECT count(*) FROM path WHERE medium_hash='$MEDIUM_HASH'" 2>/dev/null | grep -o '[0-9]*' | head -1 || echo "unknown")

# Drop the working tables
sudo -u "${SUDO_USER:-$USER}" psql "$DB_URL" -c "DROP TABLE $TABLE_NAME, $TEMP_TABLE;" 2>/dev/null || true

if [[ "$ACTUAL_RECORDS" == "0" ]]; then
  echo "[$(date -Iseconds)] ✓ Loading complete: $ROWS paths total for medium $MEDIUM_HASH (no new records - already loaded)" >&2
else
  echo "[$(date -Iseconds)] ✓ Loading complete: $ROWS paths loaded for medium $MEDIUM_HASH" >&2
fi
log done "{\"rows\": \"$ROWS\", \"exit\": 0}"

exit 0
