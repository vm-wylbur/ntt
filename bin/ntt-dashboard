#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "textual>=0.45.0",
#     "plotext>=5.0.0",
#     "psycopg[binary]>=3.0.0",
#     "rich>=13.0.0"
# ]
# ///
#
# Author: PB and Claude
# Date: 2025-09-28
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# ntt/bin/ntt-dashboard
#
# NTT System Dashboard - Real-time monitoring TUI for NTT tools
#
# Features:
# - Tool-specific monitoring tabs (Copier, Re-Hardlink, Verify)
# - Shared system stats across all tabs
# - Live worker status monitoring (no sudo required)
# - Database metrics and queue depth tracking
# - Processing throughput charts
#
# Requirements:
#   - Python 3.13+
#   - PostgreSQL access (same credentials as ntt-copier)
#   - Read access to worker PID files and logs

import asyncio
import os
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, NamedTuple

import plotext as plt
import psycopg
from psycopg.rows import dict_row
from rich.text import Text
from textual.app import App, ComposeResult
from textual.containers import Grid, Horizontal, Vertical
from textual.widgets import (
    DataTable, Footer, Header, Label, Log,
    TabbedContent, TabPane, Static
)
from textual.reactive import reactive
from textual.timer import Timer


# Configuration - same as ntt-copier.py for consistency
DB_URL = os.environ.get('NTT_DB_URL', 'postgresql:///copyjob')
PID_FILE = Path('/tmp/ntt-workers.pids')
WORKER_LOG_PATTERN = '/tmp/ntt-worker-*.log'
VERIFY_LOG = Path(os.environ.get('NTT_LOG_JSON', '/var/log/ntt/verify.jsonl'))

# Set PostgreSQL user to original user when running under sudo
if 'SUDO_USER' in os.environ:
    os.environ['PGUSER'] = os.environ['SUDO_USER']
elif os.geteuid() == 0 and 'USER' in os.environ:
    os.environ['PGUSER'] = 'postgres'

# If running as root and DB_URL doesn't specify a user, add the original user
if os.geteuid() == 0 and 'SUDO_USER' in os.environ:
    if '://' in DB_URL and '@' not in DB_URL:
        DB_URL = DB_URL.replace(':///', f"://{os.environ['SUDO_USER']}@localhost/")


class WorkerStatus(NamedTuple):
    """Immutable worker status data for caching and display."""
    pid: int
    worker_id: str
    status: str  # 'running', 'stopped', 'error'
    uptime: str
    files_processed: int = 0
    current_rate: float = 0.0


class SystemMetrics(NamedTuple):
    """Immutable system metrics for dashboard display."""
    queue_depth: int
    files_processed_today: int
    total_files_processed: int
    processing_rate: float  # files per second
    dedup_ratio: float  # percentage
    last_updated: datetime


class SystemStatsPanel(Static):
    """Reusable system stats panel that updates globally."""

    def compose(self) -> ComposeResult:
        """Create the system stats display."""
        with Horizontal(id="system-stats-row"):
            yield Label("CPU: --", id="cpu-compact")
            yield Label("Temp: --", id="temp-compact")
            yield Label("IO: --", id="io-compact")
            yield Label("Mem: --", id="mem-compact")
            yield Label("Pressure: --", id="pressure-compact")

    def update_stats(self, cpu: float, temp: float, iowait: float, mem: float, pressure: float):
        """Update all system stats displays."""
        # Update CPU
        cpu_label = self.query_one("#cpu-compact", Label)
        cpu_label.update(f"CPU: {cpu:.1f}%")
        if cpu > 80:
            cpu_label.styles.color = "red"
        elif cpu > 60:
            cpu_label.styles.color = "yellow"
        else:
            cpu_label.styles.color = "green"

        # Update Temperature
        temp_label = self.query_one("#temp-compact", Label)
        if temp > 0:
            temp_label.update(f"Temp: {temp:.0f}Â°C")
            if temp > 85:
                temp_label.styles.color = "red"
            elif temp > 70:
                temp_label.styles.color = "yellow"
            else:
                temp_label.styles.color = "green"
        else:
            temp_label.update("Temp: N/A")
            temp_label.styles.color = "dim"

        # Update IOwait
        io_label = self.query_one("#io-compact", Label)
        io_label.update(f"IO: {iowait:.1f}%")
        if iowait > 30:
            io_label.styles.color = "red"
        elif iowait > 10:
            io_label.styles.color = "yellow"
        else:
            io_label.styles.color = "green"

        # Update Memory
        mem_label = self.query_one("#mem-compact", Label)
        mem_label.update(f"Mem: {mem:.1f}%")
        if mem > 90:
            mem_label.styles.color = "red"
        elif mem > 80:
            mem_label.styles.color = "yellow"
        else:
            mem_label.styles.color = "green"

        # Update Pressure
        pressure_label = self.query_one("#pressure-compact", Label)
        pressure_label.update(f"Pressure: {pressure:.0f}%")
        if pressure >= 80:
            pressure_label.styles.color = "red"
        elif pressure >= 60:
            pressure_label.styles.color = "yellow"
        else:
            pressure_label.styles.color = "green"


class NTTDashboard(App):
    """Main TUI application for NTT system monitoring."""

    CSS = """
    #system-stats-row {
        height: 3;
        background: $surface;
        padding: 1;
        margin-bottom: 1;
    }

    #system-stats-row Label {
        width: 1fr;
        text-align: center;
    }

    .status-running {
        color: green;
    }
    .status-stopped {
        color: red;
    }
    .status-error {
        color: yellow;
    }
    .status-done {
        color: cyan;
    }
    .status-unknown {
        color: orange;
    }

    .metric-good {
        color: green;
    }
    .metric-warning {
        color: yellow;
    }
    .metric-critical {
        color: red;
    }

    .panel-title {
        background: $primary;
        color: $text;
        text-style: bold;
        padding: 1;
        width: 100%;
        text-align: center;
    }

    .metrics-container {
        padding: 1;
    }

    DataTable {
        height: 100%;
    }
    """

    # Reactive attributes for live updates
    worker_count = reactive(0)
    queue_depth = reactive(0)
    processing_rate = reactive(0.0)

    def __init__(self):
        super().__init__()
        self.db_conn: Optional[psycopg.Connection] = None
        self.update_timer: Optional[Timer] = None
        self.worker_data: List[WorkerStatus] = []
        self.metrics_history: List[SystemMetrics] = []
        self.current_cpu_load: float = 0.0
        self.current_iowait: float = 0.0
        self.current_cpu_temp: float = 0.0
        self.current_mem_usage: float = 0.0
        self.current_mem_pressure: float = 0.0
        self.inode_dedup_ratio: float = 0.0
        self.hardlink_stats = None
        self.prev_hardlink_stats = None
        self.hardlink_stats_time = None
        self.hardlink_blob_rate: float = 0.0
        self.hardlink_link_rate: float = 0.0
        self.verify_stats = None
        self.system_stats_panels: List[SystemStatsPanel] = []

    def compose(self) -> ComposeResult:
        """Create the main UI layout with tool-specific tabs."""
        yield Header()
        yield Footer()

        with TabbedContent(initial="copier"):
            # Copier Tab
            with TabPane("Copier", id="copier"):
                with Vertical():
                    stats_panel = SystemStatsPanel(id="copier-system-stats")
                    self.system_stats_panels.append(stats_panel)
                    yield stats_panel

                    with Grid(id="copier-grid"):
                        # Worker Status Panel
                        with Vertical(id="copier-workers"):
                            yield Static("Worker Status", classes="panel-title")
                            worker_table = DataTable(id="copier-worker-table")
                            worker_table.add_columns("PID", "Worker", "Status", "Uptime", "Files", "Rate")
                            yield worker_table

                        # Queue and Metrics Panel
                        with Vertical(id="copier-metrics"):
                            yield Static("Queue & Metrics", classes="panel-title")
                            with Vertical(classes="metrics-container"):
                                yield Label("Queue Depth: --", id="copier-queue")
                                yield Label("Processing: --", id="copier-rate")
                                yield Label("Today: --", id="copier-today")
                                yield Label("Total: --", id="copier-total")
                                yield Label("Dedup: --", id="copier-dedup")

            # Re-Hardlink Tab
            with TabPane("Re-Hardlink", id="hardlink"):
                with Vertical():
                    stats_panel = SystemStatsPanel(id="hardlink-system-stats")
                    self.system_stats_panels.append(stats_panel)
                    yield stats_panel

                    with Grid(id="hardlink-grid"):
                        # Progress Panel
                        with Vertical(id="hardlink-progress"):
                            yield Static("Re-Hardlink Progress", classes="panel-title")
                            with Vertical(classes="metrics-container"):
                                yield Label("Total Blobs: --", id="hl-total")
                                yield Label("Complete: --", id="hl-complete")
                                yield Label("Incomplete: --", id="hl-incomplete")
                                yield Label("Progress: --", id="hl-progress")
                                yield Label("Links: --", id="hl-links")

                        # Rates Panel
                        with Vertical(id="hardlink-rates"):
                            yield Static("Processing Rates", classes="panel-title")
                            with Vertical(classes="metrics-container"):
                                yield Label("Blob Rate: --", id="hl-blob-rate")
                                yield Label("Link Rate: --", id="hl-link-rate")
                                yield Label("Est. Time: --", id="hl-eta")

            # Verify Tab
            with TabPane("Verify", id="verify"):
                with Vertical():
                    stats_panel = SystemStatsPanel(id="verify-system-stats")
                    self.system_stats_panels.append(stats_panel)
                    yield stats_panel

                    with Grid(id="verify-grid"):
                        # Verify Status Panel
                        with Vertical(id="verify-status"):
                            yield Static("Verify Status", classes="panel-title")
                            with Vertical(classes="metrics-container"):
                                yield Label("Status: --", id="verify-status-label")
                                yield Label("Verified: --", id="verify-count")
                                yield Label("Errors: --", id="verify-errors")
                                yield Label("Rate: --", id="verify-rate")
                                yield Label("Last Check: --", id="verify-last")

                        # Verify Log Panel
                        with Vertical(id="verify-log"):
                            yield Static("Recent Activity", classes="panel-title")
                            yield Log(id="verify-log-view", max_lines=20)

    async def on_mount(self) -> None:
        """Initialize dashboard on startup."""
        self.title = "NTT Dashboard"

        # Connect to database
        try:
            await self.connect_database()
            self.notify("Connected to NTT database", severity="information")
        except Exception as e:
            self.notify(f"Database connection failed: {e}", severity="error")

        # Start periodic updates (slower to avoid CPU overload)
        self.update_timer = self.set_interval(5.0, self.update_data)

        # Initial data load
        await self.update_data()

    async def connect_database(self) -> None:
        """Establish database connection."""
        loop = asyncio.get_event_loop()
        self.db_conn = await loop.run_in_executor(
            None, lambda: psycopg.connect(DB_URL, row_factory=dict_row)
        )

    async def update_data(self) -> None:
        """Update all dashboard data."""
        try:
            # Add a small delay to prevent tight loops
            await asyncio.sleep(0.1)

            # Update system stats (shared across all tabs)
            cpu_load, iowait = await self.get_cpu_stats()
            self.current_cpu_load = cpu_load
            self.current_iowait = iowait

            self.current_cpu_temp = await self.get_cpu_temperature()

            mem_usage, mem_pressure = await self.get_memory_stats()
            self.current_mem_usage = mem_usage
            self.current_mem_pressure = mem_pressure

            # Update all system stats panels
            for panel in self.system_stats_panels:
                panel.update_stats(
                    self.current_cpu_load,
                    self.current_cpu_temp,
                    self.current_iowait,
                    self.current_mem_usage,
                    self.current_mem_pressure
                )

            # Update tool-specific data
            await self.update_worker_status()
            await self.update_verify_status()

            if self.db_conn:
                await self.update_system_metrics()

            # Refresh tool-specific displays
            self.refresh_copier_display()
            self.refresh_hardlink_display()
            self.refresh_verify_display()

        except Exception as e:
            self.notify(f"Update error: {e}", severity="warning")

    async def update_worker_status(self) -> None:
        """Read worker PIDs and check their status."""
        worker_data = []

        if not PID_FILE.exists():
            self.worker_data = []
            return

        try:
            with open(PID_FILE) as f:
                pids = [line.strip() for line in f if line.strip()]

            for i, pid_str in enumerate(pids):
                if not pid_str:
                    continue

                try:
                    pid = int(pid_str)
                    worker_id = f"worker-{i+1:02d}"

                    # Check if process is running
                    status = await self.check_process_status(pid)

                    # Parse worker log for stats
                    files_processed, current_rate, is_completed = await self.parse_worker_log(worker_id)

                    # Mark as 'done' if process is stopped AND work completed
                    if status == 'stopped' and is_completed:
                        status = 'done'

                    uptime = await self.get_process_uptime(pid)

                    worker_data.append(WorkerStatus(
                        pid=pid,
                        worker_id=worker_id,
                        status=status,
                        uptime=uptime,
                        files_processed=files_processed,
                        current_rate=current_rate
                    ))

                except ValueError:
                    continue

            self.worker_data = worker_data
            self.worker_count = len([w for w in worker_data if w.status == 'running'])

        except Exception as e:
            self.notify(f"Worker status error: {e}", severity="warning")

    async def check_process_status(self, pid: int) -> str:
        """Check if a process is running using efficient OS signal."""
        try:
            os.kill(pid, 0)  # Signal 0 = check existence
            return 'running'
        except ProcessLookupError:
            return 'stopped'
        except PermissionError:
            return 'unknown'
        except (OSError, ValueError):
            return 'error'

    async def get_process_uptime(self, pid: int) -> str:
        """Get process uptime string."""
        try:
            import subprocess
            loop = asyncio.get_event_loop()

            def get_uptime():
                result = subprocess.run(
                    ['ps', '-p', str(pid), '-o', 'etime='],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    return result.stdout.strip()
                return "Unknown"

            return await loop.run_in_executor(None, get_uptime)
        except Exception:
            return "Error"

    async def parse_worker_log(self, worker_id: str) -> tuple[int, float, bool]:
        """Parse worker log file to extract files processed, current rate, and completion status."""
        try:
            import re
            log_file = Path(f"/tmp/ntt-{worker_id}.log")

            if not log_file.exists():
                return 0, 0.0, False

            loop = asyncio.get_event_loop()

            def parse_log():
                try:
                    with open(log_file, 'r') as f:
                        lines = f.readlines()

                    recent_lines = lines[-30:] if len(lines) > 30 else lines

                    files_processed = 0
                    current_rate = 0.0
                    is_completed = False

                    # Check for completion
                    for line in recent_lines:
                        if any(phrase in line.lower() for phrase in [
                            'worker complete',
                            'completed successfully',
                            'finished processing',
                            'all files processed',
                            'worker exiting',
                            'shutting down gracefully',
                            'work complete'
                        ]):
                            is_completed = True

                    # Extract latest heartbeat
                    for line in reversed(recent_lines):
                        if 'Heartbeat:' in line:
                            match = re.search(r'Heartbeat: (\d+) files, [\d.]+\s*MB, ([\d.]+)\s*MB/s', line)
                            if match:
                                files_processed = int(match.group(1))
                                mb_per_sec = float(match.group(2))
                                current_rate = mb_per_sec * 10  # Rough inodes/sec estimate
                                break

                    return files_processed, current_rate, is_completed

                except Exception:
                    return 0, 0.0, False

            return await loop.run_in_executor(None, parse_log)

        except Exception:
            return 0, 0.0, False

    async def update_verify_status(self) -> None:
        """Update verification status from log file."""
        try:
            if not VERIFY_LOG.exists():
                self.verify_stats = None
                return

            loop = asyncio.get_event_loop()

            def parse_verify_log():
                try:
                    import json
                    import subprocess
                    
                    # Use grep to find the last final_summary (much faster than reading whole file)
                    result = subprocess.run(
                        ['grep', '"type": "final_summary"', str(VERIFY_LOG)],
                        capture_output=True, text=True
                    )
                    
                    verified_count = 0
                    error_count = 0
                    skipped_count = 0
                    last_check = None
                    status = "Idle"
                    display_lines = []
                    
                    with open('/tmp/dash-grep-debug.log', 'w') as df:
                        df.write(f"returncode: {result.returncode}\n")
                        df.write(f"stdout: {len(result.stdout) if result.stdout else 0}\n")
                    
                    if result.returncode == 0 and result.stdout:
                        # Get the last matching line
                        last_summary_line = result.stdout.strip().split('\n')[-1]
                        entry = json.loads(last_summary_line)
                        record = entry.get('record', {})
                        extra = record.get('extra', {})
                        
                        if extra.get('type') == 'final_summary':
                            stats = extra.get('stats', {})
                            verified_count = stats.get('blobs_success', 0)
                            error_count = stats.get('blobs_failed', 0)
                            skipped_count = stats.get('blobs_skipped', 0)
                            status = "Complete"
                            
                            # Get timestamp
                            time_info = record.get('time', {})
                            if time_info:
                                timestamp = time_info.get('repr', '')
                                if timestamp:
                                    last_check = timestamp.split()[1][:8]
                    
                    # Get recent progress lines for display
                    with open(VERIFY_LOG, 'r') as f:
                        recent = f.readlines()[-50:]
                    
                    for line in reversed(recent):
                        try:
                            entry = json.loads(line)
                            text = entry.get('text', '')
                            record = entry.get('record', {})
                            level = record.get('level', {}).get('name', '')
                            
                            if level == 'ERROR':
                                status = "Error"
                            
                            if level not in ['DEBUG'] and len(display_lines) < 10:
                                if '/data/cold/' not in text:
                                    display_lines.append(text.strip())
                        except:
                            continue
                    
                    return {
                        'status': status,
                        'verified': verified_count,
                        'errors': error_count,
                        'skipped': skipped_count,
                        'last_check': last_check or "Never",
                        'recent_lines': list(reversed(display_lines))
                    }
                    
                except Exception as e:
                    import traceback
                    with open("/tmp/ntt-dashboard-verify-error.log", "w") as f:
                        f.write(f"Parse error: {e}\n")
                        f.write(traceback.format_exc())
                    return None

            self.verify_stats = await loop.run_in_executor(None, parse_verify_log)
            with open("/tmp/ntt-dashboard-verify-debug.log", "w") as f:
                f.write(f"verify_stats = {self.verify_stats}\n")

        except Exception as e:
            import traceback
            with open("/tmp/ntt-dashboard-verify-error.log", "a") as f:
                f.write(f"Outer error: {e}\n")
                f.write(traceback.format_exc())

    async def get_memory_stats(self) -> tuple[float, float]:
        """Get memory usage and pressure statistics."""
        try:
            loop = asyncio.get_event_loop()

            def read_memory_info():
                try:
                    with open('/proc/meminfo', 'r') as f:
                        meminfo = {}
                        for line in f:
                            parts = line.split()
                            if len(parts) >= 2:
                                key = parts[0].rstrip(':')
                                value = int(parts[1]) * 1024
                                meminfo[key] = value

                    total = meminfo.get('MemTotal', 0)
                    available = meminfo.get('MemAvailable', 0)

                    if total == 0:
                        return 0.0, 0.0

                    used = total - available
                    usage_pct = (used / total) * 100

                    # Calculate pressure based on available memory
                    available_pct = (available / total) * 100

                    if available_pct < 5:
                        pressure = 95.0
                    elif available_pct < 10:
                        pressure = 80.0
                    elif available_pct < 20:
                        pressure = 60.0
                    elif available_pct < 30:
                        pressure = 40.0
                    else:
                        pressure = 20.0

                    return usage_pct, pressure

                except Exception:
                    return 0.0, 0.0

            return await loop.run_in_executor(None, read_memory_info)

        except Exception:
            return 0.0, 0.0

    async def get_cpu_temperature(self) -> float:
        """Get maximum CPU temperature from thermal zones."""
        try:
            import glob
            loop = asyncio.get_event_loop()

            def read_temps():
                try:
                    max_temp = 0.0

                    thermal_zones = glob.glob('/sys/class/thermal/thermal_zone*/temp')
                    for zone_path in thermal_zones:
                        try:
                            with open(zone_path, 'r') as f:
                                temp = int(f.read().strip()) / 1000.0
                                max_temp = max(max_temp, temp)
                        except (IOError, ValueError):
                            continue

                    if max_temp == 0.0:
                        hwmon_temps = glob.glob('/sys/class/hwmon/hwmon*/temp*_input')
                        for temp_path in hwmon_temps:
                            try:
                                with open(temp_path, 'r') as f:
                                    temp = int(f.read().strip()) / 1000.0
                                    max_temp = max(max_temp, temp)
                            except (IOError, ValueError):
                                continue

                    return max_temp if max_temp > 0 else -1.0

                except Exception:
                    return -1.0

            return await loop.run_in_executor(None, read_temps)

        except Exception:
            return -1.0

    async def get_cpu_stats(self) -> tuple[float, float]:
        """Read CPU load and IOwait from /proc/stat."""
        try:
            loop = asyncio.get_event_loop()

            def read_proc_stat():
                try:
                    with open('/proc/stat', 'r') as f:
                        cpu_line = f.readline()

                    parts = cpu_line.split()
                    if len(parts) < 6:
                        return 0.0, 0.0

                    user = int(parts[1])
                    nice = int(parts[2])
                    system = int(parts[3])
                    idle = int(parts[4])
                    iowait = int(parts[5])

                    total = user + nice + system + idle + iowait
                    if total == 0:
                        return 0.0, 0.0

                    if not hasattr(self, 'prev_cpu_stats'):
                        self.prev_cpu_stats = (user + nice + system, idle, iowait, total)
                        return 0.0, 0.0

                    prev_active, prev_idle, prev_iowait, prev_total = self.prev_cpu_stats

                    delta_total = total - prev_total
                    if delta_total == 0:
                        return self.current_cpu_load, self.current_iowait

                    current_active = user + nice + system
                    delta_active = current_active - prev_active
                    delta_iowait = iowait - prev_iowait

                    cpu_load = (delta_active / delta_total) * 100
                    iowait_pct = (delta_iowait / delta_total) * 100

                    self.prev_cpu_stats = (current_active, idle, iowait, total)

                    return cpu_load, iowait_pct

                except Exception:
                    return 0.0, 0.0

            return await loop.run_in_executor(None, read_proc_stat)

        except Exception:
            return 0.0, 0.0

    async def update_system_metrics(self) -> None:
        """Query database for system metrics."""
        if not self.db_conn:
            return

        try:
            loop = asyncio.get_event_loop()

            # Query re-hardlink progress
            def query_hardlink_progress():
                with self.db_conn.cursor() as cur:
                    cur.execute("""
                        SELECT column_name
                        FROM information_schema.columns
                        WHERE table_name = 'blobs'
                        AND column_name = 'expected_hardlinks'
                    """)
                    has_expected_column = cur.fetchone() is not None

                    if has_expected_column:
                        cur.execute("""
                            SELECT
                                COUNT(*) as total_blobs,
                                COUNT(*) FILTER (WHERE n_hardlinks >= expected_hardlinks) as complete_blobs,
                                COUNT(*) FILTER (WHERE n_hardlinks < expected_hardlinks) as incomplete_blobs,
                                SUM(expected_hardlinks) as total_expected,
                                SUM(n_hardlinks) as total_created
                            FROM blobs
                            WHERE expected_hardlinks > 0
                        """)
                        return cur.fetchone()
                    return None

            # Query queue depth
            def query_queue_depth():
                with self.db_conn.cursor() as cur:
                    cur.execute("SELECT COUNT(*) as count FROM inode WHERE copied = false")
                    return cur.fetchone()['count']

            # Query processing stats
            def query_processing_stats():
                with self.db_conn.cursor() as cur:
                    cur.execute("""
                        SELECT
                            COUNT(*) FILTER (WHERE copied = true) as total_processed,
                            COUNT(*) FILTER (WHERE copied = true AND processed_at >= CURRENT_DATE) as today_processed,
                            COUNT(*) FILTER (WHERE copied = true AND processed_at >= NOW() - INTERVAL '1 hour') as hour_processed
                        FROM inode
                    """)
                    return cur.fetchone()

            # Query dedup ratio
            def query_hash_dedup():
                with self.db_conn.cursor() as cur:
                    cur.execute("""
                        WITH sample_paths AS (
                            SELECT i.hash
                            FROM path p
                            JOIN inode i ON i.dev = p.dev AND i.ino = p.ino
                            WHERE i.hash IS NOT NULL
                            LIMIT 5000
                        )
                        SELECT
                            COUNT(*) as sample_size,
                            COUNT(DISTINCT hash) as unique_hashes
                        FROM sample_paths
                    """)
                    result = cur.fetchone()

                    if result and result['sample_size'] > 0:
                        return ((result['sample_size'] - result['unique_hashes'])
                               / result['sample_size']) * 100
                    return 0.0

            try:
                queue_depth = await loop.run_in_executor(None, query_queue_depth)
            except Exception as e:
                self.notify(f"Queue depth query error: {e}", severity="error")
                raise

            try:
                stats = await loop.run_in_executor(None, query_processing_stats)
            except Exception as e:
                self.notify(f"Processing stats query error: {e}", severity="error")
                raise

            try:
                dedup_ratio = await loop.run_in_executor(None, query_hash_dedup)
            except Exception as e:
                self.notify(f"Dedup query error: {e}", severity="error")
                raise

            try:
                hardlink_stats = await loop.run_in_executor(None, query_hardlink_progress)
            except Exception as e:
                self.notify(f"Hardlink stats query error: {e}", severity="error")
                raise

            # Calculate processing rate
            processing_rate = stats['hour_processed'] / 3600.0 if stats['hour_processed'] else 0.0

            # Create metrics object
            metrics = SystemMetrics(
                queue_depth=queue_depth,
                files_processed_today=stats['today_processed'],
                total_files_processed=stats['total_processed'],
                processing_rate=processing_rate,
                dedup_ratio=dedup_ratio,
                last_updated=datetime.now()
            )

            # Update reactive attributes
            self.queue_depth = queue_depth
            self.processing_rate = processing_rate

            # Store for history
            self.metrics_history.append(metrics)
            if len(self.metrics_history) > 300:
                self.metrics_history.pop(0)

            # Calculate hardlink rates
            if hardlink_stats:
                current_time = time.time()

                if self.prev_hardlink_stats and self.hardlink_stats_time:
                    time_delta = current_time - self.hardlink_stats_time
                    if time_delta > 0:
                        blobs_delta = hardlink_stats['complete_blobs'] - self.prev_hardlink_stats['complete_blobs']
                        self.hardlink_blob_rate = blobs_delta / time_delta if blobs_delta >= 0 else 0.0

                        if hardlink_stats['total_created'] and self.prev_hardlink_stats['total_created']:
                            links_delta = hardlink_stats['total_created'] - self.prev_hardlink_stats['total_created']
                            self.hardlink_link_rate = links_delta / time_delta if links_delta >= 0 else 0.0
                        else:
                            self.hardlink_link_rate = 0.0

                self.prev_hardlink_stats = hardlink_stats.copy() if hardlink_stats else None
                self.hardlink_stats_time = current_time
                self.hardlink_stats = hardlink_stats
            else:
                self.hardlink_stats = None

        except Exception as e:
            self.notify(f"Metrics query error: {e}", severity="warning")

    def refresh_copier_display(self):
        """Update copier tab displays."""
        # Update worker table
        try:
            worker_table = self.query_one("#copier-worker-table", DataTable)
            worker_table.clear()

            for worker in self.worker_data:
                status_style = f"status-{worker.status}"
                worker_table.add_row(
                    str(worker.pid),
                    worker.worker_id,
                    Text(worker.status.title(), style=status_style),
                    worker.uptime,
                    str(worker.files_processed),
                    f"{worker.current_rate:.1f}/s"
                )
        except Exception:
            pass

        # Update metrics
        if self.metrics_history:
            latest = self.metrics_history[-1]

            try:
                self.query_one("#copier-queue", Label).update(
                    f"Queue Depth: {latest.queue_depth:,}"
                )
                self.query_one("#copier-rate", Label).update(
                    f"Processing: {latest.processing_rate:.1f} inodes/sec"
                )
                self.query_one("#copier-today", Label).update(
                    f"Today: {latest.files_processed_today:,}"
                )
                self.query_one("#copier-total", Label).update(
                    f"Total: {latest.total_files_processed:,}"
                )
                self.query_one("#copier-dedup", Label).update(
                    f"Dedup: {latest.dedup_ratio:.1f}%"
                )
            except Exception:
                pass

    def refresh_hardlink_display(self):
        """Update re-hardlink tab displays."""
        if not self.hardlink_stats:
            return

        stats = self.hardlink_stats

        try:
            self.query_one("#hl-total", Label).update(
                f"Total Blobs: {stats['total_blobs']:,}"
            )
            self.query_one("#hl-complete", Label).update(
                f"Complete: {stats['complete_blobs']:,}"
            )
            self.query_one("#hl-incomplete", Label).update(
                f"Incomplete: {stats['incomplete_blobs']:,}"
            )

            if stats['total_blobs'] > 0:
                progress = (stats['complete_blobs'] / stats['total_blobs']) * 100
                self.query_one("#hl-progress", Label).update(
                    f"Progress: {progress:.1f}%"
                )

                # Calculate ETA
                if self.hardlink_blob_rate > 0 and stats['incomplete_blobs'] > 0:
                    eta_seconds = stats['incomplete_blobs'] / self.hardlink_blob_rate
                    if eta_seconds < 3600:
                        eta_str = f"{int(eta_seconds / 60)} min"
                    elif eta_seconds < 86400:
                        eta_str = f"{eta_seconds / 3600:.1f} hrs"
                    else:
                        eta_str = f"{eta_seconds / 86400:.1f} days"
                    self.query_one("#hl-eta", Label).update(f"Est. Time: {eta_str}")
                else:
                    self.query_one("#hl-eta", Label).update("Est. Time: --")

            if stats['total_expected'] and stats['total_created']:
                created_pct = (stats['total_created'] / stats['total_expected']) * 100
                self.query_one("#hl-links", Label).update(
                    f"Links: {stats['total_created']:,}/{stats['total_expected']:,} ({created_pct:.1f}%)"
                )

            # Update rates with color coding
            blob_rate_label = self.query_one("#hl-blob-rate", Label)
            if self.hardlink_blob_rate > 0:
                blob_rate_label.update(f"Blob Rate: {self.hardlink_blob_rate:.1f} blobs/sec")
                if self.hardlink_blob_rate > 200:
                    blob_rate_label.styles.color = "green"
                elif self.hardlink_blob_rate > 100:
                    blob_rate_label.styles.color = "cyan"
                else:
                    blob_rate_label.styles.color = "yellow"
            else:
                blob_rate_label.update("Blob Rate: --")

            link_rate_label = self.query_one("#hl-link-rate", Label)
            if self.hardlink_link_rate > 0:
                link_rate_label.update(f"Link Rate: {self.hardlink_link_rate:.0f} links/sec")
                if self.hardlink_link_rate > 10000:
                    link_rate_label.styles.color = "green"
                elif self.hardlink_link_rate > 5000:
                    link_rate_label.styles.color = "cyan"
                else:
                    link_rate_label.styles.color = "yellow"
            else:
                link_rate_label.update("Link Rate: --")

        except Exception:
            pass

    def refresh_verify_display(self):
        """Update verify tab displays."""
        if not self.verify_stats:
            try:
                self.query_one("#verify-status-label", Label).update("Status: Not Running")
                self.query_one("#verify-count", Label).update("Verified: --")
                self.query_one("#verify-errors", Label).update("Errors: --")
                self.query_one("#verify-rate", Label).update("Rate: --")
                self.query_one("#verify-last", Label).update("Last Check: --")
            except Exception:
                pass
            return

        stats = self.verify_stats

        try:
            # Update status with color
            status_label = self.query_one("#verify-status-label", Label)
            status_label.update(f"Status: {stats['status']}")
            if stats['status'] == "Running":
                status_label.styles.color = "green"
            elif stats['status'] == "Error":
                status_label.styles.color = "red"
            elif stats['status'] == "Complete":
                status_label.styles.color = "cyan"
            else:
                status_label.styles.color = "dim"

            # Show verified count with skipped info
            verified_text = f"Verified: {stats['verified']:,}"
            if stats.get('skipped', 0) > 0:
                verified_text += f" ({stats['skipped']:,} skipped)"
            self.query_one("#verify-count", Label).update(verified_text)

            # Update errors with color
            error_label = self.query_one("#verify-errors", Label)
            error_label.update(f"Errors: {stats['errors']}")
            if stats['errors'] > 0:
                error_label.styles.color = "red"
            else:
                error_label.styles.color = "green"

            # Calculate rate (placeholder for now)
            self.query_one("#verify-rate", Label).update("Rate: --")

            self.query_one("#verify-last", Label).update(f"Last Check: {stats['last_check']}")

            # Update log view with recent lines
            log_view = self.query_one("#verify-log-view", Log)
            if 'recent_lines' in stats:
                for line in stats['recent_lines']:
                    if line.strip():
                        log_view.write_line(line.strip())

        except Exception:
            pass

    def on_unmount(self) -> None:
        """Cleanup on exit."""
        if self.update_timer:
            self.update_timer.stop()

        if self.db_conn:
            self.db_conn.close()


def main():
    """Entry point for dashboard application."""
    app = NTTDashboard()
    app.run()


if __name__ == "__main__":
    main()
