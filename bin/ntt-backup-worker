#!/usr/bin/env python3
# Author: PB and Claude
# Date: 2025-10-13
# License: (c) HRDAG, 2025, GPL-2 or newer
#
# ------
# ntt/bin/ntt-backup-worker
#
# Iterative backup worker for copying deduplicated blobs to external drive

import argparse
import psycopg2
import subprocess
import json
import time
import os
from pathlib import Path
from datetime import datetime

class BackupWorker:
    def __init__(self, batch_size=1000, progress_interval=60,
                 db_url=None, source_root='/data/fast/ntt/by-hash',
                 dest_root='/mnt/ntt-backup/by-hash',
                 log_file='/var/log/ntt/backup.jsonl'):
        self.batch_size = batch_size
        self.progress_interval = progress_interval
        self.db_url = db_url or os.environ.get('NTT_DB_URL', 'postgres:///copyjob')
        self.source_root = Path(source_root)
        self.dest_root = Path(dest_root)
        self.log_file = Path(log_file)

        # Ensure log directory exists
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        self.conn = psycopg2.connect(self.db_url)

        self.stats = {
            'files_copied': 0,
            'bytes_copied': 0,
            'start_time': time.time(),
            'last_progress_time': time.time()
        }

    def get_pending_batch(self):
        """Query DB for next batch of pending blobids"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT blobid FROM blobs
                WHERE (external_copied IS FALSE OR external_copied IS NULL)
                  AND (external_copy_failed IS FALSE OR external_copy_failed IS NULL)
                LIMIT %s
            """, (self.batch_size,))
            return [row[0] for row in cur.fetchall()]

    def blobid_to_relpath(self, blobid):
        """Convert blobid to relative path: 00/00/00002152fd...b327"""
        prefix1 = blobid[0:2]
        prefix2 = blobid[2:4]
        return f"{prefix1}/{prefix2}/{blobid}"

    def relpath_to_blobid(self, relpath):
        """Convert relative path back to blobid"""
        return Path(relpath).name

    def run_rsync_batch(self, blobids):
        """Run rsync for batch, parse output, return (successful_ids, failed_ids, bytes)"""
        # Create temp file with relative paths
        temp_file = Path(f"/tmp/backup-batch-{int(time.time())}.txt")
        with temp_file.open('w') as f:
            for blobid in blobids:
                f.write(self.blobid_to_relpath(blobid) + '\n')

        try:
            # Run rsync with itemized output
            result = subprocess.run([
                'rsync', '-a',
                '--no-perms', '--no-owner', '--no-group',
                '--ignore-times',
                '--itemize-changes',
                '--out-format=%i %n %l',
                f'--files-from={temp_file}',
                f'{self.source_root}/',
                f'{self.dest_root}/'
            ], capture_output=True, text=True, check=False)

            # Parse output
            successful = []
            total_bytes = 0

            for line in result.stdout.splitlines():
                if line.startswith('>f'):
                    parts = line.split()
                    if len(parts) >= 3:
                        relpath = parts[1]
                        size = int(parts[2])
                        blobid = self.relpath_to_blobid(relpath)
                        successful.append(blobid)
                        total_bytes += size

            # Failed = requested but not in successful
            successful_set = set(successful)
            failed = [bid for bid in blobids if bid not in successful_set]

            return successful, failed, total_bytes

        finally:
            temp_file.unlink(missing_ok=True)

    def mark_batch_copied(self, blobids):
        """Mark batch as successfully copied in single transaction"""
        with self.conn.cursor() as cur:
            cur.execute("""
                UPDATE blobs
                SET external_copied = TRUE,
                    external_last_checked = NOW()
                WHERE blobid = ANY(%s)
            """, (blobids,))
            self.conn.commit()

    def mark_batch_failed(self, blobids):
        """Mark batch as failed"""
        with self.conn.cursor() as cur:
            cur.execute("""
                UPDATE blobs
                SET external_copy_failed = TRUE
                WHERE blobid = ANY(%s)
            """, (blobids,))
            self.conn.commit()

        # Log each failure
        for blobid in blobids:
            self.log_event('error', {'blobid': blobid, 'error': 'rsync failed'})

    def log_event(self, event_type, data):
        """Append JSONL event to log file"""
        event = {
            'timestamp': datetime.now().isoformat(),
            'event': event_type,
            **data
        }
        with self.log_file.open('a') as f:
            f.write(json.dumps(event) + '\n')

    def update_progress(self):
        """Print console progress and log to JSONL"""
        elapsed = time.time() - self.stats['start_time']
        files_per_min = (self.stats['files_copied'] / elapsed) * 60 if elapsed > 0 else 0
        mb_per_min = (self.stats['bytes_copied'] / elapsed / 1024 / 1024) * 60 if elapsed > 0 else 0

        # Get remaining count
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT COUNT(*) FROM blobs
                WHERE (external_copied IS FALSE OR external_copied IS NULL)
                  AND (external_copy_failed IS FALSE OR external_copy_failed IS NULL)
            """)
            remaining = cur.fetchone()[0]

        # Calculate ETA
        eta_hours = (remaining / files_per_min / 60) if files_per_min > 0 else 0
        eta_days = eta_hours / 24

        # Console output
        gb_copied = self.stats['bytes_copied'] / 1024 / 1024 / 1024
        timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M:%S%z')

        if eta_days > 1:
            eta_str = f"{eta_days:.1f}d"
        else:
            eta_str = f"{eta_hours:.1f}h"

        print(f"[{timestamp}] Progress: {self.stats['files_copied']:,} files ({gb_copied:.2f} GB) | "
              f"Rate: {files_per_min:.0f} files/min, {mb_per_min:.1f} MB/min | "
              f"Remaining: {remaining:,} | ETA: {eta_str}")

        # JSONL log
        self.log_event('progress', {
            'files_copied': self.stats['files_copied'],
            'bytes_copied': self.stats['bytes_copied'],
            'files_per_min': round(files_per_min, 1),
            'mb_per_min': round(mb_per_min, 1),
            'remaining': remaining,
            'eta_hours': round(eta_hours, 1)
        })

        self.stats['last_progress_time'] = time.time()

    def run(self):
        """Main backup loop"""
        self.log_event('start', {'batch_size': self.batch_size})
        print(f"Starting backup worker (batch_size={self.batch_size})")
        print(f"Source: {self.source_root}")
        print(f"Dest: {self.dest_root}")
        print(f"Log: {self.log_file}")
        print()

        try:
            while True:
                batch = self.get_pending_batch()
                if not batch:
                    print("No more pending blobs. Backup complete!")
                    break

                batch_start = time.time()
                successful, failed, bytes_copied = self.run_rsync_batch(batch)
                batch_duration = time.time() - batch_start

                # Update database
                if successful:
                    self.mark_batch_copied(successful)
                if failed:
                    self.mark_batch_failed(failed)

                # Update stats
                self.stats['files_copied'] += len(successful)
                self.stats['bytes_copied'] += bytes_copied

                # Log batch completion
                self.log_event('batch_complete', {
                    'batch_size': len(batch),
                    'successful': len(successful),
                    'failed': len(failed),
                    'bytes': bytes_copied,
                    'duration_sec': round(batch_duration, 2)
                })

                # Progress update?
                if time.time() - self.stats['last_progress_time'] > self.progress_interval:
                    self.update_progress()

        except KeyboardInterrupt:
            print("\nInterrupted by user. Progress saved.")
            self.update_progress()
            self.log_event('interrupted', {})
        except Exception as e:
            print(f"\nError: {e}")
            self.log_event('error', {'error': str(e)})
            raise
        finally:
            self.conn.close()
            self.log_event('stop', {
                'total_files': self.stats['files_copied'],
                'total_bytes': self.stats['bytes_copied']
            })

def main():
    parser = argparse.ArgumentParser(description='NTT External Backup Worker')
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='Number of blobs per batch (default: 1000)')
    parser.add_argument('--progress-interval', type=int, default=60,
                       help='Seconds between progress updates (default: 60)')
    parser.add_argument('--db-url', default=None,
                       help='Database URL (default: from NTT_DB_URL env)')
    parser.add_argument('--source-root', default='/data/fast/ntt/by-hash',
                       help='Source directory (default: /data/fast/ntt/by-hash)')
    parser.add_argument('--dest-root', default='/mnt/ntt-backup/by-hash',
                       help='Destination directory (default: /mnt/ntt-backup/by-hash)')
    parser.add_argument('--log-file', default='/var/log/ntt/backup.jsonl',
                       help='JSONL log file (default: /var/log/ntt/backup.jsonl)')

    args = parser.parse_args()

    worker = BackupWorker(
        batch_size=args.batch_size,
        progress_interval=args.progress_interval,
        db_url=args.db_url,
        source_root=args.source_root,
        dest_root=args.dest_root,
        log_file=args.log_file
    )

    worker.run()

if __name__ == '__main__':
    main()
